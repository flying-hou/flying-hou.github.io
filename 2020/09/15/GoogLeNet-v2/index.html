<!DOCTYPE html>
<html lang="zh-CN">





<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" type="image/png" href="/img/favicon.png">
  <meta name="viewport"
        content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="description" content="">
  <meta name="author" content="htx">
  <meta name="keywords" content="">
  <title>文献翻译——Batch Normalization(GoogLeNet-V2) - htx&#39;s blog</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.0.0/styles/github-gist.min.css" />
  

  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_yg9cfy8wd6.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_pjno9b9zyxs.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script  src="/js/utils.js" ></script>


  <style>
    html {
      filter: grayscale(100%);
    }
  </style>

<meta name="generator" content="Hexo 4.2.1"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand"
       href="/">&nbsp;<strong>htx's blog</strong>&nbsp;</a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="https://pan.htx1998.cn" target="_blank" rel="noopener">
                <i class="iconfont icon-briefcase"></i>
                云盘
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" data-toggle="modal" data-target="#modalSearch">&nbsp;&nbsp;<i
                class="iconfont icon-search"></i>&nbsp;&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="view intro-2" id="background" parallax=true
         style="background: url('https://tva1.sinaimg.cn/large/e6c9d24ely1h4lsdsd3a2j21900u0gqf.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="container text-center white-text fadeInUp">
            <span class="h2" id="subtitle">
              
            </span>

            
              
  <div class="mt-3 post-meta">
    <i class="iconfont icon-date-fill" aria-hidden="true"></i>
    <time datetime="2020-09-15 09:59">
      2020年9月15日 上午
    </time>
  </div>


<div class="mt-1">
  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      8.7k 字
    </span>
  

  
    
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      94
       分钟
    </span>
  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid">
  <div class="row">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-md">
      <div class="container nopadding-md" id="board-ctn">
        <div class="py-5" id="board">
          <div class="post-content mx-auto" id="post">
            
              <p class="note note-info">
                
                  本文最后更新于：2020年9月15日 上午
                
              </p>
            
            <article class="markdown-body">
              <h1 id="批量归一化：通过减少内部协变量偏移来加速深度网络训练"><a href="#批量归一化：通过减少内部协变量偏移来加速深度网络训练" class="headerlink" title="批量归一化：通过减少内部协变量偏移来加速深度网络训练"></a>批量归一化：通过减少内部协变量偏移来加速深度网络训练</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>训练深度神经网络非常复杂，因为在训练过程中，随着先前各层的参数发生变化，各层输入的分布也会发生变化。这通过要求较低的学习率和仔细的参数初始化来减慢训练速度，并且很难训练具有饱和非线性的模型。我们将这种现象称为<em>内部协变量偏移</em>，并通过标准化层输入来解决这个问题。我们的方法力图使标准化成为模型架构的一部分，并为<em>每个训练中的小批量数据</em>执行标准化。批标准化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化器，在某些情况下不需要Dropout。将批量标准化应用到最先进的图像分类模型上，批标准化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批标准化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了<code>4.9％ top-5</code>的验证误差（和<code>4.8％</code>测试误差），超过了人类评估者的准确性。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>深度学习极大地推动了视觉，语音和许多其他领域的最新发展。随机梯度下降（SGD）已被证明是一种训练深层网络的有效方法，并且动量（Sutskever等，2013）和Adagrad（Duchi等，2011）等SGD变体已被用来实现最先进的性能。SGD优化网络参数Θ，以最小化损失</p>
<script type="math/tex; mode=display">\Theta=argmin \frac{1}{N}\sum_{i=1}^{N}ℓ（X_i,\Theta)</script><p>x<sub>1…N</sub>是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为m的<em>小批量数据</em>x<sub>1…m</sub>。通过计算$\frac{1}{m}\frac{\partialℓ(x_i,\Theta)}{\partial\Theta}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而提升。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算m次效率更高。</p>
<p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。</p>
<p>层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，据说会经历<em>协变量转移</em>（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。然而，协变量漂移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。考虑网络计算</p>
<script type="math/tex; mode=display">ℓ=F_2(F_1(u,\Theta_1),\Theta_2)</script><p>F1和F2是任意变换，学习参数Θ1，Θ2以便最小化损失ℓ。学习Θ2可以看作输入x=F1(u,Θ1)送入到子网络ℓ=F2(x,Θ2)。</p>
<p>例如，梯度下降步骤</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gio7sby4h0j30ha04ut8z.jpg" srcset="/img/loading.gif" alt="image-20200912220127281" style="zoom: 50%;" /></center>





<p>（对于批大小m和学习率α）与输入为x的单独网络F2完全等价。因此，输入分布特性使训练更有效——例如训练数据和测试数据之间有相同的分布——也适用于训练子网络。因此x的分布在时间上保持固定是有利的。然后，Θ2不必重新调整来补偿x分布的变化。</p>
<p>子网络输入的固定分布对于子网络外的层也有积极的影响。考虑一个激活函数为z=g(Wu+b)的层，u是层输入，权重矩阵W和偏置向量b是要学习的层参数，$g(x)=\frac{1}{1+exp(−x)}$。随着|x|的增加，g′(x)趋向于0。这意味着对于x=Wu+b的所有维度，除了那些具有小的绝对值之外，流向u的梯度将会消失，模型将缓慢的进行训练。然而，由于x受W,b和下面所有层的参数的影响，训练期间那些参数的改变可能会将x的许多维度移动到非线性的饱和状态并减慢收敛。这个影响随着网络深度的增加而放大。在实践中，饱和问题和由此产生的梯度消失通常通过使用修正线性单元(Nair &amp; Hinton, 2010) ReLU(x)=max(x,0)，仔细的初始化(Bengio &amp; Glorot, 2010; Saxe et al., 2013)和小的学习率来解决。然而，如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。</p>
<p>我们把训练过程中深度网络内部结点的分布变化称为<em>内部协变量转移</em>。消除它可以保证更快的训练。我们提出了一种新的机制，我们称为为<em>批标准化</em>，它是减少内部协变量转移的一个步骤，这样做可以显著加速深度神经网络的训练。它通过标准化步骤来实现，标准化步骤修正了层输入的均值和方差。批标准化减少了梯度对参数或它们的初始值尺度上的依赖，对通过网络的梯度流动有有益的影响。这允许我们使用更高的学习率而没有发散的风险。此外，批标准化使模型正则化并减少了对Dropout(Srivastava et al., 2014)的需求。最后，批标准化通过阻止网络陷入饱和模式让使用饱和非线性成为可能。</p>
<p>在4.2小节，我们将批标准化应用到性能最好的ImageNet分类网络上，并且表明我们可以使用仅7％的训练步骤来匹配其性能，并且可以进一步超过其准确性一大截。通过使用批标准化训练的网络的集合，我们取得了top-5错误率，其改进了ImageNet分类上已知的最佳结果。</p>
<h1 id="2-减少内部协变量偏移"><a href="#2-减少内部协变量偏移" class="headerlink" title="2. 减少内部协变量偏移"></a>2. 减少内部协变量偏移</h1><p>由于训练过程中网络参数的变化，我们将<em>内部协变量转移</em>定义为网络激活分布的变化。为了改善训练，我们寻求减少内部协变量转移。随着训练的进行，通过固定层输入x的分布，我们期望提高训练速度。众所周知(LeCun et al., 1998b; Wiesler &amp; Ney, 2011)如果对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除内部协变量转移的不良影响。</p>
<p>我们考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins &amp; Kavukcuoglu)。然而，如果这些修改分散在优化步骤中，那么梯度下降步骤可能会试图以要求正规化进行更新的方式来更新参数，这会降低梯度下降步骤的影响。例如，考虑一个层，其输入u加上学习到的偏置b，通过减去在训练集上计算的激活值的均值对结果进行归一化：x̂=x−E[x]，x=u+b, X={x1…N}是训练集上x值的集合，$E[x]=\frac1N∑^N_{i=1}x_i$。如果梯度下降步骤忽略了E[x]对b的依赖，那它将更新b←b+Δb，其中Δb∝−∂ℓ/∂x̂。然后u+(b+Δb)−E[u+(b+Δb)]=u+b−E[u+b]。因此，结合b的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，b将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。我们在最初的实验中已经观察到了这一点，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。</p>
<p>上述方法的问题是梯度下降优化没有考虑到标准化中发生的事实。为了解决这个问题，我们希望确保对于任何参数值，网络<em>总是</em>产生具有所需分布的激活值。这样做将允许关于模型参数损失的梯度来解释标准化，以及它对模型参数Θ的依赖。设x为层的输入，将其看作向量，X是这些输入在训练集上的集合。标准化可以写为变换</p>
<script type="math/tex; mode=display">x̂=Norm(x,X)</script><p>它不仅依赖于给定的训练样本x而且依赖于所有样本X——它们中的每一个都依赖于Θ，如果x是由另一层生成的。对于反向传播，我们将需要计算雅可比行列式</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gip9tbg5psj30fy03a74i.jpg" srcset="/img/loading.gif" alt="image-20200913195712646" style="zoom: 50%;" /></center>

<p>忽略后一项会导致上面描述的爆炸。在这个框架中，白化层输入是昂贵的，因为它要求计算协方差矩阵$Cov[x]=E_{x∈X}[xx^T]−E[x]E[x]^T$和它的平方根倒数，从而生成白化的激活$Cov[x]^{−1/2}(x−E[x])$和这些变换进行反向传播的偏导数。这促使我们寻求一种替代方案，以可微分的方式执行输入标准化，并且在每次参数更新后不需要对整个训练集进行分析。</p>
<p>以前的一些方法（例如（Lyu＆Simoncelli，2008））使用通过单个训练样本计算的统计信息，或者在图像网络的情况下，使用给定位置处不同特征图上的统计。然而，通过丢弃激活值绝对尺度改变了网络的表示能力。我们希望通过对相对于整个训练数据统计信息的单个训练样本的激活值进行归一化来保留网络中的信息。</p>
<h1 id="3-通过Mini-Batch统计进行标准化"><a href="#3-通过Mini-Batch统计进行标准化" class="headerlink" title="3. 通过Mini-Batch统计进行标准化"></a>3. 通过Mini-Batch统计进行标准化</h1><p>由于每一层输入的整个白化是代价昂贵的并且不是到处可微分的，因此我们做了两个必要的简化。首先是我们将单独标准化每个标量特征，从而代替在层输入输出对特征进行共同白化，使其具有零均值和单位方差。对于具有d维输入x=(x(1)…x(d))的层，我们将标准化每一维</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gipcgq77alj30b804gwem.jpg" srcset="/img/loading.gif" alt="image-20200913212857571" style="zoom:50%;" /></center>

<p>其中期望和方差在整个训练数据集上计算。如(LeCun et al., 1998b)中所示，这种标准化加速了收敛，即使特征没有去相关。</p>
<p>注意简单标准化层的每一层输入可能会改变层可以表示什么。例如，标准化sigmoid的输入会将它们约束到非线性的线性状态。为了解决这个问题，我们要确保<em>插入到网络中的变换可以表示恒等变换</em>。为了实现这个，对于每一个激活值x(k)，我们引入成对的参数γ(k)，β(k)，它们会归一化和移动标准化值：</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gipd6lxsrsj30bq02m3yi.jpg" srcset="/img/loading.gif" alt="image-20200913215350237" style="zoom:50%;" /></center>

<p>这些参数与原始的模型参数一起学习，并恢复网络的表示能力。实际上，通过设置$γ(k)=\sqrt{Var[x^{(k)}]}$和$β(k)=E[x^{(k)}]$，我们可以重新获得原始的激活值，如果这是要做的最优的事。</p>
<p>每个训练步骤的批处理设置是基于整个训练集的，我们将使用整个训练集来标准化激活值。然而，当使用随机优化时，这是不切实际的。因此，我们做了第二个简化：由于我们在随机梯度训练中使用小批量，因此每个小批量都会产生每次激活的均值和方差的估计值。这样，用于标准化的统计信息可以完全参与梯度反向传播。注意，通过计算每一维的方差而不是联合协方差，可以实现小批量的使用；在联合情况下，将需要正则化，因为小批量大小可能小于白化的激活值的数量，从而导致单个协方差矩阵。</p>
<p>考虑一个大小为m的小批量数据B。由于标准化被单独地应用于每一个激活，所以让我们集中在一个特定的激活x(k)，为了清晰忽略k。在小批量数据里我们有这个激活的m个值，</p>
<script type="math/tex; mode=display">B= \{ x_{1...m} \}</script><p>设标准化值为x̂1…m，它们的线性变换为y1…m。我们把变换</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giqzpkcmcej30am01odfs.jpg" srcset="/img/loading.gif" alt="image-20200915073847452" style="zoom:50%;" /></center>

<p>看作<em>批标准化变换</em>。我们在算法1中提出了BN变换。在算法中，为了数值稳定，ϵ是一个加到小批量数据方差上的常量。</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giqzqt2k5aj30qw0m2jui.jpg" srcset="/img/loading.gif" alt="image-20200915074001053" style="zoom:50%;" /></center>

<p>BN变换可以添加到网络上来操纵任何激活。在公式y=BNγ,β(x)中，我们指出参数γ和β需要进行学习，但应该注意到在每一个训练样本中BN变换不单独处理激活。相反，BNγ,β(x)取决于训练样本和<em>小批量数据中的其它样本</em>。缩放和移动的值y传递到其它的网络层。标准化的激活值x̂在我们的变换内部，但它们的存在至关重要。只要每个小批量的元素从相同的分布中进行采样，如果我们忽略ϵ，那么任何x̂值的分布都具有期望为0，方差为1。这可以通过观察∑mi=1x̂i=0和1m∑mi=1x̂2i=1看到，并取得预期。每一个标准化的激活值$\hat{x}^{(k)}$可以看作由线性变换y(k)=γ(k)x̂(k)+β(k)组成的子网络的输入，接下来是原始网络的其它处理。所有的这些子网络输入都有固定的均值和方差，尽管这些标准化的x̂(k)的联合分布可能在训练过程中改变，但我们预计标准化输入的引入会加速子网络的训练，从而加速整个网络的训练。</p>
<p>在训练过程中我们需要通过这个变换反向传播损失ℓ的梯度，以及计算关于BN变换参数的梯度。我们使用的链式法则如下（简化之前）：</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir051pussj30pe0eagnj.jpg" srcset="/img/loading.gif" alt="image-20200915075342426" style="zoom:50%;" /></center>

<p>因此，BN变换是将标准化激活引入到网络中的可微变换。这确保了在模型训练时，层可以继续学习输入分布，表现出更少的内部协变量转移，从而加快训练。此外，应用于这些标准化的激活上的学习到的仿射变换允许BN变换表示恒等变换并保留网络的能力。</p>
<h2 id="3-1-批标准化网络的训练和推断"><a href="#3-1-批标准化网络的训练和推断" class="headerlink" title="3.1 批标准化网络的训练和推断"></a>3.1 批标准化网络的训练和推断</h2><p>为了<em>批标准化</em>一个网络，根据算法1，我们指定一个激活的子集，然后在每一个激活中插入BN变换。任何以前接收x作为输入的层现在接收BN(x)作为输入。采用批标准化的模型可以使用批梯度下降，或者用小批量数据大小为m&gt;1的随机梯度下降，或使用它的任何变种例如Adagrad (Duchi et al., 2011)进行训练。依赖小批量数据的激活值的标准化可以有效地训练，但在推断过程中是不必要的也是不需要的；我们希望输出只确定性地取决于输入。为此，一旦网络训练完成，我们使用总体统计来进行标准化</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir09r48gnj309403ujre.jpg" srcset="/img/loading.gif" alt="image-20200915075814112" style="zoom:50%;" /></center>

<p>而不是小批量数据统计。跟训练过程中一样，如果忽略ϵ，这些标准化的激活具有相同的均值0和方差1。我们使用无偏方差估计$Var[x]=\frac{m}{m−1}E_\Beta[σ^2_\Beta]$，其中期望是在大小为m的小批量训练数据上得到的，σ2是其样本方差。使用这些值移动平均，我们在训练过程中可以跟踪模型的准确性。由于均值和方差在推断时是固定的，因此标准化是应用到每一个激活上的简单线性变换。它可以进一步由缩放γ和转移β组成，以产生代替BN(x)的单线性变换。算法2总结了训练批标准化网络的过程。</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir0dyyq7vj30rm12ajy5.jpg" srcset="/img/loading.gif" alt="image-20200915080216895" style="zoom:50%;" /></center>

<h2 id="3-2-批标准化卷积网络"><a href="#3-2-批标准化卷积网络" class="headerlink" title="3.2. 批标准化卷积网络"></a>3.2. 批标准化卷积网络</h2><p>批标准化可以应用于网络的任何激活集合。这里我们专注于仿射变换和元素级非线性组成的变换：</p>
<p>z=g(Wu+b)</p>
<p>其中W和b是模型学习的参数，g(⋅)是非线性例如sigmoid或ReLU。这个公式涵盖了全连接层和卷积层。我们在非线性之前通过标准化x=Wu+b加入BN变换。我们也可以标准化层输入u，但由于u可能是另一个非线性的输出，它的分布形状可能在训练过程中改变，并且限制其第一矩或第二矩不能去除协变量转移。相比之下，Wu+b更可能具有对称，非稀疏分布，即“更高斯”（Hyvärinen＆Oja，2000）；对其标准化可能产生具有稳定分布的激活。</p>
<p>注意，由于我们对Wu+b进行标准化，偏置b可以忽略，因为它的效应将会被后面的中心化取消（偏置的作用会归入到算法1的β）。因此，z=g(Wu+b)被z=g(BN(Wu))替代，其中BN变换独立地应用到x=Wu的每一维，每一维具有单独的成对学习参数γ(k)，β(k)。</p>
<p>另外，对于卷积层我们希望标准化遵循卷积特性——为的是同一特征映射的不同元素，在不同的位置，以相同的方式进行标准化。为了实现这个，我们在所有位置联合标准化了小批量数据中的所有激活。在算法1中，我们让是跨越小批量数据的所有元素和空间位置的特征图中所有值的集合——因此对于大小为m的小批量数据和大小为p×q的特征映射，我们使用有效的大小为m′=|B|=m⋅pq的小批量数据。我们每个特征映射学习一对参数γ<sup>(k)</sup>和β<sup>(k)</sup>，而不是每个激活。算法2进行类似的修改，以便推断期间BN变换对在给定的特征映射上的每一个激活应用同样的线性变换。</p>
<h2 id="3-3-批标准化可以提高学习率"><a href="#3-3-批标准化可以提高学习率" class="headerlink" title="3.3 批标准化可以提高学习率"></a>3.3 批标准化可以提高学习率</h2><p>在传统的深度网络中，学习率过高可能会导致梯度爆炸或梯度消失，以及陷入差的局部最小值。批标准化有助于解决这些问题。通过标准化整个网络的激活值，在数据通过深度网络传播时，它可以防止层参数的微小变化被放大。例如，这使sigmoid非线性更容易保持在它们的非饱和状态，这对训练深度sigmoid网络至关重要，但在传统上很难实现。</p>
<p>批标准化也使训练对参数的缩放更有弹性。通常，大的学习率可能会增加层参数的缩放，这会在反向传播中放大梯度并导致模型爆炸。然而，通过批标准化，通过层的反向传播不受其参数缩放的影响。实际上，对于标量a，BN(Wu)=BN((aW)u)</p>
<p>因此<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir17x42cej30d804gglw.jpg" srcset="/img/loading.gif" alt="image-20200915083103735" style="zoom:50%;" /></p>
<p>因此标量不影响层的雅可比行列式，从而不影响梯度传播。此外，因此更大的权重会导致<em>更小的</em>梯度，并且批标准化会稳定参数的增长。</p>
<p>我们进一步推测，批标准化可能会导致雅可比行列式的奇异值接近于1，这被认为对训练是有利的(Saxe et al., 2013)。考虑具有标准化输入的两个连续的层，并且变换位于这些标准化向量之间：ẑ=F(x̂)。如果我们假设x̂和ẑ是高斯分布且不相关的，那么F(x̂)≈Jx̂是对给定模型参数的一个线性变换，x̂和ẑ有单位方差，并且I=Cov[ẑ]=JCov[x̂]JT=JJT。因此，J是正交的，其保留了反向传播中的梯度大小。尽管上述假设在现实中不是真实的，但我们希望批标准化有助于梯度传播更好的执行。这有待于进一步研究。</p>
<h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="4-1-随时间激活"><a href="#4-1-随时间激活" class="headerlink" title="4.1 随时间激活"></a>4.1 随时间激活</h2><p>为了验证内部协变量转移对训练的影响，以及批标准化对抗它的能力，我们考虑了在MNIST数据集上预测数字类别的问题(LeCun et al., 1998a)。我们使用非常简单的网络，28x28的二值图像作为输入，以及三个全连接层，每层100个激活。每一个隐藏层用sigmoid非线性计算y=g(Wu+b)，权重W初始化为小的随机高斯值。最后的隐藏层之后是具有10个激活（每类1个）和交叉熵损失的全连接层。我们训练网络50000次迭代，每份小批量数据中有60个样本。如第3.1节所述，我们在网络的每一个隐藏层后添加批标准化。我们对基准线和批标准化网络之间的比较感兴趣，而不是实现在MNIST上的最佳性能（所描述的架构没有）。</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir1eu093cj30qa0jyq7f.jpg" srcset="/img/loading.gif" alt="image-20200915083742349" style="zoom:50%;" /></center>

<blockquote>
<p>图1。(a)使用批标准化和不使用批标准化训练的网络在MNIST上的测试准确率，以及训练的迭代次数。批标准化有助于网络训练的更快，取得更高的准确率。(b，c)典型的sigmoid在训练过程中输入分布的演变，显示为15%，50%，85%。批标准化使分布更稳定并降低了内部协变量转移。</p>
</blockquote>
<p>图1(a)显示了随着训练进行，两个网络在提供的测试数据上正确预测的分数。批标准化网络具有更高的测试准确率。为了调查原因，我们在训练过程中研究了原始网络N和批标准化网络NtrBN(Alg. 2)中的sigmoid输入。在图1(b，c)中，我们显示，对于来自每个网络的最后一个隐藏层的一个典型的激活，其分布如何演变。原始网络中的分布随着时间的推移而发生显著变化，无论是平均值还是方差，都会使后面的层的训练复杂化。相比之下，随着训练的进行，批标准化网络中的分布更加稳定，这有助于训练。</p>
<h2 id="4-2-ImageNet分类"><a href="#4-2-ImageNet分类" class="headerlink" title="4.2. ImageNet分类"></a>4.2. ImageNet分类</h2><p>我们将批标准化化应用于在ImageNet分类任务（Russakovsky等，2014）上训练的Inception网络的新变种（Szegedy等，2014）。网络具有大量的卷积和池化层，和一个softmax层用来在1000个可能之中预测图像的类别。卷积层使用ReLU作为非线性。与（Szegedy等人，2014年）中描述的网络的主要区别是5×5卷积层被两个连续的3x3卷积层替换，最多可以有128个滤波器。该网络包含13.6⋅10<sup>6</sup>个参数，除了顶部的softmax层之外，没有全连接层。在其余的文本中我们将这个模型称为Inception。训练在大型分布式架构（Dean et al，2012）上进行，10个模型副本中的每一个都使用了5个并行步骤，使用异步带动量的SGD（Sutskever等，2013），小批量数据大小为32。随着训练进行，所有网络都通过计算验证准确率@1来评估，即每幅图像使用单个裁剪图像，在1000个可能性中预测正确标签的概率。</p>
<p>在我们的实验中，我们评估了几个带有批标准化的Inception修改版本。在所有情况下，如第3.2节所述，批标准化以卷积方式应用于每个非线性的输入，同时保持架构的其余部分不变。</p>
<h3 id="4-2-1-加速BN网络"><a href="#4-2-1-加速BN网络" class="headerlink" title="4.2.1. 加速BN网络"></a>4.2.1. 加速BN网络</h3><p>将批标准化简单添加到网络中不能充分利用我们方法的优势。为此，我们进行了以下修改：</p>
<p><em>提高学习率</em>。在批标准化模型中，我们已经能够从高学习率中实现训练加速，没有不良的副作用（第3.3节）。</p>
<p><em>删除Dropout</em>。如第3.4节所述,删除Dropout批标准化实现了一些与Dropout相同的目标。从修改后的BN-Inception状态中删除Dropout可以加快训练速度，而不会增加过度拟合的情况。</p>
<p><em>减少L2全中正则化</em>。虽然在Inception中模型参数的L2损失会控制过拟合，但在修改的BN-Inception中，损失的权重减少了5倍。我们发现这提高了在提供的验证数据上的准确性。</p>
<p><em>加速学习率衰减</em>。在训练Inception时，学习率呈指数衰减。因为我们的网络训练速度比Inception更快，所以我们将学习速度降低加快6倍。</p>
<p><em>删除局部响应归一化</em>。虽然Inception和其它网络（Srivastava等人，2014）从中受益，但是我们发现使用批标准化它是不必要的。</p>
<p><em>更彻底地搅乱训练样本</em>。我们启用了分布内部搅乱训练数据，这样可以防止同一个例子一起出现在一个mini-batch中。这导致验证准确率提高了约1％，这与批标准化作为正则化项的观点是一致的：它每次被看到时都会影响一个样本，在我们的方法中内在的随机化应该是最有益的。</p>
<p><em>减少光照扭曲</em>。因为批标准化网络训练更快，并且观察每个训练样本更少的次数，所以通过更少地扭曲它们，我们让训练器关注更多的“真实”图像。</p>
<h3 id="4-2-2-单网络分类"><a href="#4-2-2-单网络分类" class="headerlink" title="4.2.2. 单网络分类"></a>4.2.2. 单网络分类</h3><p>我们评估了下面的网络，所有的网络都在LSVRC2012训练数据上训练，并在验证数据上测试：</p>
<p><em>Inception</em>：在4.2小节开头描述的网络，以0.0015的初始学习率进行训练。</p>
<p><em>BN-Baseline</em>：每个非线性之前加上批标准化，其它的与Inception一样。</p>
<p><em>BN-x5</em>：带有批标准化的Inception，修改在4.2.1小节中。初始学习率增加5倍到了0.0075。原始Inception增加同样的学习率会使模型参数达到机器无限大。</p>
<p><em>BN-x30</em>：类似于<em>BN-x5</em>，但初始学习率为0.045（Inception学习率的30倍）。</p>
<p><em>BN-x5-Sigmoid</em>：类似于<em>BN-x5</em>，但使用sigmoud非线性$g(t)=\frac1{1+exp(−x)}$来代替ReLU。我们也尝试训练带有sigmoid的原始Inception，但模型保持在相当于机会的准确率。</p>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir2czy07hj30pw0jy0v8.jpg" srcset="/img/loading.gif" alt="image-20200915091032564" style="zoom:50%;" /></center>

<blockquote>
<p>图2。Inception和它的批标准化变种在单个裁剪图像上的验证准确率以及训练步骤的数量。</p>
</blockquote>
<center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir2fiswe9j30oc0gswhk.jpg" srcset="/img/loading.gif" alt="image-20200915091258256" style="zoom:50%;" /></center>

<blockquote>
<p>图3。对于Inception和它的批标准化变种，达到Inception最大准确率(72.2%)所需要的训练步骤数量，以及网络取得的最大准确率。</p>
</blockquote>
<p>在图2中，我们显示了网络的验证集准确率，作为训练步骤次数的函数。Inception网络在31⋅10<sup>6</sup>次训练步骤后达到了72.2％的准确率。图3显示，对于每个网络，达到同样的72.2％准确率需要的训练步骤数量，以及网络达到的最大验证集准确率和达到该准确率的训练步骤数量。</p>
<p>通过仅使用批标准化（BN-Baseline），我们在不到Inception一半的训练步骤数量内将准确度与其相匹配。通过应用4.2.1小节中的修改，我们显著提高了网络的训练速度。<em>BN-x5</em>需要比Inception少14倍的步骤就达到了72.2％的准确率。有趣的是，进一步提高学习率（BN-x30）使得该模型最初训练有点慢，但可以使其达到更高的最终准确率。这种现象是违反直觉的，应进一步调查。在6⋅10<sup>6</sup>步骤之后，BN-x30达到74.8％的准确率，即比Inception达到72.2％的准确率所需的步骤减少了5倍。</p>
<p>我们也证实了尽管训练这样的网络是众所周知的困难，但是当使用sigmoid作为非线性时，内部协变量转移的减少允许具有批标准化的深层网络被训练。的确，BN-x5-Sigmoid取得了69.8％的准确率。没有批标准化，使用sigmoid的Inception从未达到比1/1000准确率更好的结果。</p>
<h3 id="4-2-3-组合分类"><a href="#4-2-3-组合分类" class="headerlink" title="4.2.3. 组合分类"></a>4.2.3. 组合分类</h3><p>目前在ImageNet大型视觉识别竞赛中报道的最佳结果是传统模型（Wu et al。，2015）的Deep Image组合和（He等，2015）的组合模型。后者报告了ILSVRC测试服务器评估的<code>4.94％</code>的top-5错误率。这里我们在测试服务器上报告<code>4.82％</code>的测试错误率。这提高了以前的最佳结果，并且根据（Russakovsky等，2014）这超过了人类评估者的评估准确率。</p>
<p>对于我们的组合，我们使用了6个网络。每个都是基于BN-x30的，进行了以下一些修改：增加卷积层中的初始权重；使用Dropout（丢弃概率为5％或10％，而原始Inception为40％）；模型最后的隐藏层使用非卷积批标准化。每个网络在大约6⋅106个训练步骤之后实现了最大的准确率。组合预测是基于组成网络的预测类概率的算术平均。组合和多裁剪图像推断的细节与（Szegedy et al，2014）类似。</p>
<p>我们在图4中证实了批标准化使我们能够在ImageNet分类挑战基准上设置新的最佳结果。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir2uu2kvcj315k0c6juh.jpg" srcset="/img/loading.gif" alt="image-20200915092741097"></p>
<blockquote>
<p>图4。批标准化Inception与以前的最佳结果在提供的包含5万张图像的验证集上的比较。组合结果是在测试集上由测试服务器评估的结果。BN-Inception组合在验证集的5万张图像上取得了<code>4.9% top-5</code>的错误率。所有报道的其它结果是在验证集上。</p>
</blockquote>
<h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h1><p>我们提出了一种新颖的机制，大大加快了深度网络的训练。他是基于前提协变量转移的，已知其会使机器学习系统的训练复杂化，也适用于子网络和层，并且从网络的内部激活中去除它可能有助于训练。这确保了标准化可以被用来训练网络的任何优化方法进行恰当的处理。为了让深度网络训练中常用的随机优化方法可用，我们对每个小批量数据执行标准化，并通过标准化参数来反向传播梯度。批标准化每个激活只增加了两个额外的参数，这样做可以保持网络的表示能力。我们提出了一个用于构建，训练和执行推断的批标准化网络算法。所得到的网络可以用饱和非线性进行训练，能更容忍增加的训练率，并且通常不需要Dropout来进行正则化。</p>
<p>仅仅将批标准化添加到了最新的图像分类模型中便在训练中取得了实质的加速。通过进一步提高学习率，删除Dropout和应用批标准化所提供的其它修改，我们只用了少部分的训练步骤就达到了以前的技术水平——然后在单网络图像分类中击败了最先进的技术。此外，通过组合多个使用批标准化训练的模型，我们在ImageNet上的表现显著优于最好的已知系统。</p>
<p>我们的方法与（Gülçehre＆Bengio，2013）的标准化层相似，尽管这两个方法解决的目标不同。批标准化寻求在整个训练过程中激活值的稳定分布，并且对非线性的输入进行归一化，因为这时更有可能稳定分布。相反，标准化层被应用于非线性的输出，这导致了更稀疏的激活。我们没有观察到非线性输入是稀疏的，无论是有批标准化还是没有批标准化。批标准化的其它显著差异包括学习到的缩放和转移允许BN变换表示恒等，卷积层处理以及不依赖于小批量数据的确定性推断。</p>
<p>在这项工作中，我们没有探索批标准化可能实现的全部可能性。我们的未来工作包括将我们的方法应用于循环神经网络（Pascanu et al.，2013），其中内部协变量转移和梯度消失或爆炸可能特别严重，这将使我们能够更彻底地测试假设标准化改善了梯度传播（第3.3节）。需要对批标准化的正则化属性进行更多的研究，我们认为这是BN-Inception中删除丢弃时我们观察到的改善的原因。我们计划调查批标准化是否有助于传统意义上的域自适应——即网络执行标准化是否能够更容易泛化到新的数据分布，也许仅仅是对总体均值和方差的重新计算（Alg.2）。最后，我们认为，该算法的进一步理论分析将允许更多的改进和应用。</p>
<blockquote>
<p>本文转自：<a href="https://blog.csdn.net/quincuntial/article/details/78124582" target="_blank" rel="noopener">https://blog.csdn.net/quincuntial/article/details/78124582</a></p>
<p>作者：Tyan</p>
</blockquote>
<h1 id="附：原文"><a href="#附：原文" class="headerlink" title="附：原文"></a>附：原文</h1>

	<div class="row">
    <embed src="/pdf/GoogLeNetV2.pdf" width="100%" height="550" type="application/pdf">
	</div>




            </article>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/%E6%96%87%E7%8C%AE%E7%BF%BB%E8%AF%91/">文献翻译</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/GoogLeNet/">GoogLeNet</a>
                    
                      <a class="hover-with-bg" href="/tags/Batch-Normalization/">Batch Normalization</a>
                    
                  </div>
                
              </div>
              
              
                <div class="post-prevnext row">
                  <div class="post-prev col-6">
                    
                    
                      <a href="/2020/09/18/9-18/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">阅读笔记（9.18)</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </div>
                  <div class="post-next col-6">
                    
                    
                      <a href="/2020/09/04/9-4/">
                        <span class="hidden-mobile">阅读笔记（9.4)</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </div>
                </div>
              
            </div>

            
              <!-- Comments -->
              <div class="comments" id="comments">
                
                
  <div id="vcomments"></div>
  <script type="text/javascript">
    function loadValine() {
      addScript('https://cdn.staticfile.org/valine/1.4.14/Valine.min.js', function () {
        new Valine({
          el: "#vcomments",
          app_id: "bnCEF7PLYkERuDi9gYGIAK1q-gzGzoHsz",
          app_key: "ohFc9mmlCQxYi22T4AMQA2JY",
          placeholder: "说点什么吧~（请在上方填写您的昵称，昵称将显示在你的评论上）",
          path: window.location.pathname,
          avatar: "identicon",
          meta: ["nick","mail","link"],
          pageSize: "10",
          lang: "zh-CN",
          highlight: false,
          recordIP: true,
          serverURLs: "",
        });
      });
    }
    createObserver(loadValine, 'vcomments');
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://valine.js.org" target="_blank" rel="nofollow noopener noopener">comments
      powered by Valine.</a></noscript>


              </div>
            
          </div>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div id="tocbot"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    
  </main>

  
    <a id="scroll-top-button" href="#" role="button">
      <i class="iconfont icon-arrowup" aria-hidden="true"></i>
    </a>
  

  
    <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
  

  

  

  <footer class="mt-5">
  <div class="text-center py-3">

    
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>



    <div>
      <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
      <script>
          var now = new Date();
          function createtime() {
              var grt= new Date("06/22/2020 00:00:00");
              now.setTime(now.getTime()+250);
              days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
              hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
              if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
              mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
              seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
              snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
              document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 ";
              document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
          }
          setInterval("createtime()",250);
      </script>
    </div>


    <div>
      <span id="Copyright">载入版权...</span>
      <script>
          var now = new Date();
          function createtime2() {
              var year = now.getFullYear();
              document.getElementById("Copyright").innerHTML = "Copyright © "+year+" htx's Blog, All rights reserved.";
          }
          setInterval("createtime2()",250);
      </script>
    </div>


    
  <!-- 备案信息 -->
  <div class="beian">
    <a href="http://beian.miit.gov.cn/" target="_blank"
       rel="nofollow noopener">豫ICP备2020026254号</a>
    
  </div>


    

  </div>
</footer>

<!-- SCRIPTS -->
<script  src="https://cdn.staticfile.org/jquery/3.4.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.4.1/js/bootstrap.min.js" ></script>
<script  src="/js/debouncer.js" ></script>
<script  src="/js/main.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/lazyload.js" ></script>
  



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>
  <script  src="/js/clipboard-use.js" ></script>



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





  <script  src="https://cdn.staticfile.org/tocbot/4.11.1/tocbot.min.js" ></script>
  <script>
    $(document).ready(function () {
      var boardCtn = $('#board-ctn');
      var boardTop = boardCtn.offset().top;

      tocbot.init({
        tocSelector: '#tocbot',
        contentSelector: 'article.markdown-body',
        headingSelector: 'h1,h2,h3,h4,h5,h6',
        linkClass: 'tocbot-link',
        activeLinkClass: 'tocbot-active-link',
        listClass: 'tocbot-list',
        isCollapsedClass: 'tocbot-is-collapsed',
        collapsibleClass: 'tocbot-is-collapsible',
        collapseDepth: 0,
        scrollSmooth: true,
        headingsOffset: -boardTop
      });
      if ($('.toc-list-item').length > 0) {
        $('#toc').css('visibility', 'visible');
      }
    });
  </script>



  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    var typed = new Typed('#subtitle', {
      strings: [
        '  ',
        "文献翻译——Batch Normalization(GoogLeNet-V2)&nbsp;",
      ],
      cursorChar: "_",
      typeSpeed: 70,
      loop: false,
    });
    typed.stop();
    $(document).ready(function () {
      $(".typed-cursor").addClass("h2");
      typed.start();
    });
  </script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.2.2/anchor.min.js" ></script>
  <script>
    anchors.options = {
      placement: "right",
      visible: "hover",
      
    };
    var el = "h1,h2,h3,h4,h5,h6".split(",");
    var res = [];
    for (item of el) {
      res.push(".markdown-body > " + item)
    }
    anchors.add(res.join(", "))
  </script>



  <script  src="/js/local-search.js" ></script>
  <script>
    var path = "/local-search.xml";
    var inputArea = document.querySelector("#local-search-input");
    inputArea.onclick = function () {
      searchFunc(path, 'local-search-input', 'local-search-result');
      this.onclick = null
    }
  </script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />

  <script>
    $('#post img:not(.no-zoom img, img[no-zoom]), img[zoom]').each(
      function () {
        var element = document.createElement('a');
        $(element).attr('data-fancybox', 'images');
        $(element).attr('href', $(this).attr('src'));
        $(this).wrap(element);
      }
    );
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.staticfile.org/mathjax/3.0.5/es5/tex-svg.js" ></script>

  



  
  
    <script type="text/javascript">
      //定义获取词语下标
      var a_idx = 0;
      jQuery(document).ready(function ($) {
        //点击body时触发事件
        $("body").click(function (e) {
          //需要显示的词语
          var a = new Array("富强", "民主", "文明", "和谐", "自由", "平等", "公正", "法治", "爱国", "敬业", "诚信", "友善");
          //设置词语给span标签
          var $i = $("<span/>").text(a[a_idx]);
          //下标等于原来下标+1  余 词语总数
          a_idx = (a_idx + 1) % a.length;
          //获取鼠标指针的位置，分别相对于文档的左和右边缘。
          //获取x和y的指针坐标
          var x = e.pageX, y = e.pageY;
          //在鼠标的指针的位置给$i定义的span标签添加css样式
          $i.css({
            "z-index": 999,
            "top": y - 20,
            "left": x,
            "position": "absolute",
            "font-weight": "bold",
            "color": rand_color()
          });
          // 随机颜色
          function rand_color() {
            return "rgb(" + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + "," + ~~(255 * Math.random()) + ")"
          }
          //在body添加这个标签
          $("body").append($i);
          //animate() 方法执行 CSS 属性集的自定义动画。
          //该方法通过CSS样式将元素从一个状态改变为另一个状态。CSS属性值是逐渐改变的，这样就可以创建动画效果。
          //详情请看http://www.w3school.com.cn/jquery/effect_animate.asp
          $i.animate({
            //将原来的位置向上移动180
            "top": y - 180,
            "opacity": 0
            //1500动画的速度
          }, 1500, function () {
            //时间到了自动删除
            $i.remove();
          });
        });
      })
      ;
    </script>
  











  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?2f3f98d16f957573ec883289e3293112";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  

  

  

  

  





<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
</body>
</html>
