<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>模仿航拍中国片头，向航拍中国致敬！</title>
    <link href="/2022/11/24/cover%E8%88%AA%E6%8B%8D%E4%B8%AD%E5%9B%BD%E7%89%87%E5%A4%B4/"/>
    <url>/2022/11/24/cover%E8%88%AA%E6%8B%8D%E4%B8%AD%E5%9B%BD%E7%89%87%E5%A4%B4/</url>
    
    <content type="html"><![CDATA[<p>《航拍中国》第四季收官之作即将播出，老家河南压轴出场！从2016年拥有第一架无人机算起，玩航拍已经6年有余，无人机就像我的另一只眼睛，带我从高空俯瞰熟悉的世界。我整理了6年来拍摄的近千GB素材，模仿《航拍中国》片头剪成了这个短片，向《航拍中国》致敬！</p><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=389757651&bvid=BV16d4y1C7th&cid=884330569&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" > </iframe></div>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
      <category>旅行日记</category>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>热烈庆祝博主的论文被IEEE ICME2022接收！</title>
    <link href="/2022/07/27/ICME/"/>
    <url>/2022/07/27/ICME/</url>
    
    <content type="html"><![CDATA[<p>热烈庆祝本人攻硕期间发表的第一篇论文被IEEE ICME2022国际会议录用！</p><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1h4lsump6yej20u01310vt.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>作为一枚零写作经验的科研萌新第一次投稿就被CCF-B类会议成功收录，这是如此的幸运。<br>感谢实验室的各位老师的悉心指导，特别感谢qq师兄的无私帮助。祝大家科研顺利，多发paper！</p>]]></content>
    
    
    <categories>
      
      <category>论文发表</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ICME</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>听，大海的声音！——厦门音乐广场航拍</title>
    <link href="/2022/01/01/%E9%9F%B3%E4%B9%90%E5%B9%BF%E5%9C%BA%E8%88%AA%E6%8B%8D/"/>
    <url>/2022/01/01/%E9%9F%B3%E4%B9%90%E5%B9%BF%E5%9C%BA%E8%88%AA%E6%8B%8D/</url>
    
    <content type="html"><![CDATA[<p>视频拍摄于2021年11月的厦门音乐广场，分享给大家，预祝我们2022年生龙活虎、虎虎生威！</p><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=891819254&bvid=BV16P4y1G7tJ&cid=447115506&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" > </iframe></div>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
      <category>旅行日记</category>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>双子塔灯光秀航拍——庆祝建党百年</title>
    <link href="/2021/06/30/%E5%8F%8C%E5%AD%90%E5%A1%94%E7%81%AF%E5%85%89%E7%A7%80/"/>
    <url>/2021/06/30/%E5%8F%8C%E5%AD%90%E5%A1%94%E7%81%AF%E5%85%89%E7%A7%80/</url>
    
    <content type="html"><![CDATA[<p>庆祝中国共产党成立100周年——“鹭飞花舞，百年欢庆”主题灯光秀在鹭江道片区上演，厦门用璀璨的光影点亮鹭岛之夜。6.27日晚我骑车到鹭江道航拍了双子塔的灯光秀，剪辑成了一分多钟的小片子，并被厦门晚报录用。</p><p>生逢盛世当不负盛世，祝伟大的党生日快乐！——9514.8万分之一</p><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=631353101&bvid=BV16b4y1C7eS&cid=360748769&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" > </iframe></div>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
      <category>旅行日记</category>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XMU&amp;双子塔航拍</title>
    <link href="/2021/04/24/XMU%E8%88%AA%E6%8B%8D/"/>
    <url>/2021/04/24/XMU%E8%88%AA%E6%8B%8D/</url>
    
    <content type="html"><![CDATA[<p>上周末和百年校庆期间拍的一些航拍素材，今天抽空大概剪了一个小片子。</p><p>以后有空的时候多拍些素材（航拍、延时摄影），攒到毕业应该可以出个短片了。弥补一下因为疫情没给ZZU拍短片的遗憾。</p><p>P.S: 电脑版的剪映挺好用的，感觉不输Final Cut Pro，喜欢剪辑的小伙伴可以尝试一下，比PR、Edius等专业软件容易上手。</p><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=930106302&bvid=BV1KK4y1o7qV&cid=324628526&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" > </iframe></div>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
      <category>旅行日记</category>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>纪录片《一级响应》(5集全)</title>
    <link href="/2021/04/12/yjxy/"/>
    <url>/2021/04/12/yjxy/</url>
    
    <content type="html"><![CDATA[<p>最近在看一部抗疫纪录片《一级响应》，拍的十分真实、感人，片子真实的记录了2020年初的武汉，看完之后感触非常深。武汉不愧为一座英雄的城市，武汉人民不愧为英雄的人民！</p><p>国家广播电视总局重点指导项目——五集纪录片《一级响应》于4月8日起在湖北卫视、东方卫视播出。</p><p>2020年一月，病毒在武汉从初露头角，到突然爆发，再到武汉在春节期间封城。二月，多方援助抵达，方舱医院建立，除了病人和医生，全国人民同武汉一起共克时艰。三月，春暖花开，疫情控制有力，生活开始出现转机。四月武汉重生，但是围绕着人们如何恢复正常生活秩序的新问题涌现。五月，走出身体问题的人们开始要面对心理的重建，以及当事情开始逐步风情浪静，开始反思疫情，总结经验，援助国外。</p><p style="text-align: center;"><iframe  src='http://app.cjyun.org/video/player/index?vid=397932&amp;thumb=http%3A%2F%2Fimg.cjyun.org%2Fa%2F10008%2F202104%2Fce16d98646c1c480fb23c2539e14b2c8.jpg&amp;sid=10008&amp;next=&amp;autoStart=0&amp;type=&amp;customid=8992' frameborder=0 width='550' height='414' allowfullscreen></iframe></p><p style="text-align: center;"><span style="font-size: 18px;">纪录片《一级响应》第一集 完整版</span></p><p><span style="font-size: 18px;"></span></p><p style="text-align: center;"><iframe  src='http://app.cjyun.org/video/player/index?vid=397931&amp;thumb=http%3A%2F%2Fimg.cjyun.org%2Fa%2F10008%2F202104%2Fa0a2cad45f3ea62cff228408b881614e.jpg&amp;sid=10008&amp;next=&amp;autoStart=0&amp;type=&amp;customid=8992' frameborder=0 width='550' height='414' allowfullscreen></iframe></p><p style="text-align: center;"><span style="font-size: 18px;">纪录片《一级响应》第二集 完整版</span></p><p><span style="font-size: 18px;"></span></p><p style="text-align: center;"><iframe  src='http://app.cjyun.org/video/player/index?vid=398043&amp;thumb=http%3A%2F%2Fimg.cjyun.org%2Fa%2F10008%2F202104%2F62749e8db71d87f7ed773f69639f08e6.jpg&amp;sid=10008&amp;next=&amp;autoStart=0&amp;type=&amp;customid=8992' frameborder=0 width='550' height='414' allowfullscreen></iframe></p><p style="text-align: center;"><span style="font-size: 18px;">纪录片《一级响应》第三集 完整版</span></p><p><span style="font-size: 18px;"></span><span style="font-size: 18px;"></span></p><p style="text-align: center;"><iframe  src='http://app.cjyun.org/video/player/index?vid=398042&amp;thumb=http%3A%2F%2Fimg.cjyun.org%2Fa%2F10008%2F202104%2Fc44fbddc459ccb55dea4fc30f7b8629e.jpg&amp;sid=10008&amp;next=&amp;autoStart=0&amp;type=&amp;customid=8992' frameborder=0 width='550' height='414' allowfullscreen></iframe></p><p style="text-align: center;"><span style="font-size: 18px;">纪录片《一级响应》第四集 完整版</span></p><p><span style="font-size: 18px;"></span></p><p style="text-align: center;"><iframe  src='http://app.cjyun.org/video/player/index?vid=398041&amp;thumb=http%3A%2F%2Fimg.cjyun.org%2Fa%2F10008%2F202104%2F88f6967f0e95c876148e219047e3417c.jpg&amp;sid=10008&amp;next=&amp;autoStart=0&amp;type=&amp;customid=8992' frameborder=0 width='550' height='414' allowfullscreen></iframe></p><p style="text-align: center;"><span style="font-size: 18px;">纪录片《一级响应》第五集 完整版</span></p><p><span style="font-size: 18px;"><br></span></p><p style="text-align: right;"><span style="font-size: 18px;">（来源：湖北广播电视台纪录片部）</span></p><blockquote><p>原载于：<a href="http://news.hbtv.com.cn/appgd/p/1960372.html" target="_blank" rel="noopener">http://news.hbtv.com.cn/appgd/p/1960372.html</a></p></blockquote>]]></content>
    
    
    <categories>
      
      <category>纪录片</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（4.2)</title>
    <link href="/2021/04/02/4-2/"/>
    <url>/2021/04/02/4-2/</url>
    
    <content type="html"><![CDATA[<h2 id="论文阅读方面："><a href="#论文阅读方面：" class="headerlink" title="论文阅读方面："></a>论文阅读方面：</h2><p>[1] Chefer H, Gur S, Wolf L. Transformer Interpretability Beyond Attention Visualization[J]. arXiv preprint arXiv:2012.09838, 2020.</p><p>[2] Carion N, Massa F, Synnaeve G, et al. End-to-end object detection with transformers[C]//European Conference on Computer Vision. Springer, Cham, 2020: 213-229.</p><p>[3] Wang Y, Xu Z, Wang X, et al. End-to-End Video Instance Segmentation with Transformers[J]. arXiv preprint arXiv:2011.14503, 2020.</p><h2 id="代码执行方面："><a href="#代码执行方面：" class="headerlink" title="代码执行方面："></a>代码执行方面：</h2><p>本周对[1]这篇文章的代码进行了复现，在我们的ImageNet VIDs数据集上进行了测试，效果如下：</p><p><img src="https://i.loli.net/2021/04/02/gwCWGoJfVUXiQvI.png" srcset="/img/loading.gif" alt="image-20210402162802646"></p><p>其中，第一列是原图，第2-6列是模型预测出的得分最高的前5类的Heatmap，最后一列是我加的，是对前5列图像的融合。为什么要做融合呢？是因为该方法对于目标的分类过于详细，光熊就分为了棕熊、美国黑熊、北极熊等等…在我们的数据集中并没有分的这么详细，进行融合就可以保证最终提取出感兴趣区域的效果。</p><h4 id="自己的想法："><a href="#自己的想法：" class="headerlink" title="自己的想法："></a><strong>自己的想法：</strong></h4><ul><li>未来考虑如何将本方法应用于视频目标检测中去，比如对原始特征图用该热图进行增强？这是目前的初步想法，未来计划做实验进行验证。</li></ul><h1 id="1-Transformer-Interpretability-Beyond-Attention-Visualization"><a href="#1-Transformer-Interpretability-Beyond-Attention-Visualization" class="headerlink" title="1.Transformer Interpretability Beyond Attention Visualization"></a>1.Transformer Interpretability Beyond Attention Visualization</h1><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>自注意力机制，特别是Transformer逐渐成为文本处理领域的主流，并且在CV领域流行起来。为了可视化图像中有助于分类的部分，现有方法依赖于所获取到的注意力图，或者沿着注意力图进行启发式传播。本文提出了一种新的方法来为Transformer计算相关性。该方法基于深度泰勒分解规则分配局部相关性，之后传播这些相关性。该传播包含注意力层和跳跃连接。作者的解决方案基于一个特定的公式，该公式显示了跨层保持总体相关性。</p><p>Transformer网络的主要构成是自注意力层，其在两个tokens之间分配一个成对的注意力值。</p><ul><li>代码地址：<a href="https://github.com/hila-chefer/Transformer-Explainability" target="_blank" rel="noopener">https://github.com/hila-chefer/Transformer-Explainability</a></li></ul><h4 id="本文提出的可视化方法："><a href="#本文提出的可视化方法：" class="headerlink" title="本文提出的可视化方法："></a>本文提出的可视化方法：</h4><p><img src="https://i.loli.net/2021/03/26/fg6OuBk9csyiUAh.png" srcset="/img/loading.gif" alt="image-20210326093924093"></p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><ul><li>本文提出了一种基于Transformer的方法将有助于分类的部分进行可视化。</li><li>其效果相对之前的方法有很大提高，并且是开源的，可以考虑将该方法用于VID</li></ul><h1 id="2-End-to-end-object-detection-with-transformers"><a href="#2-End-to-end-object-detection-with-transformers" class="headerlink" title="2.End-to-end object detection with transformers"></a>2.End-to-end object detection with transformers</h1><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>作者将目标检测框看为一个直接的集合预测问题，简化了检测的pipeline,有效的移除了需要手动设置的内容，如非极大值抑制、anchor生成，这些内容包含了我们对于任务的先验知识。作者提出的新框架的主要成分是DETR，其是一个基于集合的全局loss，强制通过双向匹配统一的进行预测，并且是一个Transformer Encoder-decoder结构。给出一个固定的所学习到的目标queries的小集合，DETR可以推出目标的关系、全局图像上下文，来直接并行输出最终的预测结果集合。</p><ul><li>代码地址：<a href="https://github.com/facebookresearch/detr" target="_blank" rel="noopener">https://github.com/facebookresearch/detr</a></li></ul><h3 id="DETR模型"><a href="#DETR模型" class="headerlink" title="DETR模型"></a>DETR模型</h3><p>在检测中直接进行集合预测有两个因素必不可少：1.一个集合预测Loss，强制其在预测和GT box之间唯一匹配。2.一个预测一组目标并对其关系建模的结构。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEgy1gpkrxugvjxj30tn07tmzu.jpg" srcset="/img/loading.gif" alt=""></p><blockquote><p>DETR使用CNN作为backbone来学习输入图像的特征，之后将其压平，与positional encoding合并后送入Transformer的编码器。解码器以object queries以及编码器的输出作为输入。最后将解码器的输出送入一个共享的前向传播网络(FFN)来预测边界框的类别和位置。</p></blockquote><h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h2><ul><li>本文最大的创新点是摒弃了原有的CNN网络，使用Transformer做目标检测。</li><li>文中提到该方法可适用于任何深度学习框架，并且核心代码仅有几百行（Pytorch）。</li><li>缺点是对小目标的检测效果不尽如人意，还有继续优化的空间。</li></ul><h1 id="3-End-to-End-Video-Instance-Segmentation-with-Transformers"><a href="#3-End-to-End-Video-Instance-Segmentation-with-Transformers" class="headerlink" title="3. End-to-End Video Instance Segmentation with Transformers"></a>3. End-to-End Video Instance Segmentation with Transformers</h1><h3 id="简介："><a href="#简介：" class="headerlink" title="简介："></a>简介：</h3><p>视频实例分割（VIS）需要同时对视频中的实例进行进行分类、分割、跟踪。本文在Transformer之上提出了一个新的视频实例分割框架：VisTR，其将VIS任务看作一个直接的端到端并行序列解码/预测问题。其核心是一个高效的实例序列匹配和分割策略，它作为一个整体在序列级别上监督和分段实例。VisTR对实例分割进行帧化处理，并在相同的相似性学习视角进行跟踪，因此简化了pipeline，与现有方法有很大不同。</p><p>本质上，实例分割和实例跟踪都与相似性学习有关：实例分割学习像素级别的相似性，实例跟踪学习多个实例之间的相似性。于是很自然的想法是：在单个框架内解决这两个子任务，其还可以相互促进。</p><ul><li>代码地址：<a href="https://github.com/Epiphqny/VisTR" target="_blank" rel="noopener">https://github.com/Epiphqny/VisTR</a></li></ul><h2 id="VisTR"><a href="#VisTR" class="headerlink" title="VisTR"></a>VisTR</h2><p><img src="https://i.loli.net/2021/03/30/1VpvWgnNC7SawuY.png" srcset="/img/loading.gif" alt="image-20210330180022972"></p><p>整体结构如图所示，首先使用标准的CNN模块独立的提取每一帧的特征，之后按照顺序串联图像特征形成clip-level特征序列。第二步，将clip-level的特征序列输入Transformer，输出一个有序的目标预测序列。</p><p>图中相同的形状表示相同图像的预测结果，相同的颜色代表不同图像的相同实例。预测序列的顺序与输入图像的顺序相同，并且每一帧内的实例预测顺序也相同。因此，视力跟踪在实例分割的框架中被无缝的、自然的实现。</p><p>为了实现上述目标，有两个主要挑战：</p><ol><li>如何维持输出的顺序。</li><li>如何从Transformer网络的输出获得每个实例mask序列。</li></ol><p>作者针对这两个问题，分别提出了instance sequence matching 策略、instance sequence segmentation模块。</p><p>instance sequence matching 策略在输出实例序列 和GT实例序列之间，执行双向的图匹配，从宏观上监督顺序。Instance sequence segmentation通过自注意力为每个实例跨多帧积累mask特征，并且通过3D卷积对每个实例的mask序列进行分割。作者展示了位置信息对于VIS密集预测任务的重要性，并且分析了实例查询嵌入在不同级别的影响。</p><p>原始Transformer是auto-regressive（自回归）的，一个一个的生成token。为了效率，作者采用了一个非自回归的Transformer变体，来实现并行序列生成。</p><h2 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h2><h4 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h4><ul><li>基于Transformer提出了一个新的视频实例分割框架VisTR，将VIS任务看做一个直接的端到端的平行序列解码/预测问题。</li><li>VisTR从一个全新的相似性学习的视角解决VIS问题。在一个相同的框架内实现了分割和跟踪。</li><li>VisTR提出了两个新的策略：instance sequence matching &amp; segmentation。这两个策略使得可在整个序列级别上监督和分割实例。</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer Interpretability</tag>
      
      <tag>DETR</tag>
      
      <tag>VisTR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>厦门大学信息学院计算机科学系硕士生2020级1班党支部开展清明祭扫活动</title>
    <link href="/2021/03/29/qmjs/"/>
    <url>/2021/03/29/qmjs/</url>
    
    <content type="html"><![CDATA[<blockquote><p>原载于：<a href="https://informatics.xmu.edu.cn/info/1004/16452.htm" target="_blank" rel="noopener">https://informatics.xmu.edu.cn/info/1004/16452.htm</a></p></blockquote><p>​    清明前夕，厦门大学信息学院计算机科学系硕士生2020级1班党支部组织党员前往革命烈士陵园祭扫，深切缅怀革命先烈，弘扬践行革命精神。</p><p>​      2021年3月27日下午，支部党员来到思明校区内的罗扬才烈士陵园，在罗扬才烈士的雕像前肃立默哀，并怀着敬畏怀念之心献上鲜花。鲜花在微风中轻柔地舒展，是同志们对罗扬才烈士最深切的缅怀。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15mnvbvyj30oj0id47p.jpg" srcset="/img/loading.gif" alt="img"></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15mqqz0cj30oh0ic46h.jpg" srcset="/img/loading.gif" alt="img"></p><p>​      随后，大家前往厦门革命烈士陵园，参观革命烈士事迹陈列馆。支部党员通过馆内陈列的日记、遗物和奖章，认真学习先烈们的英勇事迹，细细追寻革命烈士血与火的足迹，用心回味关于英烈们的历历往事。在场的党员们一边参观，一边低声讨论，大家都被先烈们的壮举及高尚的道德情操深深震撼。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15mpvgb4j30ro0krn2x.jpg" srcset="/img/loading.gif" alt="img"></p><p>​      参观活动结束后，支部党员自发来到庄严的革命烈士纪念碑前，肃立默哀、敬献鲜花，表达对先烈的崇高敬意。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15mrme5tj30sl0leakn.jpg" srcset="/img/loading.gif" alt="img"></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15mmo4cgj30bj0bkn2q.jpg" srcset="/img/loading.gif" alt="img"></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15moxl8fj30qd0jsqdo.jpg" srcset="/img/loading.gif" alt="img"></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gp15mnb3uvj30t70lw49i.jpg" srcset="/img/loading.gif" alt="img"></p><p>​      庄严的仪式让支部党员再一次体会到党员身份的光荣和党员使命的崇高。大家纷纷表示：在今后的科研和学习中，将进一步端正态度，牢记初心使命，传承红色基因，做到学史增信，学史崇德，学史力行，以优异的成绩迎接建党、立校“双百年”。</p><p align="right">信息学院计算机科学系硕士生2020级1班党支部</p><p><p align="right">2021年3月29日 <p/></p>]]></content>
    
    
    <categories>
      
      <category>支部新闻</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>图像目标检测简单综述</title>
    <link href="/2021/03/29/%E5%9B%BE%E5%83%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/"/>
    <url>/2021/03/29/%E5%9B%BE%E5%83%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="目标检测器的构成（YOLOv4）："><a href="#目标检测器的构成（YOLOv4）：" class="headerlink" title="目标检测器的构成（YOLOv4）："></a>目标检测器的构成（YOLOv4）：</h1><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0wtzqztoj30hp08kjrv.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0wtzqztoj30hp08kjrv.jpg" srcset="/img/loading.gif" alt="img"></a></p><ul><li>Backbone:进行预训练。</li><li><p>Neck:从不同的步骤中收集特征图</p><ul><li>Feature Pyramid Network（FPN）,Path Aggregation Network（PAN），BiFPN和NAS-FPN。</li><li>一个neck由多个bottom-up路径和多个top-down路径组成</li></ul></li><li><p>Head:预测类别信息和目标物体的边界框。</p><ul><li>One-stage:<ul><li>YOLO, SSD和RetinaNet</li><li>Anchor-free:CenterNet, CornerNet, FCOS</li></ul></li><li>Two-stage:<ul><li>R-CNN系列（fast R-CNN, faster R-CNN,R-FCN和Libra R-CNN）</li><li>Anchor-free:RepPoints</li></ul></li></ul></li></ul><p><strong>检测器通常由两部分组成：</strong>backbone<strong>和</strong>head。前者在ImageNet上进行预训练，后者用来预测类别信息和目标物体的边界框。在GPU平台上运行的检测器，它们的backbone可能是VGG, ResNet, ResNetXt,或者是DenseNet。在CPU平台上运行的检测器，它们的backbone可能是SqueezeNet，MobileNet或者是ShuffleNet。对于head部分，通常分为两类：one-stage和two-stage的目标检测器。Two-stage的目标检测器的代表是R-CNN系列，包括：fast R-CNN, faster R-CNN,R-FCN和Libra R-CNN. 还有基于anchor-free的Two-stage的目标检测器，比如RepPoints。One-stage目标检测器的代表模型是YOLO, SSD和RetinaNet。在最近几年，出现了基于anchor-free的one-stage的算法，比如CenterNet, CornerNet, FCOS等等。在最近几年，目标检测器在backbone和head之间会插入一些网络层，这些网络层通常用来从不同的步骤中收集特征图。我们将其称之为目标检测器的neck。通常，一个neck由多个bottom-up路径和多个top-down路径组成。使用这种机制的网络包括Feature Pyramid Network（FPN）,Path Aggregation Network（PAN），BiFPN和NAS-FPN。</p><p>一个普通的目标检测器由下面四个部分组成：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0wwndik4j30qq0wqtem.jpg" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gknafkbowbj31jp0u0tq2.jpg" srcset="/img/loading.gif" alt="image-20201113092812981"></p><h1 id="1-two-stage-（基于候选窗口）"><a href="#1-two-stage-（基于候选窗口）" class="headerlink" title="1.two-stage:（基于候选窗口）"></a>1.two-stage:（基于候选窗口）</h1><p>提取候选区域、CNN训练 两步</p><h2 id="1-1-Selective-Search"><a href="#1-1-Selective-Search" class="headerlink" title="1.1 Selective Search:"></a>1.1 Selective Search:</h2><p>R-CNN和Fast R-CNN使用该方法提取候选窗口。</p><ul><li>传统目标检测算法使用穷举法或滑动窗口法来产生候选窗口，产生的冗余候选区域较多。</li><li>Selective Search方法框架：<ul><li>输入：彩色图片。</li><li>输出：物体可能的位置，实际上是很多的矩形坐标。</li><li>首先，将图片初始化为很多小区域R。初始化一个相似集合为空集：S</li><li>计算所有相邻区域之间的相似度（考虑了颜色、纹理、尺寸和空间交叠4个参数），放入集合 S 中，集合 S 保存的是一个区域对以及它们之间的相似度。</li><li>找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空。</li><li>从 R 中找出所有区域的 bounding box（即包围该区域的最小矩形框），这些 box 就是物体可能的区域。</li></ul></li></ul><h2 id="1-2-R-CNN"><a href="#1-2-R-CNN" class="headerlink" title="1.2 R-CNN"></a>1.2 R-CNN</h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bpccrmj30v20ba0vf.jpg" srcset="/img/loading.gif" alt="img"></p><ul><li>R-CNN系列算法由3个独立的步骤组成：产生候选窗口、特征提取、SVM分类及窗口回归。</li><li><strong>R-CNN总体思路：</strong> <strong>通过Selective Search检测出约2000个Region proposals ——&gt; 使用CNN提取Region特征——&gt;使用SVM对提取到的特征进行分类 ——&gt;执行非极大值抑制得到Bbox及其类别</strong></li><li>细节：<ul><li>R-CNN采用Selective Search生成候选区域</li><li>R-CNN采用AlexNet网络</li><li>先在ImageNet上对AlexNet进行预训练，然后利用成熟的权重参数在PASCAL VOC上进行fine-tune.</li><li>R-CNN在Pascal VOC 2012 的数据集上，能够将目标检测的验证指标 mAP 提升到 53.3%,这相对于之前最好的结果提升了 30%。</li></ul></li><li>缺点：<ul><li>训练时间长：分阶段多次训练，而且对每个候选区域都要单独计算一次Feature map。</li><li>占用空间大：每个候选区域的Feature map都要写入磁盘保存。</li><li>步骤繁多：整个模型包括多个模块，且每个模块是相互独立的。</li><li>由于全连接层限制，输入图像为固定尺寸。</li><li>每个图像块输入CNN单独处理，无特征提取共享。</li><li>重复计算：多数候选区域是互相重叠的，重叠部分会被多次提取Feature。</li></ul></li></ul><blockquote><p>Fast R-CNN提出了一个ROI池化层，其实质上是SPP-Net的一个特殊情况（故在介绍Fast R-CNN之前，先介绍SPP-Net）</p></blockquote><h2 id="1-3-SPP-Net"><a href="#1-3-SPP-Net" class="headerlink" title="1.3 SPP Net"></a>1.3 SPP Net</h2><h3 id="动机："><a href="#动机：" class="headerlink" title="动机："></a>动机：</h3><ul><li>CNN一般都含有卷积层和全连接层，卷积层无需固定尺寸的图像，而<strong>全连接层则需要固定大小的输入</strong>。</li><li>在此之前，普通的CNN若想实现输出一个固定维度的向量到全连接层，必须<strong>固定输入图像的尺寸</strong>。在希望检测各种大小的图片的时候，需要经过crop，或者warp等一系列操作，这都在一定程度上导致图片信息的丢失和变形，限制了识别精确度。而SPP-Net在普通的CNN中加入了SPP层，使得输入图像可以是<strong>任意尺寸</strong>的，输出为<strong>固定维度的向量</strong>。</li><li><img src="C:\Users\admin\AppData\Roaming\Typora\typora-user-images\image-20201116142426267.png" srcset="/img/loading.gif" alt="image-20201116142426267"></li></ul><h3 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h3><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3boaucnj30g70dftat.jpg" srcset="/img/loading.gif" alt="img"></p><ul><li>输入：任意尺度的待测图像</li><li>输出：固定长度的特征向量。</li><li>黑色图片代表卷积后的特征图，将4x4、2x2、1x1的网格放在特征图上，可以得到16+4+1=21个小方格。对每个小方格执行Max-Pooling下采样，提取出1个特征值。将每个小方格提取到的特征值组合在一起，就得到一个长度为21维的特征向量。（特别的，如最后的卷积层Conv5有256个通道，所以输出（16+4+1）*256的特征）</li></ul><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><ul><li>SPP-Net使得任意大小的特征图都能够转换成固定大小的特征向量。</li><li>只对原图做一次卷积，复用卷积层的特征图，提高速度。<ul><li>由于原图与经过卷积层后得到的特征图在空间位置上存在一定的对应关系，所以只需对整张图像进行一次卷积特征提取，然后将候选区域在原图的位置映射到卷积层的特征图上得到该候选区域的特征</li><li>（R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。）</li><li><img src="https://tva1.sinaimg.cn/large/008i3skNly1gsd30vvgirj30gz08w760.jpg" srcset="/img/loading.gif" alt="img"></li></ul></li><li>SPP-Net解决了R-CNN重复提取候选区域特征的问题，同时允许各种尺寸图像作为输入，解决了图像畸变的问题，但R-CNN的其它问题，如训练步骤繁琐、磁盘空间开销大等依然有待解决。</li></ul><h3 id="意义："><a href="#意义：" class="headerlink" title="意义："></a>意义：</h3><p>​    当网络输入为任意大小图片时，首先可以进行若干卷积、池化步骤，直至网络最后几层时，在全连接层之前加入SPP层，使得任意大小的特征图转换为固定大小的特征向量，以满足全连接层的输入要求。</p><h2 id="1-4-Fast-R-CNN"><a href="#1-4-Fast-R-CNN" class="headerlink" title="1.4 Fast R-CNN"></a>1.4 Fast R-CNN</h2><h3 id="网络结构：-1"><a href="#网络结构：-1" class="headerlink" title="网络结构："></a>网络结构：</h3><p>输入：由两部分构成：1.待处理的图像  2.候选区域(Region Proposal)</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bn2cavj30zm0ea4gc.jpg" srcset="/img/loading.gif" alt="img"></p><ul><li>使用ROI池化层代替SPP，原图经过卷积后产生特征相应图，然后从每个ROI池化层提取定长特征向量，每个特征向量输入到全连接层，分支为两个输出层，一个是Softmax分类器用来预测类别，另一个用于回归bbox的位置。</li><li>ROI池化层实际上是SPP-NET的一个精简版，可以看成是SPP层的一个特殊情况。SPP-Net对每个候选区域使用了不同大小的金字塔映射，而ROI池化层只需要下采样到一个7x7的特征图。</li><li>ROI池化层与SPP-Net的效果一样，可以将不同大小的输入映射到一个固定尺度的特征向量。</li><li><img src="https://img-blog.csdnimg.cn/img_convert/d28646afb04a88d425b367018209f399.png" srcset="/img/loading.gif" alt="这里写图片描述"></li></ul><p><img src="https://tva1.sinaimg.cn/large/008i3skNgy1gsd2zeixlnj608c0a3wep02.jpg" srcset="/img/loading.gif" alt=""></p><p><img src="https://pic4.zhimg.com/v2-60fb950c3f733daf92342221207ab0bb_b.webp" srcset="/img/loading.gif" alt="img"></p><ul><li>创新点：<ul><li>用ROI层代替SPP层，可以使用BP算法更高效的训练更新整个网络。</li><li>借助多任务损失函数，将物体识别和位置修正合并到一个网络中，不再进行分步训练。</li><li>共享卷积层，不再像R-CNN一样每个候选框都使用CNN提取特征，而是输入一张完整的图片，在第五个卷积层得到每个候选框的特征。</li><li>端到端、基于多任务损失函数的一阶段训练过程，节省了存储空间，减少训练，提升检测精度。</li><li>对全连接层提出了SVD奇异值分解的优化方法，降低了全连接层需要学习的参数数目，节省计算时间。</li></ul></li><li>缺点：<ul><li>候选区域提取仍采用区域选择性搜索算法，耗费时间</li></ul></li></ul><h2 id="1-5-Faster-R-CNN"><a href="#1-5-Faster-R-CNN" class="headerlink" title="1.5 Faster R-CNN"></a>1.5 Faster R-CNN</h2><ul><li>摒弃区域选择性搜索算法，使用Region Proposal Network（RPN）产生候选框，即将搜索候选框的工作也交由神经网络，大大减少了计算量，提高了检测速度。</li><li>引入anchor box以应对目标形状变化问题。（Anchor box就是位置和大小固定的box，可理解为实现设定好的固定的proposal）</li><li>Faster R-CNN将RPN放在最后一个卷积层后面，RPN直接训练得到候选区域。Faster RCNN抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。</li><li>所有窗口经过ROI池化后都会送入后续子网络进行分类和窗口回归。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bmijefj30cj0cm0tf.jpg" srcset="/img/loading.gif" alt="img"></p><h3 id="RPN"><a href="#RPN" class="headerlink" title="RPN"></a>RPN</h3><ul><li>图中RPN网络分为两条线，上面的分支通过Softmax分类anchors获得前景和背景。下面的分支用于计算对于anchors的边界框回归的偏移量，以获得较精确的目标候选区。（注：这里的较精确是相对于后面全连接层的再一次box regression而言）。</li><li>RPN网络将每个样本映射为一个概率值和四个坐标值，概率值反应这个anchor box有物体的概率，四个坐标值用于回归定义物体的位置。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bnwi0qj31820lkthq.jpg" srcset="/img/loading.gif" alt="img"></p><ul><li><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bpp09gj30w00l40vq.jpg" srcset="/img/loading.gif" alt="img"></li></ul><p>(1) RPN网络的作用就是将候选区域的提取也整合到了神经网络中，而且RPN网络和Fast R-CNN共用卷积网络，几乎没有增加计算量。</p><p>(2) RPN提取候选框的方法是对conv feature map上的每个点为中心取9种不同比例大小的anchors（3种尺度，3种比例），再按照比例映射到原图中即为提取的region proposals。具体的实现方法如上图，先用一个256维的3x3卷积核以步长为1进行特征提取，这样就可以得到一个256x1的特征向量，将这个256长度的特征向量分别输入两个分支，其中一个输出长度为2x9=18表示9个anchors是物体和不是物体的概率，另一个输出长度为4x9=36表示每个proposals的四个坐标。</p><p>(3) 在得到了概率和bbox的坐标之后映射到原图中得到region proposals，再进行一次非极大值抑制(NMS)得到最终输入ROI池化层的proposals。</p><ul><li>优点：<ul><li>设计了RPN网络，利用CNN卷积后的特征图生成候选区域，取代了SS和EB等方法，极大地提升了生成候选区域的速度。</li><li>将候选区域的提取也整合到了神经网络中。引入anchor box以应对目标形状变化问题。</li><li>训练RPN与Fast R-CNN共享部分卷积层，极大地提升了检测速度。</li><li>第一个基于深度学习的端到端目标检测算法，具有一定实时性。</li></ul></li><li>缺点：<ul><li>RPN网络的训练较为耗时。</li><li>RPN不善于处理一些极端尺度及形状的目标物体检测。</li></ul></li></ul><h2 id="1-6-R-FCN"><a href="#1-6-R-FCN" class="headerlink" title="1.6 R-FCN"></a>1.6 R-FCN</h2><ul><li>使用位置敏感的ROI池化层，使池化结果不需再经过子网络，直接进行分类和窗口回归。</li></ul><h2 id="1-7-FPN"><a href="#1-7-FPN" class="headerlink" title="1.7 FPN"></a>1.7 FPN</h2><h2 id="1-8-Mask-R-CNN"><a href="#1-8-Mask-R-CNN" class="headerlink" title="1.8 Mask R-CNN"></a>1.8 Mask R-CNN</h2><h1 id="2-one-stage-（基于全局回归分类）"><a href="#2-one-stage-（基于全局回归分类）" class="headerlink" title="2.one-stage:（基于全局回归分类）"></a>2.one-stage:（基于全局回归分类）</h1><h2 id="2-1-SSD"><a href="#2-1-SSD" class="headerlink" title="2.1 SSD"></a>2.1 SSD</h2><p>本文提出了第一个基于深层网络的目标检测器，它不会对假设边框中的像素或特征进行重新取样，有效避免了Faster-R-CNN中的重复采样。</p><p>YOLO只做了一次边框回归和打分，速度较快，能达到实时效果。但由于只做了一次边框回归和打分，导致对小目标训练不充分，对目标尺度敏感，对尺度变化较大的物体泛化能力较差。</p><p>R-CNN系列算法是基于“Proposal+Classification”的Two Stage目标检测算法，这一类算法先预先回归一次边框，然后进行骨干网络训练，精度较高但速度较慢。</p><p>针对YOLO和Faster R-CNN各自的优缺点，SSD算法采用了One Stage的理念，提高检测速度。同时<strong>融入Faster R-CNN中的Anchors思想</strong>，进行特征分层提取并依次计算边框回归和分类操作，可适应多种尺度的目标训练和检测任务，同时精度高，实时性好。</p><p>SSD网络的设计思想是：分层特征提取，依次进行边框回归和分类。因为不同层次的特征图代表不同层次的语义信息：低层次的特征图代表低层语义信息（含有更多细节），适合小尺度目标学习，能提高语义分割质量；高层次的特征图代表高层语义信息，适合对大尺度目标进行深入学习，能平滑分割结果。</p><p>SSD网络的结构被分为6个stage，每个stage学到一幅特征图，然后进行边框回归和分类，每个Stage包含多个卷积层操作。SSD采用<strong>VGG16</strong>的前五层卷积网络作为第一个stage，将VGG16中<strong>fc6和fc7</strong>两个全连接层转换为两个卷积层conv6和conv7作为第2个stage。接着增加了Conv8、Conv9、Conv10、Conv11四层网络，用来提取高层次语义信息。</p><p>SSD算法的基本步骤是：输入图片经过卷积神经网络提取特征，生成特征图；抽取其中六层特征图，并在其每个点上生成default box；将生成的所有default box执行非极大值抑制，输出筛选后的结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghwg9pb9pkj310q0eiqdp.jpg" srcset="/img/loading.gif" alt="img"></p><p>这里用到的default box和Faster RCNN中的anchor很像，在Faster RCNN中anchor只用在最后一个卷积层，但是在本文中，default box是应用在多个不同层的feature map上。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghwzus25dwj30zo0j6tc4.jpg" srcset="/img/loading.gif" alt="img"></p><p> 总的框数：38x38x4、19x19x6、10x10x6、5x5x6、3x3x4、1x1x4，也就是 5773、2166、600、150、36、4，加起来一共有 8732 个box，然后我们将这些box送入NMS模块中，获得最终的检测结果。<strong>default box在不同的feature层有不同的scale，在同一个feature层又有不同的aspect ratio，因此基本上可以覆盖输入图像中的各种形状和大小的object</strong></p><p>对于具有p个通道（num_output）的大小为m×n的特征层，使用3×3×p卷积核卷积操作，其中Padding和stride都为1，保证卷积后的特征图大小不变。但通道数增加，增加为（4*Classes+4)。</p><p>在conv4_3层，有一层Classifier层，使用一层（3,3,(4*（Classes+4）)）卷积进行卷积（Classes是识别的物体的种类数，代表的是每一个物体的得分，4为x，y，w，h坐标，乘号前边的4为default box的数量），这一层的卷积则是提取出feature map，</p><p><strong>Hard negative mining（硬负挖掘）</strong> ：在匹配步骤之后，大多数默认框都是负样本，特别是当可能的默认框数量很大时。这导致了训练期间正负样本的严重不平衡（正负样本的不均衡会导致发散或者精度经常保持为1）。我们不是使用所有负样本，而是使用每个默认框的最高置信度来使负样本排序，然后挑选较高置信度的负样本，以便负样本和正样本之间的比率至多为3：1。我们发现，这导致更快的优化和更稳定的训练。</p><p><strong>SSD算法总结：</strong></p><p>创新点：</p><ul><li>采用不同尺度的特征图进行检测，大尺度的特征图可用来检测小物体，小尺度的特征图检测大物体。</li><li>采用不同尺度和长宽比的先验框。</li><li>实现高精度的端到端训练，即使输入相对低分辨率的图像依然可以在速度和精度上得到权衡。</li></ul><p>优点：运行速度快，可与YOLO媲美；检测精度高，可与Faster R-CNN媲美。</p><p>缺点：需人工设置prior box的基础参数，prior box的基础大小和形状不能直接通过学习获得。</p><p><strong>算法的结果：对于300*300的输入，SSD可以在VOC2007 test上有74.3%的mAP，速度是59 FPS(Nvidia Titan X)，对于512*512的输入, SSD可以有76.9%的mAP</strong>。相比之下Faster RCNN是73.2%的mAP和7FPS，YOLO是63.4%的mAP和45FPS。即便对于分辨率较低的输入也能取得较高的准确率</p><h2 id="预备知识："><a href="#预备知识：" class="headerlink" title="预备知识："></a><strong>预备知识：</strong></h2><p>在继续学习RetinaNet之前，首先要搞清楚何为Hard/Easy Positive/Negative Example。从字面意思上理解，Hard为困难样本、较难分类的样本，Easy为容易分类的样本、Positive为正样本，Negative为负样本。正负样本比较好区分：bbox与ground-truth的IoU大于某一阈值（一般为0.5）即为正样本，IoU小于该阈值即为负样本。</p><p>Hard和Easy的区分主要看是否在前景和背景的过渡区域上。如果不在前景和背景的过渡区域上，分类较为明确，称为Easy。若在过渡区域上，则较难分类，称为Hard。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb4us3dj30ld0bwq44.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb4us3dj30ld0bwq44.jpg" srcset="/img/loading.gif" alt="img"></a></p><h2 id="2-2-RetinaNet"><a href="#2-2-RetinaNet" class="headerlink" title="2.2 RetinaNet"></a>2.2 RetinaNet</h2><p>本文提出了一个可与最先进two-stage检测器的精度匹配的One-Stage检测器。为了实现该结果，作者将训练过程中的类不平衡问题看作主要障碍，并提出了一个新的损失函数来消除该障碍。</p><p>类不平衡问题是指如SSD中产生了很多很多default box，这些default box中大多数都是负样本，特别是当可能的默认框数量很大时。这导致了训练期间正负样本的严重不平衡，训练步骤容易被已分类的背景样本所主导。这一问题在SSD中使用 <strong>Hard negative mining</strong>解决。</p><p>在一张输入图片中，检测器对每张图片评估104-105个候选位置，但只有少数位置包含物体，故负样本多于正样本，出现类不平衡问题。</p><p>类不平衡将引发两个问题：</p><ol><li>训练效率低，大多数位置是easy negative ，没有贡献有用的学习信号。（大多数负样本不在过渡区域，分类明确，称为easy negative。）</li><li>负样本过多造成loss太大，以至于淹没掉正样本的loss，不利于目标收敛，导致模型退化。</li></ol><p>上述问题2将导致无法得出一个能对模型训练提供正确指导的loss，训练步骤容易被已分类的背景样本所主导（而Two Stage方法得到proposal后，其候选区域要远远小于One Stage产生的候选区域，因此不会产生严重的类别失衡问题）。常用的解决此问题的方法就是负样本挖掘，或其它更复杂的用于过滤负样本从而使正负样本数维持一定比率的样本取样方法。本文提出了一个新的损失函数Focal Loss，更高效的替代之前处理类不平衡问题的方法。</p><h3 id="Focal-loss"><a href="#Focal-loss" class="headerlink" title="Focal loss"></a>Focal loss</h3><p>Focal Loss被用来解决one-stage检测方案中训练时前景和背景极端不平衡的问题。从<strong>二分类的交叉熵(CE)</strong>损失来介绍Focal Loss，</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1wva3ph1j30dr02a74a.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1wva3ph1j30dr02a74a.jpg" srcset="/img/loading.gif" alt="image-20200924182142381"></a></p><p>y=±1指定ground-truth类，p∈[0,1]是模型对于类标签y=1的概率。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xd6svy0j30cw0390ss.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xd6svy0j30cw0390ss.jpg" srcset="/img/loading.gif" alt="image-20200924183857022"></a></p><p>CE损失在图1中用蓝线画出，一个值得注意的性质是：即使样本已经被很容易的分类(p&gt;&gt;0.5)，CE损失依然很大。当把大量easy样本加和起来，这些小的损失能淹没稀有的类别。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xen7zh1j30gb0aj75c.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xen7zh1j30gb0aj75c.jpg" srcset="/img/loading.gif" alt="image-20200924184020821"></a></p><h4 id="平衡的交叉熵"><a href="#平衡的交叉熵" class="headerlink" title="平衡的交叉熵"></a>平衡的交叉熵</h4><p>解决类不平衡的一种常用方法是对类1引入权重因子α。在实际应用中，α可采用逆类频率设置，也可作为交叉验证设置的超参数。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xsqbgn0j30bg01aq2s.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xsqbgn0j30bg01aq2s.jpg" srcset="/img/loading.gif" alt="image-20200924185352955"></a></p><p>这种损失是CE的简单扩展，我们将其作为我们提出的Focal损失的实验基线。</p><h4 id="Focal-Loss-定义"><a href="#Focal-Loss-定义" class="headerlink" title="Focal Loss 定义"></a>Focal Loss 定义</h4><p>训练过程中的类不平衡问题淹没了交叉熵损失。容易被分类的负样本构成了大部分损失，并支配着梯度。尽管α平衡了正负样本的重要性，但他没有区分easy/hard样本。<strong>作者重构了loss function来降低easy样本的权重，关注hard negative的训练。</strong></p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1ymj45y2j30ci01mmx1.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1ymj45y2j30ci01mmx1.jpg" srcset="/img/loading.gif" alt="image-20200924192231368"></a></p><p>从图中也可看出：当γ逐渐增大时，较容易分类的loss几乎为0，减少了easy样本的损失贡献。而Pt较小的部分（hard example）的loss依然较大，可以保证在累加后让hard examples贡献更多的loss，在训练时给予hard examples更多的优化。</p><p>性质1：当一个样本被误分类并且概率很低，调制因子趋近于1，损失不受影响。当概率趋近于1，因子趋近于0，这样就降低了已经被良好分类的样本的损失占比。</p><p>性质2：聚焦参数γ平滑的调整easy样本的权重降低率。当γ=0时 FL=CE，随着γ的增加调制因子的影响也最值增加。（我们发现γ=2在实验中效果最佳）。</p><p>直观的说，调制因子减少了easy样本的损失贡献，并且扩展了损失较低样本的范围。例如当γ=2，一个样本被分类的概率为0.9，FL损失比CE损失低100倍，当概率增加到0.968，损失低了1000倍。这反过来增加了校正误分类样本的重要性。（它的损失降低了4倍，当概率&lt;0.5,γ=2）</p><h3 id="RetinaNet："><a href="#RetinaNet：" class="headerlink" title="RetinaNet："></a><strong>RetinaNet</strong>：</h3><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb5c1omj30n306aq4r.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb5c1omj30n306aq4r.jpg" srcset="/img/loading.gif" alt="img"></a></p><p>RetinaNet由一个基石网络(ResNet+FPN)和两个特定任务的子网络组成。基石网络负责在整个输入图像上计算卷积特征图，并且它不是一个自卷积神经网络。第一个子网络在基石网络的输出上做卷积目标分类，第二个子网络执行卷积边框回归。</p><p>输入图像经过Backbone后，可以得到P3-P7特征图金字塔（下标表示特征金字塔的层数，Pl的特征图分辨率比原始图像小2l），每层有C=256个通道。得到特征金字塔后，对特征金字塔的每一层分别使用两个子网络（分类子网络+检测框位置回归子网络）。</p><h3 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a><strong>总结：</strong></h3><ul><li>作者将极端类不平衡问题看作阻止one-stage方法超过two-stage方法的主要障碍。</li><li>为解决该问题，提出了一个新的损失函数Focal Loss，更高效的替代之前处理类不平衡问题的方法。</li><li>提出RetinaNet：ResNet+FPN+2个FCN子网络，精度超过Faster R-CNN。</li></ul><h2 id="2-3-CornerNet-Anchor-free"><a href="#2-3-CornerNet-Anchor-free" class="headerlink" title="2.3 CornerNet(Anchor free)"></a>2.3 CornerNet(Anchor free)</h2><h3 id="目前被经常使用的anchor-box有两个缺点："><a href="#目前被经常使用的anchor-box有两个缺点：" class="headerlink" title="目前被经常使用的anchor box有两个缺点："></a><strong>目前被经常使用的anchor box有两个缺点：</strong></h3><ul><li>需要一个非常大的anchor boxes集合。检测器需要判断每个anchor box是否与ground truth box 足够重叠。通常仅仅一小部分anchors与ground truth重叠，这就造成阳性anchors box和阴性anchors box 之间的巨大不平衡，并且减慢训练。</li><li>使用anchor box引入大量超参数和设计选择。包括：框的数量、框的尺寸、框的长宽比。</li></ul><h3 id="CornerNet："><a href="#CornerNet：" class="headerlink" title="CornerNet："></a><strong>CornerNet：</strong></h3><p>一种去掉anchor boxes的one-stage目标检测方法。CornerNet通过一对关键点（边界框左上角和右下角）来检测目标。使用一个卷积网络预测两个热图集合（相同目标类别的所有实例的），来表示不同目标类别的边角点位置，一个是左上角边角点的热图集合，一个是右下角边角点的热图集合。此外，网络为每一个被检测到的边角点预测一个嵌入式向量，这样同一目标的两个边角点的嵌入式向量之间的距离很小，就能基于这个距离将其分为一组。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb9g912j30nr06aacs.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb9g912j30nr06aacs.jpg" srcset="/img/loading.gif" alt="img"></a></p><p>使用沙漏网络作为CornerNet的Back-bone网络。沙漏网络后面跟着两个预测模块，一个模块预测左上角点，另一个预测右下角点。每个模块有他们自己的Corner Pooling模块来池化来自沙漏网络的特征，之后预测热图、嵌入式向量和偏移。</p><h3 id="Corner-pooling"><a href="#Corner-pooling" class="headerlink" title="Corner pooling:"></a><strong>Corner pooling:</strong></h3><p>一种新的池化层，通过编码明确的先验知识，可帮助卷积神经网络更好地定位边界框的角落。Corner Pooling在两个特征图上执行，从第一个特征图上（在每个像素位置）最大池化所有的特征向量，第二个特征图直接池化所有特征向量，然后将两个池化结果相加。</p><h3 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a><strong>总结：</strong></h3><h4 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a><strong>创新点：</strong></h4><ul><li>提出CornerNet:一种新的目标检测方法，使用单个卷积神经网络，将目标边界框当做一组关键点（左上角、右下角）进行检测，消除了Anchor Boxes。</li><li>提出Corner Pooling，一种新的池化层，帮助网络更好的定位关键点。</li></ul><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点：</strong></h4><ul><li>CornerNet需要学习一个额外的距离向量，进行复杂的后处理来对属于相同实例的边角点分组。</li></ul><h2 id="2-4-CenterNet"><a href="#2-4-CenterNet" class="headerlink" title="2.4 CenterNet"></a>2.4 CenterNet</h2><h2 id="2-5-FCOS-Anchor-free"><a href="#2-5-FCOS-Anchor-free" class="headerlink" title="2.5 FCOS(Anchor free)"></a>2.5 FCOS(Anchor free)</h2><p>当前所有主流检测器（例如Faster R-CNN、SSD、YOLO）都依赖于一个预定义的anchor boxes集合，一直以来人们相信使用anchor box是检测器成功的关键。尽管他们很成功，但了解基于anchor的检测器的缺点也很重要：</p><p>(1) 检测性能对anchor boxes的尺寸、长宽比、数量敏感。基于anchor的检测器的超参数需要被仔细调整。</p><p>(2) anchor boxes的尺寸和长宽比保持固定，检测器在候选目标形状出现大变化，特别是小目标时会遭遇困难。预定义的anchor boxes也会妨碍检测器的泛化能力，在不同的目标尺寸或长宽比的新检测任务上，anchor需要被重新设计。</p><p>(3) 为了实现高召回率，基于anchor的检测器需要在输入图像上密集的放置anchor box。（例如FPN上一张短边为800的图片有超过180K个anchor boxes）。这些anchor box在训练时绝大多数被标注为负样本。过多的负样本加重了训练期间的正负样本不平衡。</p><p>(4) Anchor box包含复杂的计算，例如计算和ground-truth bbox之间的IoU。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb7bb87j30dh07qtgs.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb7bb87j30dh07qtgs.jpg" srcset="/img/loading.gif" alt="img"></a></p><h3 id="全卷积One-stage检测器："><a href="#全卷积One-stage检测器：" class="headerlink" title="全卷积One-stage检测器："></a><strong>全卷积One-stage检测器：</strong></h3><p>检测器直接回归bbox，并将这些位置看作训练样本（而不是anchors)。如果某个位置(x,y)在任一ground-truth box中，那么将其看为正样本，c<em>为其类别标签，c</em>=0表示其为负样本（背景）。4维的实向量t <em>= {l</em>,t<em>,r</em>,b*}是位置回归目标，里面的元素是从该位置到bbox四条边的距离（如图左所示）。如果某个位置有多个边界框，将其看为模棱两可的样本（如图右所示），FCOS简单的选择最小区域的边界框作为回归目标。FCOS利用尽可能多的前景样本来训练回归器。这与基于anchor的检测器不同，它们仅将与GT的IoU足够高的样本看为正样本。这也是FCOS比基于anchor的检测器出色的原因之一。</p><h3 id="用FPN为FCOS进行多等级预测："><a href="#用FPN为FCOS进行多等级预测：" class="headerlink" title="用FPN为FCOS进行多等级预测："></a><strong>用FPN为FCOS进行多等级预测：</strong></h3><p><strong><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb66lp9j30ms09j76f.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb66lp9j30ms09j76f.jpg" srcset="/img/loading.gif" alt="img"></a></strong></p><p>FCOS采用FPN在不同等级的特征图上检测不同尺寸的目标。P3-P7是用于最终预测的特征等级，P3-P5由backboneCNN的特征图产生。P6/P7分别通过在P5/P6上应用一个步长为2的卷积层产生。</p><p>与基于Anchor的检测器不同（它们为不同的特征层级分配不同尺寸的anchor），FCOS直接限制了每个层级上bbox回归的范围。FCOS首先为所有特征层级上每个位置计算l<em>,t</em>,r<em>,b</em>。如果某个位置满足max(l<em>,t</em>,r<em>,b</em>)&gt;mi 或 max(l<em>,t</em>,r<em>,b</em>)&lt;mi-1,就将其设为一个负样本，不再需要回归边界框。这里mi是该特征层级上需要去回归的最大距离。m2,m3,m4,m5,m6,m7分别为0,64,128,256,512,∞。(例如P3的范围是[0,64],P4的范围是[64,128])</p><h3 id="Center-ness："><a href="#Center-ness：" class="headerlink" title="Center-ness："></a><strong>Center-ness：</strong></h3><p>在FCOS上使用多层级的预测之后，FCOS的性能依然与基于Anchor的检测器有很大代沟。作者观察到这是因为许多位置预测的低质量的边界框远离目标的中心点。FCOS引入了“Center-ness”分支来预测某个像素与其对应bbox中心点的偏移。这个评分被被用于降低检测到的低质量的边界框的权重，并且在NMS中合并检测结果。Center-ness使得基于FCN的检测器比基于anchor的检测器效果更好。</p><p>作者提出了一个简单高效的策略来压制检测到的低质量边界框，没有引入任何超参数。具体来说，FCOS增加了一个与分类分支并行的单层分支来预测一个位置的”Center-ness”。Center-ness描述了该位置到目标中心点位置的正规化距离。</p><p><a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb8kwmbj30av02eaaa.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb8kwmbj30av02eaaa.jpg" srcset="/img/loading.gif" alt="img"></a></p><p>在测试时最终评分（用于排列检测到的边界框）通过将预测到的Center-ness与相对应的类别评分相乘得到。Center-ness可以减少远离目标中心点的边界框的权重分数。因此，在最终的非极大值抑制过程中，这些低质量的边界框有很大概率被过滤掉，从而显著提高检测器的性能。</p><h3 id="总结：-3"><a href="#总结：-3" class="headerlink" title="总结："></a><strong>总结：</strong></h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><strong>优点：</strong></h4><ul><li>FCOS使检测与许多其他FCN可解决的任务(如语义分割)统一起来，使重用这些任务中的思想变得更加容易。</li><li>将检测变为Proposal free和Anchor free，显著降低了设计参数的数量，使训练变的简单。</li><li>通过消除Anchor boxes，FCOS完全避免了Anchor boxes相关的复杂计算和超参数（例如在训练期间计算Anchor box和Ground truth box的IoU），并且以逐像素预测的方式进行目标检测，得以更快的训练和测试，同时减少训练期间内存用量。</li><li>FCOS可以作为Two-Stage检测器的区域建议网络(RPNs)，并显著优于基于Anchor的RPN。</li><li>FCOS检测器通过很小的修改就能立即扩展到解决其他视觉任务，包括实例分割和关键点检测。</li></ul><h2 id="2-6-YOLO"><a href="#2-6-YOLO" class="headerlink" title="2.6 YOLO"></a>2.6 YOLO</h2><p>图像经过一个神经网络即可完成目标位置及所属类别的预测，检测速度达到45fps，但检测精度不高。</p><ul><li>优点：<ul><li>速度快。将检测看为回归问题，可以在整幅图像上进行推理，测试时只需在图像上运行神经网络即可预测检测结果。</li><li>检测速度快。每秒可以实时处理45帧图像。较小的网络Fast YOLO，其处理能力达到155帧/秒，精度实现了两倍于当时其他实时检测网络的mAP。</li><li>背景误检率低。YOLOv1在预测时关注整张图像，联系上下文信息，将背景检测为目标的可能性更小。与Fast-R-CNN相比，YOLOv1产生的背景误检率少于一半。</li><li>可以学习物体的泛化表示。在自然图像上训练的模型应用在艺术图像上效果也较好。</li></ul></li><li>缺点：<ul><li>检测精度较低，虽然可快速识别图像中的物体，但难以精确定位某些物体，尤其是成群的小物体。</li><li>YOLOv1直接从数据中学习预测bounding box，因此很难在新的或不寻常的宽高比的对象中进行泛化。</li><li>定位误差大。yolov1的损失函数没有处理好小的bounding box和大的bounding box之间的区别。因为大的bounding box的小误差通常是良性的，但小的bounding box的小误差对IOU的影响要大得多。</li></ul></li></ul><h2 id="2-7-YOLOv2"><a href="#2-7-YOLOv2" class="headerlink" title="2.7 YOLOv2"></a>2.7 YOLOv2</h2><ul><li>加入了<strong>Batch Normalization</strong>批归一化层，去掉全连接层，进行多尺度训练、使用维度聚类、细粒度特征、多尺度训练等改进方法。</li><li>针对检测数据集数据量较少、分类数据集数据量较多的问题，提出一种新的方法，将不同的数据集组合在一起。具体方法是：一方面采用WordTree融合数据集，另一方面联合训练分类数据集和检测数据集。分类信息学习自ImageNet分类数据集，而物体位置检测则学习自 COCO 检测数据集。</li><li>对于组合数据集构建了一个视觉概念的分层模型：WordTree</li><li>引入Anchor boxes，通过预测Anchor Box的偏移值与置信度，而不是直接预测坐标值。</li><li>提出DarkNet-19，有19个卷积层和5个最大池化层</li></ul><h2 id="2-8-YOLOv3"><a href="#2-8-YOLOv3" class="headerlink" title="2.8 YOLOv3"></a>2.8 YOLOv3</h2><ul><li>使用新的网络架构DarkNet-53，有53个卷积层，使用了连续的3×3和1×1卷积层。<ul><li>借鉴了ResNet网络中的残差结构，使得网络能够达到很深程度的同时避免了梯度消失问题。去除了池化层，改用步长为2的卷积层进行特征图的降维。</li><li>测试发现Darknet-53比ResNet-101更好，且速度提高了1.5倍。Darknet-53具有与ResNet-152相似的性能，并且快2倍。</li></ul></li><li>利用多尺度进行目标检测，大大提升了对小目标的检测效果。<ul><li>YOLOv3在3种不同尺度上预测边界框，并使用类似特征金字塔网络的概念进行多尺度级联。</li><li>网络中进行上采样、下采样、特征拼接等操作，输出三张大小分别为13x13、26x26、52x52的特征图，13x13的特征图由于下采样倍数大，单元网格的感受野比较大，适合检测尺寸比较大的目标物；26x26的特征图中单元网格感受野适中，适合检测尺寸中等的目标物；52x52的特征图中单元网格感受野相对较小，适合检测尺寸较小的目标物。</li><li>YOLOv3延续了YOLOv2使用的k-means聚类方法来确定Anchor box的尺寸。为每张特征图确定3个Anchor box的尺寸，最终聚类得到9种尺寸的Anchor box。分配的方式见下表所示，遵循的原则是特征图的尺寸越小，则分配的anchor box的尺寸越大。<a href="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3monpkm2j30k502fjsb.jpg" target="_blank" rel="noopener"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3monpkm2j30k502fjsb.jpg" srcset="/img/loading.gif" alt="preview"></a></li></ul></li><li>改进损失函数，使用逻辑回归代替softmax分类器。<ul><li>YOLOv3不再采用softmax分类器，而采用独立的logistic classifiers。并且在训练过程中，使用二元交叉熵损失来进行类别预测。</li></ul></li></ul><h2 id="2-9-YOLOv4"><a href="#2-9-YOLOv4" class="headerlink" title="2.9 YOLOv4"></a>2.9 YOLOv4</h2><ul><li><p>加入SPP bolck来改善感受野大小，用PANet代替FPN进行多通道特征融合</p></li><li><p>选用CSP-DarkNet53作为主干网络，使得检测精度与速度进一步提升。</p></li><li><p><strong>YOLOv4的最终架构：</strong>Backbone:CSPDarknet53  Neck:SPP,PANet  Head:YOLOv3 </p></li><li><p>YOLOv4其实并没有特别大的创新点，文章可以概括为三个部分：目标检测算法综述+最新算法的大量实验+最优的算法组合。也就是将当前目标检测的方法进行了对比实验研究，最终找到了一个最优的组合，这个组合所带来的整体增益最高。</p></li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
      <tag>Faster R-CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（3.26)</title>
    <link href="/2021/03/26/3-26/"/>
    <url>/2021/03/26/3-26/</url>
    
    <content type="html"><![CDATA[<ul><li><h2 id="论文阅读方面："><a href="#论文阅读方面：" class="headerlink" title="论文阅读方面："></a>论文阅读方面：</h2></li></ul><p>[1] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. arXiv preprint arXiv:1706.03762, 2017.</p><p>[2] Han L, Wang P, Yin Z, et al. Exploiting Better Feature Aggregation for Video Object Detection[C]//Proceedings of the 28th ACM International Conference on Multimedia. 2020: 1469-1477.</p><p>[3] Chen Q, Wang Y, Yang T, et al. You Only Look One-level Feature[J]. arXiv preprint arXiv:2103.09460, 2021.</p><p>[4] Chefer H, Gur S, Wolf L. Transformer Interpretability Beyond Attention Visualization[J]. arXiv preprint arXiv:2012.09838, 2020.</p><ul><li><h2 id="代码运行方面："><a href="#代码运行方面：" class="headerlink" title="代码运行方面："></a>代码运行方面：</h2></li></ul><p>本周对香港中文大学mmlab开源的SELSA代码进行了环境配置、初步运行测试。在不调整原始参数的情况下，其mAP可以达到82%，比原始论文中报告的80.25%高出不少。</p><p>下一步计划认真研读这份代码，在其基础上进行改进。</p><p><img src="https://i.loli.net/2021/03/26/fR5PVpXK9eqrkds.png" srcset="/img/loading.gif" alt="image-20210326165354768"></p><h1 id="1-Attention-is-all-you-need"><a href="#1-Attention-is-all-you-need" class="headerlink" title="1.Attention is all you need"></a>1.Attention is all you need</h1><ul><li><strong>这篇文章是目前很火的Transformer框架的开山之作，遂找来进行阅读。</strong></li></ul><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>主流的顺序翻译模型基于复杂的循环或卷积神经网络，其包含编码器和解码器。效果最好的模型通过注意力机制连接编码器和解码器。本文提出了一个新的简单的网络结构，Transformer，仅仅基于注意力机制，完全抛开循环和卷积。</p><p>递归模型通常沿着输入输出序列的符号位置进行因子计算。这种固有的顺序性排除了训练示例中的并行化，当序列长度变长时，并行化就变得至关重要，因为内存约束限制了示例之间的批处理。</p><p>本文提出的Transformer模型结构，避开了循环，完全依赖注意力机制来庙会输入和输出之间的全局依赖。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>自注意力（内部注意力）是一个注意力机制，联系不同位置的单个序列，来计算序列的一个表示。自注意力已经在多种任务上成功应用，包括阅读理解，抽象总结等。</p><p>端到端的memory网络基于递归注意力机制，而不是序列对其的递归，并且已经被证明在简单语言问答和语言建模任务中表现良好。</p><p>Transformer是第一个完全依赖自注意力来计算输入和输出表示的转移模型，而无需使用序列对齐的RNN或卷积。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>最有竞争力的神经序列转移模型具有一个编码-解码结构。这里的编码器将符号表示的输入序列(x1…xn)映射为连续表示的序列z=(z1,…zn)。给出z，解码器生成一个输出序列(y1,…ym)每次表示一个元素。在每一步模型是自动回归的。生成下一个时，将先前生成的符号用作附加输入。</p><p>Transformer遵循以上的整体结构，使用堆叠的self-attention和point-wise、编码器使用全连接层。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpsxfjuxj30le0uwdkv.jpg" srcset="/img/loading.gif" alt="image-20210320215803807" style="zoom:50%;" /></p><h3 id="3-1-编码器和解码器的堆叠"><a href="#3-1-编码器和解码器的堆叠" class="headerlink" title="3.1 编码器和解码器的堆叠"></a>3.1 编码器和解码器的堆叠</h3><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p>编码器由N=6个完全相同的层堆叠组成。每一层有两个子层。首先是一个multi-head 自注意力机制，其次是一个简单的position-wise的全连接前向传播网络。在这两个子层中采用了残差连接、层正规化。每个子层的输出是</p><p><script type="math/tex">LayerNorm(x+Sublayer(x))</script>,其中$Sublayer(x)$是sub-layer自身的函数。为了促进残差连接，模型中所有的子层和嵌入层的输出维度都是512维。</p><h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><p>解码器也由N=6个完全相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器插入了第三个子层，该第三个子层在编码器堆栈的输出上执行multi-head attention。与编码器相同，解码器也在每个子层中采用了残差连接、层正规化。作者也修改了解码器堆栈中的self-attention子层，以防止器出现在后续的位置。这种掩盖，以及输出嵌入被一个位置偏移的事实，确保了对位置i的预测只能依赖于小于i位置的已知输出。</p><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。将输出计算为值的加权和，其中分配给每个值的权重是通过查询与相应键的兼容性函数来计算的。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpta37gwj30xy0k2tcf.jpg" srcset="/img/loading.gif" alt="image-20210320215823909" style="zoom: 33%;" /></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>和大多数seq2seq模型一样，transformer的结构也是由encoder和decoder组成。</p><p>编码器的输入句子首先会经过一个自注意力层，可以帮助编码器再对每个单词编码时关注输入句子的其他单词。</p><p>自注意力层的输出会传递到前馈神经网络中，每个位置单词对应的前馈神经网络都完全一样。</p><p>解码器中也有编码器的自注意力层和前馈层。此外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分。</p><h1 id="2-Exploiting-Better-Feature-Aggregation-for-Video-Object-Detection"><a href="#2-Exploiting-Better-Feature-Aggregation-for-Video-Object-Detection" class="headerlink" title="2.Exploiting Better Feature Aggregation for Video Object Detection"></a>2.Exploiting Better Feature Aggregation for Video Object Detection</h1><ul><li><strong>这篇文章是目前视频目标检测论文中报告精度最高的一篇，mAP达到84.8%。</strong></li></ul><h4 id="作者指出了当前基于关系的特征聚合方法存在的三个问题："><a href="#作者指出了当前基于关系的特征聚合方法存在的三个问题：" class="headerlink" title="作者指出了当前基于关系的特征聚合方法存在的三个问题："></a>作者指出了当前基于关系的特征聚合方法存在的三个问题：</h4><ol><li>只考虑目标间的时间依赖，忽略了空间关系</li><li>这些方法在时间阈上直接聚合支持帧所有的proposal，不考虑其是否属于同一类。使得其不可避免的从不相关的类中带来有缺陷的proposals。</li><li>其直接聚合支持proposal和目标proposal的特征，没有进行特征对齐。为后续的分类和回归带来了没有经过对齐的特征。</li></ol><h4 id="基于以上问题，本文提出了以下创新："><a href="#基于以上问题，本文提出了以下创新：" class="headerlink" title="基于以上问题，本文提出了以下创新："></a>基于以上问题，本文提出了以下创新：</h4><ul><li>提出Class-constrained spatial-temporal relation network<ul><li>操作目标region proposals，学习两种关系：<ul><li>从辅助帧采样的相同类的region proposal依赖</li><li>目标帧不同目标proposal之间的空间关系</li></ul></li><li>首次同时编码时域和空域的信息。</li></ul></li><li>提出Correlation-based feature alignment module，更好的在时域进行特征聚合。</li><li>提出了一个基于相关的特征对齐方法，在时域上对齐支持帧和目标帧，以进行特征聚合。</li><li>提出class homogeneity constraint，将视频帧顺序打乱，通过在RPN后加入一个分类器，来实现仅用相同类的proposal来增强目标proposal，减少特征聚合中有缺陷的region proposals，并且滤除来自其他类的无效的信息，得到更精确的聚合特征。这样不仅大大减少了需要计算相关性的proposal的数量，并且是在一个统一的端到端的网络框架中，不需要后处理（隐含的嵌入了传统的后处理策略）。</li></ul><p><img src="https://i.loli.net/2021/03/23/aONPqehE5HnDFXr.png" srcset="/img/loading.gif" alt="image-20210323125757262" style="zoom:50%;" /></p><p>首先使用RPN获得每一帧的proposal。之后设计一个简单的分类器来预测每个proposal的类名。之后使用<strong>时间聚合模块（TAM）</strong>用支持帧的同类proposal特征来增强目标proposal特征。特征对齐模块（TA）被插入到时间聚合模块中，以进行更好地特征聚合。最终，使用空间关系模块（SRM），分析相同帧目标的交互，来对目标的拓补关系建模，进一步使用相同帧的proposal来增强目标proposal的特征。</p><h3 id="所提出的模块："><a href="#所提出的模块：" class="headerlink" title="所提出的模块："></a>所提出的模块：</h3><p><strong>TAM模块：</strong>用于将辅助帧的同类proposal特征来增强目标proposal，其中采用Transformer机制来选出可用于特征聚合的最具信息量的辅助proposal。通过cos相似性计算出目标proposal和辅助proposal的表观相似性。之后使用计算出的相似性对特征进行加权和。最后<strong>使用Liner Projection，重新将原始的表观特征包含在内。</strong></p><p><strong>SRM模块：</strong>挖掘<strong>目标帧</strong>中proposal之间的空间位置关系，计算出几何相似性。最终的相似性又包含了上一步计算出的表观相似性。</p><p><strong>特征对齐模块</strong>：通过利用标准差和均值，计算出支持proposal（x,y)位置和目标proposal（m,n)位置的相关性（其中包含了1x1的卷积、对目标proposal的（m,n)位置进行复制、用提出的公式执行“相关”操作）。最终根据计算出的相关性权重，对支持proposal特征图进行加权和，就可以得到其在目标proposal(m,n)位置的对齐过的特征。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h3><p>作者提出了一个class-constrained时空关系网络和一基于相关的特征对齐模块。前者同时考虑了不同帧相同类的时间依赖性、以及同一帧不同目标的空间拓补关系。此外，作者将辅助帧打乱（随机采样），利用了整个视频的目标信息，相比传统的后处理方法更有效。</p><h1 id="3-You-Only-Look-One-level-Feature"><a href="#3-You-Only-Look-One-level-Feature" class="headerlink" title="3. You Only Look One-level Feature"></a>3. You Only Look One-level Feature</h1><p>本文指出FPN（特征金字塔网络）的成功之处在于其“分而治之”的解决思路而非多尺度特征融合。作者从优化的角度出发，仅使用一级特征进行检测，提出了YOLOF(You Only Look One-level Feature)。YOLOF有两个关键性模块：Dilated Encoder与Uniform Matching。</p><h3 id="YOLOF框架"><a href="#YOLOF框架" class="headerlink" title="YOLOF框架"></a>YOLOF框架</h3><p><img src="https://i.loli.net/2021/03/25/CU7IY6QEJjmqOeD.png" srcset="/img/loading.gif" alt="image-20210325190606151" style="zoom: 50%;" /></p><ul><li>BackBone。在所有模型中，作者简单的采用了ResNet与ResNeXt作为骨干网络，所有模型在ImageNet上训练，输出C5特征该通道数为2048，下采样倍率为32；</li><li>Encoder。作者参考FPN添加了两个投影层，将通道数降到512，并堆叠四个不同扩张因子的残差模块；</li><li>Decoder。在这部分，作者采用了RetinaNet的主要设计思路，它包含两个并行的任务相关的Head分别用于分类和回归。作者仅仅添加两个微小改动：(1) 参考DETR中的FFN设计让两个Head的卷积数量不同，回归Head包含4个卷积而分类Head则仅包含两个卷积；(2) 作者参考AutoAssign在回归Head上对每个锚点添加了一个隐式目标预测。</li><li>Other Detail。正如前面所提到的YOLOF中的预定义锚点是稀疏的，这会导致目标框与锚点之间的匹配质量下降。作者在图像上添加了一个随机移动操作以缓解该问题，同时作者发现这种移动对于最终的分类是有帮助的。</li></ul><h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h2><h3 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h3><ul><li>FPN的关键在于针对稠密目标检测优化问题的“分而治之”解决思路，而非多尺度特征融合</li><li>提出了一种简单而有效的无FPN的baseline模型YOLOF，其包含两个关键成分<ul><li>Dilated Encoder：用于增加感受野，覆盖所有的目标尺度。</li><li>Uniform Matching ：解决Positive Anchor的不平衡问题。</li></ul></li></ul><h1 id="4-Transformer-Interpretability-Beyond-Attention-Visualization"><a href="#4-Transformer-Interpretability-Beyond-Attention-Visualization" class="headerlink" title="4. Transformer Interpretability Beyond Attention Visualization"></a>4. Transformer Interpretability Beyond Attention Visualization</h1><h2 id="摘要-1"><a href="#摘要-1" class="headerlink" title="摘要"></a>摘要</h2><p>自注意力机制，特别是Transformer逐渐成为文本处理领域的主流，并且在CV领域流行起来。为了可视化图像中有助于分类的部分，现有方法依赖于所获取到的注意力图，或者沿着注意力图进行启发式传播。本文提出了一种新的方法来为Transformer计算相关性。该方法基于深度泰勒分解规则分配局部相关性，之后传播这些相关性。该传播包含注意力层和跳跃连接。作者的解决方案基于一个特定的公式，该公式显示了跨层保持总体相关性。</p><p>Transformer网络的主要构成是自注意力层，其在两个tokens之间分配一个成对的注意力值。</p><h4 id="本文提出的可视化方法："><a href="#本文提出的可视化方法：" class="headerlink" title="本文提出的可视化方法："></a>本文提出的可视化方法：</h4><p><img src="https://i.loli.net/2021/03/26/fg6OuBk9csyiUAh.png" srcset="/img/loading.gif" alt="image-20210326093924093" style="zoom: 67%;" /></p><h2 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h2><ul><li>本文提出了一种基于Transformer的方法将有助于分类的部分进行可视化。</li><li>其效果相对之前的方法有很大提高，并且是开源的，可以考虑将该方法用于VID。</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
      <tag>EBFA</tag>
      
      <tag>YOLOF</tag>
      
      <tag>Transformer Interpretability</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（3.19)</title>
    <link href="/2021/03/19/3-19/"/>
    <url>/2021/03/19/3-19/</url>
    
    <content type="html"><![CDATA[<h2 id="阅读论文："><a href="#阅读论文：" class="headerlink" title="阅读论文："></a>阅读论文：</h2><p>[1] Tang P, Wang C, Wang X, et al. Object detection in videos by high quality object linking[J]. IEEE transactions on pattern analysis and machine intelligence, 2019, 42(5): 1272-1278.</p><p>[2] Geng Q, Zhang H, Jiang N, et al. Object-aware Feature Aggregation for Video Object Detection[J]. arXiv preprint arXiv:2010.12573, 2020.</p><p>[3] Zheng Z, Wang P, Liu W, et al. Distance-IoU loss: Faster and better learning for bounding box regression[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2020, 34(07): 12993-13000.</p><p>[4] Han W, Khorrami P, Paine T L, et al. Seq-nms for video object detection[J]. arXiv preprint arXiv:1602.08465, 2016.</p><h1 id="1-Object-detection-in-videos-by-high-quality-object-linking"><a href="#1-Object-detection-in-videos-by-high-quality-object-linking" class="headerlink" title="1. Object detection in videos by high quality object linking"></a>1. Object detection in videos by high quality object linking</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>此前的方法大都在相邻帧关联目标，本文首次提出在同一帧内关联目标。与其他方法不同，作者在相同帧连接目标，并且跨帧传播box分数，而非传播特征。</p><p>本文创新点：</p><ol><li>提出了一个cuboid proposal network(CPN,立方体提议网络),提取约束物体运动的时空候选长方体。</li><li>提出了一个短的tubelet detection网络，在短视频中监测short tuubelets。</li><li>提出了一个短的tubelet连接算法，连接有时间重叠的short tubelets来形成长的tubelets。</li></ol><h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h4 id="不使用Object-Linking进行特征传播"><a href="#不使用Object-Linking进行特征传播" class="headerlink" title="不使用Object Linking进行特征传播"></a>不使用Object Linking进行特征传播</h4><p>FGFA STSN STMN MANet:当前帧的特征通过聚合来自其他相邻帧的特征进行增强，FGFA和MANet使用光流来对齐不同帧的特征，进而聚合。STSN使用变形卷积网络跨时空传播特征。STMN采用Conv-GRU来从相邻帧传播特征。此外，DFF也利用特征传播来加速目标检测。作者提出用非常深的高代价网络来计算关键帧的特征，从而将其通过shallow计算出的光流传播到非关键帧。以上这些方法都没有使用Object Linking。</p><h4 id="使用Object-Linking进行特征传播"><a href="#使用Object-Linking进行特征传播" class="headerlink" title="使用Object Linking进行特征传播"></a>使用Object Linking进行特征传播</h4><p>Tubelet Proposal Network首先生成静态的目标proposals，进而预测之后帧的相对位移。tubelets中box的特征通过使用CNN-LSTM网络传播到每个box用于分类。MANet为当前真的每个proposal预测相邻帧的相对位移，相邻帧box的特征通过平均池化被传播到当前帧对应box。与这些方法不同，作者在相同帧连接目标，并且跨帧传播box分数，而非传播特征。此外，作者的方法直接为视频片段生成时空cuboid proposals，而非像Tubelet Proposal Network和MANet一样生成逐帧proposal。</p><h4 id="使用Object-Linking进行分数传播"><a href="#使用Object-Linking进行分数传播" class="headerlink" title="使用Object Linking进行分数传播"></a>使用Object Linking进行分数传播</h4><p>Object detection from video tubelets witth convolutional neural networks.(Object Linking）<br>T-CNN:Tubelets with convolutional neral networks for object from videos(Object Linking)</p><p>以上两篇文章提出了两种Object Linking方法。第一种跟踪当前帧检测到的box到其相邻帧来用更高的召回率增强其原始的检测结果。分数也被传播来提升分类精度。这种连接是基于box内的平均光流向量。第二种使用跟踪算法连接目标和long tubelet，之后采用分类器来聚合tubelets中的检测分数。</p><p>Seq-NMS方法通过选择相邻帧box的空间重叠小来连接目标，不考虑运动信息，之后聚合所连接目标的分数作为最终的分数。Detect to track and track to detect中的方法同时预测两帧的目标位置，以及从前一帧到后一帧的目标位移。之后使用位移来连接检测到的目标和tubelets。相同tubelet的检测分数通过一些方式重新加权。</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>视频目标检测的任务是：推断出视频每一帧目标的位置和类别。为了得到高质量的目标连接，作者提出在相同的帧连接目标，这可以提升分类精度。</p><p>给出被分割成具有时间重叠的视频片段作为输入，作者的方法由3步组成：</p><ol><li>为一个片段生成立方体proposal。这一步旨在生成一组立方体（容器）来约束不同帧的相同目标。</li><li>为一个小片段执行Short tubelet detection。对于每个立方体proposal，回归和分类产生<strong>short tubelet（是边界框的序列，每个边界框定位一帧的目标位置）</strong>空间重叠的short tubelet通过tublet非极大值抑制移除。short tubelet是在视频片段中不同帧连接过的目标的一种表示。</li><li>对整个视频进行short tubelet linking。这一步将具有时间重叠的片段的目标连接起来，并且优化连接过的tubelets的分类分数。</li></ol><p>其中，前两步cuboid proposal generation和short tubelet detection生成时间重叠的short tubelets，从而确保可以在short tubelet linking 这一步进行连接。</p><h3 id="1-Cuboid-Proposal-Generation"><a href="#1-Cuboid-Proposal-Generation" class="headerlink" title="1.Cuboid Proposal Generation"></a>1.Cuboid Proposal Generation</h3><p>Tubelet:一组ground turth box的集合。</p><p>Cuboid：我的理解是一组相同的bbox的集合，用来约束目标的位移。在cuboid中的不同帧的目标被看成相同的目标。</p><p>作者那个Faster R-CNN的 Region Proposal Network修改为 Cuboid Proposal Network。RPN的输入是单张图像，CPN的输入是K帧图像。输出whk个cuboid proposals，其中有wxh个空间网格，其中每个位置有k个参考boxes。每个cuboid proposal都与一个客观评分关联。</p><h3 id="2-Short-Tubelet-Detection"><a href="#2-Short-Tubelet-Detection" class="headerlink" title="2.Short Tubelet Detection"></a>2.Short Tubelet Detection</h3><p>作者采用Cuboid Proposal的二维形式，作为视频片段每一帧的2Dbox(区域)proposal，对每一帧分别进行分类和细化。（就是说，类似RPN生成的proposal是对目标位置的一个粗略回归，这里选择二维的CPN输出作为每一帧的粗略proposal)</p><p>对于视频片段某一帧，作者采用与Fast R-CNN相同的方法来精炼边界框并且计算其分类分数。输入是一个二维的Region Proposal <strong>b</strong>、通过CNN获得的响应图I，之后执行ROI池化操作，ROI池化的结果喂给分类层和回归层，分别产生(C+1)维的分类分数向量、形成精炼后的边界框。</p><p>最终，对K帧分别生成K个精炼过的边界框，即为<strong>Short tubelet detection result。</strong></p><p>为了移除冗余的short tubelets，作者将标准的非极大值抑制方法(NMS)扩展为Tubelet-NMS（T-NMS)来移除位置重叠的short tubelets。（与单张图像类似，若没有NMS会有许多重叠的冗余框，作者将标准NMS扩展到Tubelet)这种策略通过助阵NMS独立的为每一帧移除2D框，保证tubelets不被破坏（如果逐帧进行NMS，会打破现有Tubelet)。</p><p>作者依据<strong>相同帧</strong>的边界框IOU来度量两个tubelets的空间重叠度。（此处可能是因为一个视频片段可能产生了多个tubelets，就需要一种方式来度量两个Tubelets的相似性，也就是说只要有一堆边界框没有完全重合，那么这两个tubelets就不相同）</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqq2us9rqj30sa03kgmg.jpg" srcset="/img/loading.gif" alt="image-20210320220736103" style="zoom:50%;" /></p><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1gojo26cfotj20ri0lu7wh.jpg" srcset="/img/loading.gif" alt="image-20210314193845899" style="zoom:50%;" /></p><h3 id="3-Short-Tubelet-Linking"><a href="#3-Short-Tubelet-Linking" class="headerlink" title="3.Short Tubelet Linking"></a>3.Short Tubelet Linking</h3><p>作者将一段视频分为一系列长度为K帧的具有重叠帧的小片段（步长为K-1），每个小片段分别生成一个tubelet，当不同tubelet的重叠帧(相同帧)的空间重合度高于一个预定义的阈值时，将其连接在一起。</p><p>作者采用了一种贪婪地连接算法：开始时将所有视频片段的short tubelets放到一个池中，并且记录每个tubelet对应的视频片段。然后算法首先从池中选出分类得分最高的short tubelet,计算其重叠帧与其他tubelet的IOU，当IOU大于0.4时，将这两个short tubelets合并为一个长的tubelet，并且移除两个Tubelet的重叠帧中较低的分数，根据等式（2）$Aggregation(·)=\frac{1}{2}(mean(·)+max(·))$计算出新的分类分数。重复以上过程，直到没有tubelet可以合并。</p><p>最终，池中剩余的tubelets形成了视频目标检测的结果。tubelet的分数被分给每一个box。</p><p><img src="https://tva1.sinaimg.cn/large/e6c9d24ely1gojnwqbrusj20o20e4gxc.jpg" srcset="/img/loading.gif" alt="image-20210314193330219" style="zoom:50%;" /></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本篇论文提出的cuboid proposal network(CPN)让人眼前一亮，其主要思路是将一个长的视频片段分割成一个个短的片段，每个短的片段之间都有一帧重合。之后通过对不同tubelet之间重叠帧进行相似性度量，将多个tubelet连接在一起，并且优化连接过的tubelets的分类分数。</p><p>这种方法与此前的聚合方法有很大不同，此前方法大都集中在选择关键帧与非关键帧，而本文的方法是将长的视频片段分割成小片段。这种思路以后可以借鉴。</p><h3 id="存在的问题："><a href="#存在的问题：" class="headerlink" title="存在的问题："></a>存在的问题：</h3><ul><li>分割片段的长度如何选择？2</li><li>图(b)中出现错误的分类，如何度量其相似性？</li><li>如果某个tubelet与其他的tubelet的IOU都很低怎么办？允许broken links</li></ul><h1 id="2-Object-aware-Feature-Aggregation-for-Video-Object-Detection"><a href="#2-Object-aware-Feature-Aggregation-for-Video-Object-Detection" class="headerlink" title="2.  Object-aware Feature Aggregation for Video Object Detection"></a>2.  Object-aware Feature Aggregation for Video Object Detection</h1><p>本文提出了一个Object-aware Feature Aggregation(OFA)模块，有选择的聚合与proposal相对应的特征。具体来说，输入特征被分为两个部分，并喂给两个路径：语义路径和定位路径。a)语义路径。首先从整个视频的proposal中收集object-aware知识。与此同时，通过计算proposal之间的相似性得到成对的语义上下文。通过聚合以上得到的成对的语义上下文和proposal的object-aware知识，这些特征编码了更多关于其他区域和整个视频的知识。b)定位路径。定位特征也可以通过成对定位上下文来增强。对比于语义路径，作者局部增强这些特征来确保特征对相对位置敏感。</p><h3 id="OFA模块"><a href="#OFA模块" class="headerlink" title="OFA模块"></a>OFA模块</h3><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpxttymjj315k0hqalg.jpg" srcset="/img/loading.gif" alt="image-20210320220246050"></p><p>给出参考帧和支持帧，使用Faster-rcnn的方式提取proposal。之后将proposal的特征用两个堆叠的OFA模块进行增强。作者设计了两条独立的路径（语义路径和定位路径）来用语义信息和的定位信息增强object-aware feature。在随后的后处理过程中，使用DIoU NMS来进一步提升性能。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><h3 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h3><ul><li>提出OFA模块，分别用两条路径并行的进行分类和回归。</li><li>使用成对的先验上下文知识来提升性能。</li><li>使用DIoU NMS的后处理方法提高性能。</li></ul><h1 id="3-Distance-IoU-loss-Faster-and-better-learning-for-bounding-box-regression"><a href="#3-Distance-IoU-loss-Faster-and-better-learning-for-bounding-box-regression" class="headerlink" title="3. Distance-IoU loss: Faster and better learning for bounding box regression"></a>3. Distance-IoU loss: Faster and better learning for bounding box regression</h1><p>边界框回归是目标检测中重要的一步。现有方法大多采用Ln-norm loss用于边界框回归，如交并比IOU。本文提出了Distance-IoU Loss（DIoU），合并预测框和目标框的正规化距离。使用DIOU Loss的收敛速度比IOU Loss和GIoU Loss更快。此外，本文总结了边界框回归中三个几何因素：1.重叠区域 2.中心点距离 3.长宽比。基于此提出了Complete Iou Loss(CIoU)，收敛更快，性能更好。通过同时应用DIoU Loss和CIoU Loss，SOTA方法的性能有所提高。此外DIoU可以作为NMS的标准，进一步加速性能提升。</p><h3 id="传统的IoU"><a href="#传统的IoU" class="headerlink" title="传统的IoU"></a>传统的IoU</h3><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpy4acm7j307e02wjrg.jpg" srcset="/img/loading.gif" alt="image-20210320220303374" style="zoom:50%;" /></p><p>其中$B(x,y,w,h)$是预测框的坐标，$B^{gt}(x^{gt},y^{gt},w^{gt},h^{gt})$是ground-truth的坐标。通常在$B$和$B^{gt}$上采用L1或L2 norm loss 来度量边界框之间的距离。之后有学者指出，Ln-norm Loss不是包含最优IoU度量的合适选择，并且提出了IoU loss，提升IoU度量：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpy9bhhfj309e02odfx.jpg" srcset="/img/loading.gif" alt="image-20210320220311046" style="zoom:50%;" /></p><p>但是IoU Loss只能在bbox有重叠的情况下使用，并不能对非重叠的情况提供任何运动梯度。</p><h3 id="GIoU-Loss"><a href="#GIoU-Loss" class="headerlink" title="GIoU Loss"></a>GIoU Loss</h3><p>之后，有学者通过对IoU loss增加一个惩罚项，提出了generalized IoU Loss(GIoU)。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpyfs5loj30e802uaa7.jpg" srcset="/img/loading.gif" alt="image-20210320220321860" style="zoom:50%;" /></p><p>其中C是覆盖$B$和$B^{gt}$的最小的框，惩罚项的加入使得预测框朝着目标框移动。（此前两框并不重叠）。</p><p>GIoU可以减轻当两框不重叠时的梯度消失问题，但仍然有一些局限性。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpykydzvj30i80aaq49.jpg" srcset="/img/loading.gif" alt="image-20210320220330072" style="zoom:50%;" /></p><blockquote><p>上图展示了GIoU loss(第一行)和DIoU Loss(第二行)的回归过程。绿色框表示目标框，黑色框是anchor box。蓝色和红色是预测框。</p></blockquote><p>如上图所示，从图中可以看出GIoU Loss首先倾向于增加预测框的尺寸，使其与目标框重叠（这是惩罚项在起作用）。之后IoU项才起作用，使得bbox的重叠区域最大。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpypn1hwj30lq0bq76r.jpg" srcset="/img/loading.gif" alt="image-20210320220337240" style="zoom:50%;" /></p><p>从图二可以看出，GIoU loss在封闭框内将退化为IoU Loss。GIoU由于过度依赖IoU项，需要更多的迭代次数才能收敛（特别是对于水平和垂直框）。</p><h3 id="DIoU-Loss"><a href="#DIoU-Loss" class="headerlink" title="DIoU Loss"></a>DIoU Loss</h3><p>本文提出了Distance-IoU Loss，简单的在IoU Loss后加了一个惩罚项，<strong>直接最小化两个边界框中心点之间的距离。</strong>这样以来使得其比GIoU Loss收敛更快。DIoU只需120个迭代就能很好的匹配目标框，而GIoU在400次迭代后也没有收敛。作者进一步进行了总结：一个好的边界框损失应该包含三个重要的几何度量：1.重叠区域 2.中心点距离 3.长宽比。这三点被长期忽略掉。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpz6llkbj30ie0323yu.jpg" srcset="/img/loading.gif" alt="image-20210320220404602" style="zoom:50%;" /></p><p>其中b和$b^{gt}$表示B和$B^{gt}$的中心点，$\rho(·)$是欧氏距离，c是能包含两个框的最小框的对角线长度。$d=\rho(b,b^{gt})$</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpzgli7sj30am07ydhf.jpg" srcset="/img/loading.gif" alt="image-20210320220419929" style="zoom:50%;" /></p><h5 id="与IoU和GIoU-loss对比："><a href="#与IoU和GIoU-loss对比：" class="headerlink" title="与IoU和GIoU loss对比："></a>与IoU和GIoU loss对比：</h5><ul><li>DIoU损失对回归问题的尺度仍然是不变的</li><li>与GIoU一样，可以为与目标框不重叠的bbox提供移动方向</li><li>当两个边界框完全匹配时$L_{IoU}=L_{GIoU}=L_{DIoU}=0$，当两个边界框离的很远时$L_{GIoU}=L_{DIoU}\rightarrow2$</li></ul><h3 id="CIoU-Loss"><a href="#CIoU-Loss" class="headerlink" title="CIoU Loss"></a>CIoU Loss</h3><p>通过联合这三个几何度量，作者进一步提出了Complete IoU Loss（CIoU），比IoU loss 和 GIoU loss收敛更快，性能更高。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpzwbtpmj30la032aag.jpg" srcset="/img/loading.gif" alt="image-20210320220445954" style="zoom:50%;" /></p><p>其中α是一个正的平衡参数，v度量长宽比的一致性：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqq02du7ij30je038aag.jpg" srcset="/img/loading.gif" alt="image-20210320220455608" style="zoom:50%;" /></p><p>平衡因子定义为：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqq09fhtej30bs03c3ym.jpg" srcset="/img/loading.gif" alt="image-20210320220506354" style="zoom:50%;" /></p><p>作者提出的DIoU和CIoU Loss可以轻易的整合到现有SOTA方法中。此外，DIoU可以作为NMS的标准，在抑制冗余框的时候，不仅考虑重叠区域，也考虑两个边界框中心点的距离，使得其在有遮挡的情况下更鲁棒。</p><h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h2><h5 id="创新点：-1"><a href="#创新点：-1" class="headerlink" title="创新点："></a>创新点：</h5><ul><li>提出DIoU Loss用于边界框回归，比IoU和GIoU loss收敛更快</li><li>考虑三个几何度量，提出CIoU Loss，更好的描述矩形框的回归</li><li>所提出的DIoU可以应用在NMS，在抑制冗余框上比原始NMS效果更好</li></ul><h1 id="4-Seq-NMS-for-Video-Object-Detection"><a href="#4-Seq-NMS-for-Video-Object-Detection" class="headerlink" title="4. Seq-NMS for Video Object Detection"></a>4. Seq-NMS for Video Object Detection</h1><p>本文是一种基于后处理的方法，给出一个包含region proposals和对应类别分数的视频序列，作者的方法通过使用一个简单的重叠判据，将相邻帧的bbox 相关联。之后选择box来最大化序列分数，这些box之后被用于在其各自对应的帧内抑制重叠box，随后被重新评分来加速弱检测。</p><h3 id="Seq-NMS"><a href="#Seq-NMS" class="headerlink" title="Seq-NMS"></a>Seq-NMS</h3><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqq0ooow7j315o0gcqdx.jpg" srcset="/img/loading.gif" alt="image-20210320220530693"></p><p>Seq-NMS有3步：1.序列选择，选择具有最高序列分数的序列box 2.序列Re-Scoring：重新对序列中box的分数进行评估计算。3.抑制：抑制重叠度高的任何box。</p><h4 id="序列选择"><a href="#序列选择" class="headerlink" title="序列选择"></a>序列选择</h4><p>对于视频序列中相邻的两帧，当两帧的IoU大于某个阈值时，第一帧的一个bbox可以与第二帧的一个bbox建立连接。作者为视频片段中每一对相邻帧寻找潜在的连接。之后尝试找到整个序列最大化的分数序列。也就是说，作者试图找到符合约束条件的box序列，即所有相邻的box必须被链接,且具有最大的目标分数之和。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqq0u5mknj315u0iw7jt.jpg" srcset="/img/loading.gif" alt="image-20210320220540137"></p><blockquote><p>文中提到的序列选择过程。当相邻帧的box之间的IoU&gt;0.5时，从中选择总分最高的一组序列。在序列选择之和，每一帧最多只有一个box，删除IoU&gt;0.3的其他box。</p></blockquote><h3 id="序列Re-Scoring"><a href="#序列Re-Scoring" class="headerlink" title="序列Re-Scoring"></a>序列Re-Scoring</h3><p>在选择完序列之后，将该序列的分数提升。作者尝试使用了平均值和最大值。</p><h3 id="抑制"><a href="#抑制" class="headerlink" title="抑制"></a>抑制</h3><p>被选择序列中的box被从连接过的box集合中移除。如果某一帧的bbox与选中的bbox的IoU大于某个阈值，则从候选框中移除它。</p><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h4><ul><li>本文提出了一种基于后处理的方法，在一定程度上可以提升视频目标检测的性能。</li></ul><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>在有多个相似的目标比较靠近时，容易出现错误检测。</li><li>有时会增加更多的误警</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CPN</tag>
      
      <tag>Object-aware</tag>
      
      <tag>DIoU Loss</tag>
      
      <tag>Seq-NMS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（3.12)</title>
    <link href="/2021/03/12/3-12/"/>
    <url>/2021/03/12/3-12/</url>
    
    <content type="html"><![CDATA[<h1 id="Attention-is-all-you-need"><a href="#Attention-is-all-you-need" class="headerlink" title="Attention is all you need"></a>Attention is all you need</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>主流的顺序翻译模型基于复杂的循环或卷积神经网络，其包含编码器和解码器。效果最好的模型通过注意力机制连接编码器和解码器。本文提出了一个新的简单的网络结构，Transformer，仅仅基于注意力机制，完全抛开循环和卷积。</p><p>递归模型通常沿着输入输出序列的符号位置进行因子计算。这种固有的顺序性排除了训练示例中的并行化，当序列长度变长时，并行化就变得至关重要，因为内存约束限制了示例之间的批处理。</p><p>本文提出的Transformer模型结构，避开了循环，完全依赖注意力机制来庙会输入和输出之间的全局依赖。</p><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>自注意力（内部注意力）是一个注意力机制，联系不同位置的单个序列，来计算序列的一个表示。自注意力已经在多种任务上成功应用，包括阅读理解，抽象总结等。</p><p>端到端的memory网络基于递归注意力机制，而不是序列对其的递归，并且已经被证明在简单语言问答和语言建模任务中表现良好。</p><p>Transformer是第一个完全依赖自注意力来计算输入和输出表示的转移模型，而无需使用序列对齐的RNN或卷积。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>最有竞争力的神经序列转移模型具有一个编码-解码结构。这里的编码器将符号表示的输入序列(x1…xn)映射为连续表示的序列z=(z1,…zn)。给出z，解码器生成一个输出序列(y1,…ym)每次表示一个元素。在每一步模型是自动回归的。生成下一个时，将先前生成的符号用作附加输入。</p><p>Transformer遵循以上的整体结构，使用堆叠的self-attention和point-wise、编码器使用全连接层。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpsxfjuxj30le0uwdkv.jpg" srcset="/img/loading.gif" alt="image-20210320215803807"></p><h3 id="3-1-编码器和解码器的堆叠"><a href="#3-1-编码器和解码器的堆叠" class="headerlink" title="3.1 编码器和解码器的堆叠"></a>3.1 编码器和解码器的堆叠</h3><h4 id="编码器"><a href="#编码器" class="headerlink" title="编码器"></a>编码器</h4><p>编码器由N=6个完全相同的层堆叠组成。每一层有两个子层。首先是一个multi-head 自注意力机制，其次是一个简单的position-wise的全连接前向传播网络。在这两个子层中采用了残差连接、层正规化。每个子层的输出是</p><p><script type="math/tex">LayerNorm(x+Sublayer(x))</script>,其中$Sublayer(x)$是sub-layer自身的函数。为了促进残差连接，模型中所有的子层和嵌入层的输出维度都是512维。</p><h4 id="解码器"><a href="#解码器" class="headerlink" title="解码器"></a>解码器</h4><p>解码器也由N=6个完全相同的层堆叠而成。除了每个编码器层中的两个子层之外，解码器插入了第三个子层，该第三个子层在编码器堆栈的输出上执行multi-head attention。与编码器相同，解码器也在每个子层中采用了残差连接、层正规化。作者也修改了解码器堆栈中的self-attention子层，以防止器出现在后续的位置。这种掩盖，以及输出嵌入被一个位置偏移的事实，确保了对位置i的预测只能依赖于小于i位置的已知输出。</p><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>注意力函数可以描述为将查询和一组键值对映射到输出，其中查询、键、值和输出都是向量。将输出计算为值的加权和，其中分配给每个值的权重是通过查询与相应键的兼容性函数来计算的。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpta37gwj30xy0k2tcf.jpg" srcset="/img/loading.gif" alt="image-20210320215823909"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>和大多数seq2seq模型一样，transformer的结构也是由encoder和decoder组成。</p><p>编码器的输入句子首先会经过一个自注意力层，可以帮助编码器再对每个单词编码时关注输入句子的其他单词。</p><p>自注意力层的输出会传递到前馈神经网络中，每个位置单词对应的前馈神经网络都完全一样。</p><p>解码器中也有编码器的自注意力层和前馈层。此外，这两个层之间还有一个注意力层，用来关注输入句子的相关部分。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Transformer</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（2.28)</title>
    <link href="/2021/02/28/2-28/"/>
    <url>/2021/02/28/2-28/</url>
    
    <content type="html"><![CDATA[<h1 id="1-CenterNet-Heatmap-propagation-for-Real-time-Video-Object-Detection"><a href="#1-CenterNet-Heatmap-propagation-for-Real-time-Video-Object-Detection" class="headerlink" title="1.CenterNet Heatmap propagation for Real-time Video Object Detection"></a>1.CenterNet Heatmap propagation for Real-time Video Object Detection</h1><p>本文介绍了一种基于CentNet One-Stage检测器的方法。以<strong>heatmap的形式传播</strong>之前可依赖的长程检测，来加速未来图像的结果。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>此前的视频目标检测方法大多是基于Two-Stage检测器的，直接在已有的时间信息聚合方法上应用One-Stage检测器困难很大，或者不可行。因为One-Stage和Two-Stage的目标边界框表示大大不同，一些方法通过对ROI池化过的特征进行操作，然而其在One-stage方法中并不存在。</p><p>本文提出了一种热图传播方法，来高效的解决视频目标检测问题。该方法基于一个One-Stage检测器：CenterNet，其对一张图像的不同类别输出所检测到的所有目标的中心点的热图。</p><p>对于视频中的一帧，将稳定的、可检测到的目标转移到一个propagation heatmap，在这张热图中，用其对应类的置信度分数来高亮每个目标中心的潜在位置。对于下一帧，考虑propagationrequest和网络输出的热图，生成一张平衡的热图。这与为每个目标生成一个在线的跟踪小片段类似，此外根据每一帧的检测结果更新置信度分数。</p><h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2.Related work"></a>2.Related work</h2><p>CornerNet检测目标边界框的关键点，例如顶点或中心点。网络的输出是关键点的热图和一些用于偏移的回归值，或者是依赖于不同结构的原始边界框尺寸。</p><h2 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3.Proposed Method"></a>3.Proposed Method</h2><h3 id="3-1-CenterNet"><a href="#3-1-CenterNet" class="headerlink" title="3.1 CenterNet"></a>3.1 CenterNet</h3><p>CenterNet是一个基于heatmap的one-stage检测器，其预测目标的中心位置和目标的尺寸。若某个像素点对应目标的中心，则为1，否则对应背景，为0。此外，网络预测一个局部偏移O来恢复输出步长引起的离散误差、以及一个回归S，用于目标尺寸回归。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gn362kh8jej31a60mwdlq.jpg" srcset="/img/loading.gif" alt="image-20210128094659101"></p><p>如图1所示，整个网络包含3个内容：一个普通的卷积网络Nfeat，如ResNet，从输入图像提取特征图。一个反卷积网络Ndecv，由3x3的变形卷积层DCL和up conv层组成。最后有三个分离的头部 Nhead，共享相同的backbone特征图，输出Y、O、S。<img src="https://tva1.sinaimg.cn/large/008eGmZEly1gn36efkcz2j309y024t8p.jpg" srcset="/img/loading.gif" alt="image-20210128095825840" style="zoom:33%;" />表示某像素点是否是目标中心，O是局部偏移，S用于回归目标尺寸。</p><p>由于计算开销的原因，所有的类共享相同的偏移预测和目标尺寸，因此，网络最终的输出尺寸是WxHx(C+4)。</p><h3 id="3-2-Heatmap传播"><a href="#3-2-Heatmap传播" class="headerlink" title="3.2 Heatmap传播"></a>3.2 Heatmap传播</h3><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1gn36rj1qu6j318x0u0b29.jpg" srcset="/img/loading.gif" alt="image-20210128101100310"></p><p>传播过程：将输出热图的面积扩大为$(2P+1)^2$,并将不同目标的热图合并为一张热图，并通过公式5和6进行热图传播。在估计边界框尺寸时，使用等式7而不是等式5，不包含跟踪长度，原因是目标的尺寸会随着时间而改变（相机和目标之间有相对位移）。在这里仅使用之前帧和当前帧进行估计。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><h3 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h3><ul><li>将One-Stage检测器CenterNet用于视频目标检测</li><li>以热图的形式传播先前帧的检测结果（而不是特征图）</li><li>由于One-Stage检测器的先天优势，该方法的速度较快，为37FPS，达到实时</li></ul><h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ul><li>该方法的mAP为76.7%，而基于Two-Stage的方法精度在82%左右</li><li>检测精度相比基于Two-Stage的检测器还有一定差距</li></ul><h1 id="2-Single-Shot-Video-Object-Detector"><a href="#2-Single-Shot-Video-Object-Detector" class="headerlink" title="2.Single Shot Video Object Detector"></a>2.Single Shot Video Object Detector</h1><p>本文所提出的SSVD，使用one-stage检测器进行特征聚合，使用特征金字塔网络FPN作为backbone，产生多尺度的特征。SSVD一方面估计运动，沿着运动路径聚合相邻帧的特征，另一方面在一个“双流结构”中直接从相邻帧采样特征。</p><p>Sampling Stream:通过self-guided从相邻帧采样，直接生成特征。</p><p>当目标因运动模糊或遮挡造成了表观退化，Sampling Stream比motion Stream有效。</p><p>当目标快速运动时，Sampling Stream不能精确的定位目标。这是由于Sampling Stream的感受野比光流生成更小。因此，Sampling Stream的运动估计范围比运动流更短。导致Sampling Stream在目标快速运动时无法捕捉运动。</p><p>最终，作者将两者结合，同时利用运动校准和self-guided采样，即采用一个双流特征聚合结构进行视频目标检测。此外，作者将single shot 目标检测框架整合到特征聚合中，更适用于计算敏感的视频场景。</p><p>SSVD包含三个核心模块：</p><ol><li>特征金字塔网络（FPN）</li><li>双流特征聚合结构</li><li>class/box子网络</li></ol><p>FPN用于输出多尺度特征图，金字塔的每张特征图被喂给双流特征聚合结构。第一个是motion Stream，根据光流估计帧间目标位移，并沿着运动路径扭曲相邻帧的特征图到参考帧。另一个是Sampling Stream，通过可变形卷积对相邻帧的时空特征进行采样，直接产生参考帧的特征图。每个流聚合过的特征图被输入到class/box子网络，进行anchor box的分类、回归anchor box到ground truth目标框。最终的结果是两个流中所有特征映射的混合输出。整个SSVD通过最小化框分类的Focal Loss和框回归的标准平滑L1损失进行端到端的训练。</p><p>本文的主要贡献是提出了一个one-stage检测器SSVD用于解决视频目标检测问题。SSVD一方面利用single-shot检测器的优点（比two-stage更简单、更快）,另一方面利用帧间时间一致性来加速检测。</p><p>SSVD属于feature-level聚合方法，创新点是探索利用双流特征聚合（motion和sampling流）。motion流通过光流和扭曲相邻帧的特征图来估计运动，执行特征聚合。sampling流通过从相邻帧进行时空采样，直接获得参考帧的特征图。</p><h2 id="Motion-Stream"><a href="#Motion-Stream" class="headerlink" title="Motion Stream"></a>Motion Stream</h2><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpkumlysj315w0oswnx.jpg" srcset="/img/loading.gif" alt="image-20210320215016113"></p><p>首先使用PWC-Net预测每个尺度的光流（由FPN得到），之后在光流的指导下执行motion-aware校准（通过双线性扭曲）。文中介绍PWC-Net光流提取法尺寸更小，更容易训练。在得到所有支持帧的校准过的特征图后，在每个尺度上对其求平均。</p><h2 id="Sampling-Stream"><a href="#Sampling-Stream" class="headerlink" title="Sampling Stream"></a>Sampling Stream</h2><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpl32cszj31200t6qf2.jpg" srcset="/img/loading.gif" alt="image-20210320215031317"></p><p>用self-guided采样，直接通过从支持帧采样获取特征。变形卷积比标准卷积多了一个2D偏移，2D偏移通过输入特征自己计算得到，没有额外的监督。在本文中，作者将标准变形卷积增加了空间采样位置，只以一个特征图为条件，来测量参考帧和支持帧之间的变形。换句话说，sampling Stream的特征聚合模块，通过输入帧的2D偏移学习预测参考帧和支持帧之间潜在的相关性，没有用到光流。</p><h1 id="3-A-Delay-Metric-for-Video-Object-Detection-What-Average-Precision-Fails-to-Tell"><a href="#3-A-Delay-Metric-for-Video-Object-Detection-What-Average-Precision-Fails-to-Tell" class="headerlink" title="3.A Delay Metric for Video Object Detection: What Average Precision Fails to Tell"></a>3.A Delay Metric for Video Object Detection: What Average Precision Fails to Tell</h1><p>Average Precision（AP）被广泛用于评估图像和视频目标检测器的检测精度。但是只用AP不足以捕捉视频目标检测自然存在的时间信息。本文提出了一个综合的度量方法：Average delay(AD)，来度量和对比检测延时。为了便于延迟计算，我们仔细地选择了ImageNet VID的一个子集，我们将其命名为ImageNet VIDT，并强调了复杂的轨迹。通过在VIDT上广泛评估检测器，我们表明大多数方法大幅度增加了检测延迟，但仍然保持较高的AP。换句话说，<strong>AP不够灵敏，无法反映视频目标检测器的时间特征。</strong>作者认为视频目标检测方法应该加上用delay metric进行评估，特别是一些不能容忍延时的应用场景：如自动驾驶。</p><h4 id="创新点：-1"><a href="#创新点：-1" class="headerlink" title="创新点："></a>创新点：</h4><ul><li>针对视频目标检测问题，提出了一种新的度量方式Average delay(AD)</li></ul><h4 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>Average delay(AD)不能很好地支持现有数据集（如ImageNet VID）</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CHP</tag>
      
      <tag>SSVD</tag>
      
      <tag>Delay metric</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（2.20)</title>
    <link href="/2021/02/20/2-20/"/>
    <url>/2021/02/20/2-20/</url>
    
    <content type="html"><![CDATA[<p>[1]Guo C, Fan B, Gu J, et al. Progressive sparse local attention for video object detection[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 3909-3918.</p><p>[2] Yao C H, Fang C, Shen X, et al. Video Object Detection via Object-Level Temporal Aggregation[C]//European Conference on Computer Vision. Springer, Cham, 2020: 160-177.</p><h1 id="1-PLSA"><a href="#1-PLSA" class="headerlink" title="1.PLSA"></a>1.PLSA</h1><p>提出了PSLA（渐近稀疏局部注意力），在一个局部区域里，用渐近稀疏步长建立跨帧的空间一致性，使用这个一致性来传播特征。基于PSLA提出了RFU（循环特征更新）对时间表观建模、和DenseFT（密集特征变换）来丰富特征表示。</p><p>PSLA不依赖光流传播帧间高级语义特征。特别的，给出两帧特征Ft和Ft+ε，PSLA首先基于这两帧的特征关系计算出一致性权重，然后用一致性权重聚合特征对齐Ft和Ft+ε。</p><p>基于PSLA提出了一个视频目标检测框架，昂贵的高级特征提取在稀疏的关键帧上执行，廉价的低级特征提取在密集的非关键帧上执行。基于提取到的特征，PSLA被用于两个不同和互补的情况：1.为了传播关键帧的高级特征到非关键帧。这使得我们可以分配大多数计算开销到关键帧，在不牺牲精度的情况下提升测试效率。此外，一个小的Quality Net网络被设计出来，用非关键帧特征的低级信息来补充传播过来的高级特征，旨在降低特征传播的混叠效应。作者将这个过程称为密集特征变换（Dense FT）。2.为了维持时间特征Ft（其建模了视频的时间表观），通过在关键帧之间传播高级特征。与此同时，一个更新Update网络也被提出来，来用关键帧的高级特征循环的更新Ft。我们的消融实验表明，利用时间上下文有助于显著提高性能。作者将这一过程称为循环特征更新（RFU）。</p><p>本文贡献：</p><ul><li>提出PSLA渐近稀疏局部注意力，不依赖额外的光流，建立特征图之间的空间一致性。显著减少了模型参数，实现更好的结果。</li><li>基于PSLA提出了两个技术。循环特征更新RFU和密集特征变换DenseFT，分别对时间表观建模、增强非关键帧的特征表示。</li><li>介绍了一个新颖的视频目标检测框架，在ImageNet VID上实现了SOTA效果。</li></ul><p>STMN使用一个类似于相关的模块在一个局部区域中对其特征，与STMN不同，我们的方法关注与稀疏邻域，并且使用softmax正规化来更好地建立空间一致性。我们的方法同时提升速度和精度，而STMN提升了精度但速度很慢。</p><h4 id="自注意力"><a href="#自注意力" class="headerlink" title="自注意力"></a>自注意力</h4><p>自注意力机制首先被用于机器翻译，为了整合足够的序列上下文和长程信息，其用所有位置的加权均值在序列的一个位置计算响应，其中的权重通过反向传播无监督学习。与之前的工作不同，PSLA是自注意力机制的一个更泛化的形式。在本文中，自注意力机制被用在时空域来对齐两张特征图。</p><h4 id="非局部操作"><a href="#非局部操作" class="headerlink" title="非局部操作"></a>非局部操作</h4><p>非局部操作是一个传统的过滤算法，被广泛用于图像去噪声、超分和纹理合成。这些方法用图像中所有像素点的加权平均计算响应，其中的权重基于块的表观相似性获得。与这些方法不同，PSLA用渐近稀疏步长关注局部区域。</p><h2 id="所提出的方法"><a href="#所提出的方法" class="headerlink" title="所提出的方法"></a>所提出的方法</h2><p>给出一个视频，每一帧通过一个CNN来提取特征，之后跟着一个任务网络Nt，本文中是目标检测。为了减少计算开销，视频帧被分为关键帧和非关键帧，其特征提取网络不同。关键帧是Nf，非关键帧是Nl。Nl比Nf更轻量化。此外，为了使用嵌入在视频中的长程时间信息，时间特征Ft在整个视频上维持，其通过所提出的循环特征更新RFU在关键帧上逐渐更新。通过时间特征的辅助，关键真的语义特征也被RFU增强，有利于最终的任务。因此，提出了DenseFT模块通过传播来自时间的特征Ft来丰富其特征。这个设计的关键假设是，非关键帧的内容与相邻的关键帧相似。RFU和DenseFT的核心是将时间特征对齐并传播到当前处理帧的时间特征上，通过PSLA模块实现。</p><h3 id="PSLA"><a href="#PSLA" class="headerlink" title="PSLA"></a>PSLA</h3><p>我们框架的核心是对齐和传播帧之间的特征图。为了这个目的，介绍PSLA一个新奇的模块，旨在建立两个特征图之间的空间一致性，来传播其特征。</p><p>PSLA首先基于一对特征单元之间的特征相似度计算<strong>一致性权重</strong>，源自两张不同特征图，分布在渐近的稀疏步长。</p><p>图3，沿着水平、垂直方向的的光流场边缘分布都集中在0附近。这表明用于计算一致性权重的特征单元，可以通过一个渐进的稀疏步长被限制到一个邻域内。这种设置允许PSLA更多的关注相邻位置（与小运动相关），较少的关注较远的位置（与大运动相关），也符合视网膜视觉感知组织的特点。</p><p>嵌入式函数f和g被用于减小通道的维数（实际通过256个1x1的卷积层实现），减少计算量。PSLA对比每个g(Ft+ε)特征单元和周围局部稀疏位置的f(Ft)单元。产生的特征相似性被正规化，来产生权重，用于对齐Ft。特征单元的相似性越高，表明更高的一致性，权重也会更高，其信息将会被更大的传播到一个新的特征单元。最终，对齐过的特征被传播到第t+ε帧。到此为止，作者并未阐明Ft和Ft+ε来自哪里，第3.3和3.4节将详细阐明。</p><p>PSLA的操作可被形式化为两步：1.基于特征相似性产生稀疏一致性权重。给出两张特征图，通过两个嵌入式函数f和g。2.Ft可被Ft+ε对齐，用一致性的权重聚合一致性的特征单元</p><p>通过引入softmax作为正规化，我们强制权重彼此竞争，因此，PSLA能捕捉最相似和最重要的特征。与注意力机制类似，可以隐含的估计两张特征图之间的一致性。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp8rhoiaj316c0q4124.jpg" srcset="/img/loading.gif" alt=""></p><blockquote><p>PSLA的目的是用一个注意力方式对齐特征图Ft到Ft+ε，可被形式化为两步：1.在嵌入特征图f（Ft+ε）的每个特征单元对比在特征图g（Ft）的周围单元，从里到外是一个渐进的稀疏步长。g(Ft)中的不同颜色的区域代表不同的步长。所得到的特征相似性被用于计算一致性权重Ct,t+ε,其捕捉特征之间的空间一致性。2.Ft中选择的特征单元被一致性权重聚合，产生F^t+ε的特征单元，其是Ft对齐过的特征图。</p></blockquote><h3 id="循环特征更新RFU"><a href="#循环特征更新RFU" class="headerlink" title="循环特征更新RFU"></a>循环特征更新RFU</h3><p>视频提供了丰富的信息，有助于目标检测。例如临近帧的视觉线索和时间上下文。然而，图像目标检测器忽略了视频序列之前帧的表观和上下文信息。这鼓舞作者提出循环特征更新（RFU）。RFU是一个沿着时间聚合稀疏关键帧的语义特征的过程，旨在利用时间上下文提升检测精度。</p><p>RFU用稀疏关键帧的语义特征，在整个视频上循环的维持和更新时间特征Ft。在这个过程中，直接用新关键帧的特征更新Ft会出问题，因为目标在视频中的移动将产生未对齐的空间特征。因此，利用PSLA来增强Ft和新关键帧的高级特征之间的空间一致性。</p><p>在对齐时间特征之后，一个小的Update神经网络被设计出来，来自适应的融合Fh和F^h。<strong>Update Net的输入是F\^t和Fh。输出是自适应的权重W\^k和Wk,</strong>中间有多个卷积层，其中W\^和W表示两张不同特征图的每一个空间位置的重要性。其被归一化，之和为1。最终Ft基于这个权重更新：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp8icye2j30iw02ugly.jpg" srcset="/img/loading.gif" alt=""></p><p><strong>（Fh是新关键帧的特征图，Ft是之前维持的时间特征图，F^t是经过PSLA对齐的结果。最后将对齐结果和当前帧的特征图输入UpdateNet进行加权和，生成新的Ft。）</strong></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp8a013uj313u0muqck.jpg" srcset="/img/loading.gif" alt=""></p><p>最终，更新过的特征Ft被用于代替Fh来产生关键帧Ik的结果，并作为更新过的时间特征。</p><h3 id="密集特征变换DenseFT"><a href="#密集特征变换DenseFT" class="headerlink" title="密集特征变换DenseFT"></a>密集特征变换DenseFT</h3><p>由于使用Nl提取的非关键帧特征效果不好，作者介绍了DenseFT，来通过特征变换和传播来自时间特征Ft的特征，生成<strong>非关键帧</strong>的语义特征。</p><p>特别的，所提取的低级特征Fl被用于PSLA来从最近的关键帧传播来自Ft的语义特征。然而，这些低级特征不包含有助于寻找空间一致性的充足的的语义信息。对齐过的特征可能不包含重要信息。为了解决这一问题，作者提出了一个轻量级的网络<strong>Transform Net</strong>，来进一步的编码所提取到的低级特征，旨在与高级语义特征近似。这是核心的一步，因为其不仅<strong>丰富了低级特征的语义信息</strong>，并且避免了特征传播的梯度直接流向Nl，因此提升了训练的鲁棒性。编码过的特征被喂给PSLA来与Ft对齐。</p><p>在将Ft传播到非关键帧之后，再次使用低级特征Fl进行融合。这是由于特征对齐过程中加权聚合引起的混叠效应，可能会使传播的特征丢失一些对识别很重要的目标外观细节。为了实现这一目标，一个Quality Net网络被嵌入在DenseFT中来补充细节信息。最终，QualityNet的输出被喂给任务网络Nt来产生非关键帧的结果。</p><p><strong>（Fl是非关键帧的特征图，首先使用TransformNet进行编码，丰富低级语义信息。之后与关键帧的Ft一起输入PSLA进行传播对齐。对齐结果再次与低级特征Fl通过QualityNet进行加权融合，输出结果喂给任务网络进行检测。QualityNet的结构和UpdateNet一致）</strong></p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp82oua0j30uk0hiwk1.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文提出了一个新颖的视频目标检测框架，核心是PSLA模块，用于有效的传播特征。此外提出了RFU和DenseFt，来对时间表观建模、增强特征表示。在ImageNet VID上达到81.4%的mAP。</p><h1 id="2-Video-Object-Detection-via-Object-Level-Temporal-Aggregation"><a href="#2-Video-Object-Detection-via-Object-Level-Temporal-Aggregation" class="headerlink" title="2.Video Object Detection via Object-Level Temporal Aggregation"></a>2.Video Object Detection via Object-Level Temporal Aggregation</h1><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>本文提出了通过时间聚合提升VID的方法。检测模型被用于稀疏关键帧来处理新目标、遮挡和快速运动。之后使用实时跟踪器来利用时间线索、在剩余帧跟踪检测到的目标，这增强了效率和时间一致性。bbox级别的目标状态通过我们的聚合模块被跨帧传播。此外，提出了一个关键帧选择策略，使用强化学习和简单的启发式，提出一个自适应的策略。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>作者认为在更高级别的聚合会更有效，并且比特征级别的聚合更通用。作者提出在目标/bbox级别聚合信息。例如检测模型的输出。首先目标状态的维度相比CNN特征图大大减少，其次，它解开了时间聚合与特征提取之间的束缚。</p><p>当采用一个不同的特征提取器或者不同的时间动态，memory-guided特征聚合需要重新训练整个模型。相反，box的预测能被简单的聚合，无需关注特征图的语义。</p><p>为了利用时间线索进行目标级别的聚合，一个直观的选择是跟踪模型。Detect and Track（D&amp;T）方法在一个联合训练的网络执行检测和跟踪，但其巨大的计算量在现实场景中不可接受。为了加速，作者提出了时间聚合模块，在目标级别整合检测和跟踪信息。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpe48xpej31620mae1w.jpg" srcset="/img/loading.gif" alt=""></p><p>如图1所示，该框架在稀疏的关键帧上应用检测，并且通过实施跟踪器传播目标状态。因为检测是在稀疏的关键帧上，因此，关键帧选择策略变得十分重要。<strong>作者对比检测和跟踪模型的正反面，通过强化学习RL和简单的启发式，提出一个自适应的关键帧选择策略，</strong>实验表明，关键帧选择策略可以生成各种检测率，对比固定的间隔性能显著提高。</p><p>通过所提出的聚合模块和关键帧选择策略，作者展示了用目标级别的时间聚合实现有竞争力的速度精度平衡的可能性。本文的主要贡献：</p><ul><li>提出一个时间聚合模块，在目标/bbox级别整合目标检测和跟踪模型</li><li>展示了一个用简单的启发式和不同的视频序列训练的强化学习训练的<strong>关键帧选择策略。</strong></li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h1><p>现有方法主要都是基于单图像目标检测器，作者的框架进一步合并了目标跟踪模型。以下简略介绍了最新的目标检测器和跟踪器，并将视频方法分为三类：基于跟踪的、基于光流的、memory-guided 聚合。</p><h3 id="单图像目标检测"><a href="#单图像目标检测" class="headerlink" title="单图像目标检测"></a>单图像目标检测</h3><p>先提出候选区域，之后进行精炼和分类(Two stage)：RCNN系列。</p><p>基于预训练的Anchor box（One stage）：YOLO、SSD。</p><p>自底向上的方法进一步探索了不用anchor box检测的可能性。：Centernet、CornerNet、Object as points、Bottom-up object detection by grouping extreme and center points.</p><h3 id="目标跟踪"><a href="#目标跟踪" class="headerlink" title="目标跟踪"></a>目标跟踪</h3><p>目标跟踪是一种常见的视觉任务，其目标是在整个视频序列中跟踪目标。给出初始化bbox作为目标模板，跟踪模型可以通过相关或CNN过滤，估计当前目标位置。现有的大多数方法假定相邻帧之间目标表观是时间平滑的。然而，变形、遮挡、巨大的运动都会对这一假设构成威胁，使得执行关联和更新目标状态变得困难。幸运的是，我们的场景不包含长程跟踪和多目标关联。关联问题被自然的避免，疑问我们只考虑目标类别，不考虑其身份。考虑到速度需求，作者选择Kernelized Corelation Fileter(KCF)和Fully-convolutional Siamese Network(SiamFC)作为实验中的跟踪器。</p><h3 id="基于跟踪的聚合"><a href="#基于跟踪的聚合" class="headerlink" title="基于跟踪的聚合"></a>基于跟踪的聚合</h3><p>作为单图像到视频的一个直观的扩展，Seq-NMS方法跨帧连接一个目标的bbox，之后随着跟踪的进行，重新分配最大的或平均的置信度分数给box。TCN方法在重新分配置信度之前，通过光流和跟踪算法联合boxes。以上这两种方法都能提供3-5%的mAP提升，但其都是offline的后处理方法。D&amp;T框架提出同时学习检测和跟踪。受益于多任务训练，其在两个任务上都获得了相当大的精度提升，但其巨大的计算量限制其用于移动设备。</p><h3 id="光流指导的聚合"><a href="#光流指导的聚合" class="headerlink" title="光流指导的聚合"></a>光流指导的聚合</h3><p>另一类方法通过光流在特征图级别聚合时间信息。DFF方法在确定的关键帧上应用昂贵的特征提取，之后传播特征图到剩余帧。尽管努力加速，大多数光流方法实际上都比关注于mobile的模型更慢。此外，挺能决定于光流估计，其需要密集的标注数据用于训练。</p><h3 id="Memory-Guided-聚合"><a href="#Memory-Guided-聚合" class="headerlink" title="Memory-Guided 聚合"></a>Memory-Guided 聚合</h3><p>最近关注于mobile的模型依赖于memory模型。Mobile video object detection with temporally-aware feature map这篇文章将LSTM单元插入到卷积层之间，来传播时间线索、精炼特征图。Looking fast and slow在大小特征提取器之间提出了一个平衡。特征提取器随着memory模型训练，用于特征聚合。尽管有联合优化的优点，当适应具有明显时间属性的未知领域时，这种结合需要强化训练。作者在其网络框架中，采用一个相似的memory模块用于时间聚合，但信息聚合是在<strong>object level</strong>的。</p><h3 id="自适应关键帧选择策略"><a href="#自适应关键帧选择策略" class="headerlink" title="自适应关键帧选择策略"></a>自适应关键帧选择策略</h3><p>大多数现有方法选用固定的关键帧选择策略，以平衡大的和轻量级的计算。许多自适应策略是为了目标跟踪和视频分割提出的。对目标检测来说，Optimizing video object detection via a scale-time lattice这篇文章通过offline度量时间传播的难度来选择关键帧。……本文强调关键帧选择的重要性，并提出两个自适应的策略：一个启发式策略和一个轻量级RL模型。</p><h1 id="3-Proposed-Approach"><a href="#3-Proposed-Approach" class="headerlink" title="3.Proposed Approach"></a>3.Proposed Approach</h1><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpecg81vj314y0nyqh3.jpg" srcset="/img/loading.gif" alt="image-20210320214402666"></p><p>理想状态，一个检测模型可以捕捉目标的存在与否。每一帧的预测独立生成，因此，对快速运动或场景变化具有很强的鲁棒性。尽管如此，我们总是在检测结果中观察到时间不一致性，特别是当目标轻微变形或部分遮挡。在另一方面，跟踪模型善于在给出初始表观后，利用时间线索来跟踪目标。其可以提供精确稳定的目标位置估计，同时计算量小。</p><p>为了利用检测模型和跟踪模型的优点，作者提出了一个框架，在目标基本将其整合。特别的，检测被用于关键帧，捕捉新目标及初始化跟踪器。跟踪器更新目标位置、在帧间传播边界框特征，促进目标关联，加速检测。作者提出的时间聚合，通过使用之前的预测来帮助稳定置信度分数和分类概率。</p><p>为了确保跟踪失败时也能触发检测，作者提出了自适应调度器来基于目标状态决定关键帧间隔。</p><h2 id="3-1时间聚合"><a href="#3-1时间聚合" class="headerlink" title="3.1时间聚合"></a>3.1时间聚合</h2><p>一个典型的检测模型对每个目标产生：置信度/目标分数c，分类概率p，候选框b。目标跟踪模型产生：候选框估计b~、跟踪 分数s（指示当前box与目标模板的匹配程度）。这些信息与跟踪持续时间一起被作为特征利用。</p><p>object-level的特征选择使我们的聚合与模型无关，可被用于任何产生上述输出的目标检测器和跟踪器。对于(c,p,b,s)中的每个特征x，将跟踪器传播产生的值表示为x~,聚合过的输出表示为x‘。</p><p>在每个关键帧，为每个新目标初始化一个跟踪器，并删除由于被遮挡或者消失的目标。被跟踪的目标与当前检测的IOU阈值相关。作者根据惯例，将IOU阈值设为0.5。一旦发生相关，已经存在目标的特征将与新检测到的聚合。如图2b所示，作者采用一个memory模型，将跟踪的信息和新的检测作为输入，更新内部状态，产生一个聚合过的输出。作者的memory模型与速度优化的Bottleneck-LSTM类似，与之不同的是，作者使用两个全连接层替代卷积操作，因为作者的输入是目标级别的特征，而非卷积特征图。</p><p>与之前的memory-guided方法不同，作者的聚合模块输入的维度很低，仅仅应用于稀疏的关键帧，因此推理时间更短。</p><p>除了基于学习的聚合，作者还提出了一个简单高效的启发式模块来缓和三个预测的不一致性。1.边界框坐标 2.分类概率 3.置信度分数。</p><p>首先，为了产生精确的和时间平滑的候选边界框，作者通过跟踪产生的边界框b~来精炼检测产生的边界框b。坐标基于检测置信度和跟踪分数进行平均。当新的检测结果比跟踪结果的置信度更高，就对检测框分配更高的权重。这个强化过程可被表示为。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpejcvz8j30fu04s3z8.jpg" srcset="/img/loading.gif" alt="image-20210320214414272" style="zoom:50%;" /></p><p>其中c~和s~表示置信度和最小跟踪分数（从之前关键帧到当前时间t）</p><p>第二，我们合并之前关键帧的分类概率，来保证类别预测的一致性。概率通过置信度分数被重新加权，并且被累加求平均：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpep44uwj30me05k75o.jpg" srcset="/img/loading.gif" alt="image-20210320214423031" style="zoom:50%;" /></p><p>其中K是关键帧集合，γ是之前预测的权重下降率。最终，置信度分数通过基于跟踪的重新赋值趋于稳定。作者在关键帧保持跟踪的置信度分数，并通过时间最大化更新当前置信度。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpetu9g4j30e203aglz.jpg" srcset="/img/loading.gif" alt="image-20210320214431017" style="zoom:50%;" /></p><p>这旨在从高置信度的帧之间选择低置信度的检测。第t帧的最终检测结果是</p><h2 id="3-2-自适应关键帧调度器"><a href="#3-2-自适应关键帧调度器" class="headerlink" title="3.2 自适应关键帧调度器"></a>3.2 自适应关键帧调度器</h2><p>固定的关键帧间隔忽略了时间变化，不能适应一些确定的事件（一个模型比其他模型效果好）。如果一个视频相当平滑，我们更喜欢跟踪而不是检测，因为其计算量更小。相反，当目标变化超过跟踪器的能力的时候，就需要频繁的检测。例如运动模糊、变形和遮挡。更重要的是，关键帧的选择决定了用于初始化跟踪器的目标模板，这将显著影响跟踪性能。</p><p>作者在实验中发现，选择目标出现的第一帧并不能产生最优的跟踪结果。相反，在第一次出现之后的一些帧有代表性的包含更可靠的和完整的模板来初始化目标跟踪器。作者也发现，跟踪分数与其他object-level的特征同样有用，并且便于指示跟踪和检测的精度。增加更多image-level的特征对关键帧选择仅提供微不足道的信息。</p><p>受以上观察的启发，作者提出了一个自适应策略，来动态的在线调整关键帧间隔D。这个任务可被形式化为一个RL问题和之后的状态空间、动作空间、奖励函数。状态向量S从相同的object-level特征构建，被用于时间聚合。对每个特征x，在S中编码最小时间xmin，最大时间xmax,平均时间xmean,变化xvar，差异xt-xt-1来促进RL训练。</p><p>给出当前帧的状态，agent(代理)从动作空间学习预测一个动作a：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpf4gkscj313s08aq6h.jpg" srcset="/img/loading.gif" alt="image-20210320214446926"></p><p>其中α是一个固定的乘法因子。当且仅当跟踪时长dt&gt;=Dt时，应用检测模型。</p><p>对于奖励R，作者在预测框b’和ground truth box b^之间计算平均IOU：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpf87e55j30du02kjrt.jpg" srcset="/img/loading.gif" alt="image-20210320214454058" style="zoom:50%;" /></p><p>作者采用标准的A3C训练体系，其在连续域和离散域均有效。考虑到目标是无序的，作者对policy和value网络采用PointNet和3个全连接层。这个模型原始被用于点云数据来保持顺序不变。在本文的工作中，作者将每个目标看作一个点数据，并且根据数据集分布设置一个最大目标数量。极端目标通过置信度分数剔除。</p><p>尽管状态空间和动作空间是低维的，视频序列的差异使得训练RL有挑战。相同的状态中相同的动作可能导致在不同视频中不同的奖励和不同的下一个状态。例如奖励和状态转移在RL agent看来是随机的。在奖励函数中结合实践和准确性约束是另一个重要问题。作者观察到，简单的加权和可能会造成不稳定的训练结果。速度奖励的一个细微不同容易导致局部最优，即策略应用检测的频率要么最高，要么最低。此外，当需要一个新的折衷点时，需要用新的奖励设置来训练策略</p><p>作者提出以下策略来稳定训练过程，产生更通用的策略。首先，我们在专家监督下通过预先训练策略网络来模拟模仿学习的概念。针对穷举搜索最优关键帧不可行的问题，提出了一种贪婪算法来近似oracle监控策略。</p><p>如算法1中所述，我们从一个固定间隔的关键帧集开始，然后迭代地<strong>用一个跟踪得分最低的非关键帧替换跟踪得分最高的关键帧。</strong>为不同的速度-准确性权衡训练一个通用策略，一个基础的间隔偏移Dbase被随机初始化为5-30帧之间。跟踪时长和基础间隔的比值d/Dbase，在状态向量中编码，这样<strong>agent</strong>就能知道相对于基频的相对检测率。</p><p>为了补偿不同视频之间不同的奖励分配，我们计算IOU分数的时间差异而不是原始值。它允许代理根据时间动态专注于策略优化。最后，我们在一个事件的结尾添加一个长期的惩罚，以限制策略过度或被动的检测。修改后的奖励函数Rt 可以表示为:</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpfh5a2aj315c05mq52.jpg" srcset="/img/loading.gif" alt="image-20210320214508242"></p><p>其中η表示监测历史，λ是长程惩罚权重。详细过程在算法2中。</p><p>为了对比RL策略，我们提出一个启发式调度器，将跟踪分数映射到关键帧间隔。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqpfku7yhj30kg02wt99.jpg" srcset="/img/loading.gif" alt="image-20210320214513906" style="zoom:50%;" /></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>作者提出了一个新的框架，通过跨帧的目标跟踪提升检测。利用时间信息和实时跟踪器聚合的信息，使得检测更一致和有效。启发式和RL策略被提出，用于自适应关键帧调度。更进一步，object-level的聚合减轻了对特征图的依赖，使得其对变幻的检测和跟踪模型更通用。当前，作者为每个目标初始化一个跟踪器，这当跟踪多个目标时，线性增加了推理时间。未来作者将使用多目标跟踪来加速模型，并且并行地运行检测和跟踪模型。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PLSA</tag>
      
      <tag>OLTA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（1.20)</title>
    <link href="/2021/01/20/1-20/"/>
    <url>/2021/01/20/1-20/</url>
    
    <content type="html"><![CDATA[<p>[1]Frankle J, Carbin M. The lottery ticket hypothesis: Finding sparse, trainable neural networks[J]. arXiv preprint arXiv:1803.03635, 2018.</p><p>[2]Frankle J, Dziugaite G K, Roy D M, et al. Stabilizing the lottery ticket hypothesis[J]. arXiv preprint arXiv:1903.01611, 2019.</p><p>[3]Girish S, Maiya S R, Gupta K, et al. The Lottery Ticket Hypothesis for Object Recognition[J]. arXiv preprint arXiv:2012.04643, 2020.</p><p>[4] Chen K, Wang J, Yang S, et al. Optimizing video object detection via a scale-time lattice[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 7814-7823.</p><h1 id="1-LTH"><a href="#1-LTH" class="headerlink" title="1.LTH"></a>1.LTH</h1><p>本周阅读的前三篇文章都是关于LTH的，LTH是2019年提出的一种模型剪枝方法，可以有效的减少模型参数、同时保证模型的精度。</p><p>LTH基于这样一个假设：一个随机初始化的密集神经网络，都包含一个子网络，可以在训练最多相同迭代次数后，与原始网络的精度匹配。</p><h2 id="过程："><a href="#过程：" class="headerlink" title="过程："></a>过程：</h2><p>1.随机初始化一组值，保存下来。开始训练，每次迭代完将参数初始化为上述值，进行剪枝。</p><p>（该训练过程与原始一样，只是加了剪枝的过程）</p><p>2.在数据集上迭代训练n次，训练哪些参数可以被剪掉 （对权重排序，每次剪掉后$p^{\frac1n}\%$) p是想减掉的百分比。</p><p>​    对想要剪掉的部分加mask置为0，下次迭代不为其赋值。</p><p>3.剪好的网络在原始数据集上重新训练。</p><blockquote><p>使用训练好的原始的任务网络，来训练剪枝网络，让剪枝网络来学习哪些参数可以移去。训练完之后，使用mask保证置为0的网络不再被赋值。最后用原始训练集重新训练剪枝过后的网络。</p></blockquote><p>Dense：大多数参数都是非零。Sparse:大多数参数都为0.</p><p>可提升速度精度。</p><p>每次迭代后权重赋为与第一次相同的值，而不是随机初始化</p><h2 id="全连接网络MINIST"><a href="#全连接网络MINIST" class="headerlink" title="全连接网络MINIST"></a>全连接网络MINIST</h2><p>在随机初始化、训练一个网络之后进行剪枝，将剩余的连接置为原始的初始化值。</p><p>采用原始的逐层剪枝策略：在每一层中移除最低量级的权重。与输出相连的剪枝率是网络其他部分的一半。</p><h4 id="迭代剪枝："><a href="#迭代剪枝：" class="headerlink" title="迭代剪枝："></a>迭代剪枝：</h4><p>在第一个剪枝round，剪得越多精度越高。</p><h2 id="LTH-Object-Detection"><a href="#LTH-Object-Detection" class="headerlink" title="LTH_Object Detection"></a>LTH_Object Detection</h2><p>目前大多数视觉识别方法的步骤是：1.在一个非常大的图像分类数据集上预训练一个模型 2.在预训练的模型上增加一个小的任务网络，对特定的任务在一个更小的数据集上联合微调预训练网络。</p><p>近些年为了减少深度网络的内存和计算量，目前主要有三种思想：</p><ol><li>Weight quantization</li><li>Sparsity via regularization</li><li>Network pruning</li></ol><p><strong>权重量化方法</strong>通过用低精度的值替代训练好的网络权重，或者用逐bit的算术操作 来减少memory一个数量级。</p><p><strong>正规化方法</strong>（如Dropout或LASSO）尝试阻止过度参数化的网络依赖于大量的特征，并鼓励学习稀疏且健壮的预测器。</p><p>以上两种方法对于减少权值的数量或内存用量很有效，但是一般会增加错误率。</p><p>相比之下，剪枝方法交替使用权重优化和权重删除来解决学习任务。（LTH属于此类）</p><p>卷积神经网络是网络结构中计算量最大的部分，预训练是最耗时的部分。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp28cl39j30vo0hmdo9.jpg" srcset="/img/loading.gif" alt=""></p><h1 id="2-Optimizing-Video-Object-Detection-via-a-Scale-Time-Lattice"><a href="#2-Optimizing-Video-Object-Detection-via-a-Scale-Time-Lattice" class="headerlink" title="2.Optimizing Video Object Detection via a Scale-Time Lattice"></a>2.Optimizing Video Object Detection via a Scale-Time Lattice</h1><p>为了平衡精度和效率，此前的方法关注于优化模型结构。本文提出了另一种观点：在scale-time space上重新分配计算。基本思想是稀疏的执行昂贵的检测，通过利用其强相关性，用代价低的网络 跨scale和time传播结果。</p><p>本文展示了一个统一的框架，结合了检测、时间传播、在Scale-Time Lattice上跨尺度优化。</p><p>给出一段视频，所提出的框架首先基于目标的运动和尺度，自适应的、稀疏的选择关键帧，并在关键帧上应用昂贵的目标检测器，来得到用于传播的有效边界框。之后，这些边界框被传播到中间的非关键帧，并通过更廉价的网络跨尺度优化（由粗到细）。通过仅稀疏调用检测器，该框架显着降低了摊销成本，同时以便宜但非常有效的传播组件保持了竞争性能。</p><p>本文贡献：</p><ol><li>所提出的Scale-Time Lattice提供了一个联合的视角和丰富的设计空间。</li><li>在此基础上设计了一个检测框架，实现了更好的速度/精度权衡</li><li>提出了一个更高效的时间传播网络和一个自适应的关键帧选择框架</li></ol><h3 id="Scale-Time-Lattice"><a href="#Scale-Time-Lattice" class="headerlink" title="Scale-Time Lattice"></a>Scale-Time Lattice</h3><p>尺度时间格，将基于稀疏图像的检测和密集视频检测结果的构建统一到一个框架。</p><p>如图2所示，Scale-Time Lattice被形式化为一个有向非循环图。每个节点表示在一个确定的尺度和时间点的检测结果，每条边表示从一个节点到另一个节点的操作。作者定义了李郎中操作：时间传播、空间优化，分别对应图中的水平和垂直边。特别的，时间传播边（蓝色）在相同的空间尺度的相邻时间点连接节点。空间优化边（绿色）在相同时间点，不同的尺度连接节点。通过这幅图，检测结果将沿着上述的确定路径，从一个节点传播到另一个节点。最终，视频检测结果可以从最下面一行的节点中得到，这些节点的尺度最细，覆盖了每一个时间步长。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp2hkzyqj31600ikh1y.jpg" srcset="/img/loading.gif" alt=""></p><h3 id="Propagation-and-Refinement-Unit（PRU）"><a href="#Propagation-and-Refinement-Unit（PRU）" class="headerlink" title="Propagation and Refinement Unit（PRU）"></a>Propagation and Refinement Unit（PRU）</h3><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp2q8choj30es0a4gma.jpg" srcset="/img/loading.gif" alt="image-20210320213253456"></p><p>PRU的输入是两个关键帧的检测结果，将其转播到中间帧，然后优化输出到下一个尺度。</p><h3 id="关键帧选择策略"><a href="#关键帧选择策略" class="headerlink" title="关键帧选择策略"></a>关键帧选择策略</h3><p>作者观察到：当目标很小并且移动很快时，时间传播得到的的特征不如单帧基于图像的检测。因此，关键帧的密度应该由传播的难度决定。也就是说，应该在小目标、运动快的目标出现的时候，选择更多的关键帧。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LTH</tag>
      
      <tag>STL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（1.10)</title>
    <link href="/2021/01/10/1-10/"/>
    <url>/2021/01/10/1-10/</url>
    
    <content type="html"><![CDATA[<p>最近因为准备期末考试，blog停更了一段时间。</p><ul><li>Jiang Z, Gao P, Guo C, et al. Video object detection with locally-weighted deformable neighbors[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33: 8529-8536.</li><li>Zhu X, Dai J, Yuan L, et al. Towards high performance video object detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 7210-7218.</li></ul><h1 id="一、LWDN"><a href="#一、LWDN" class="headerlink" title="一、LWDN"></a>一、LWDN</h1><h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h1><p>LWDN（局部加权的变形邻域）不使用耗费时间的光流提取网络，可以潜在的对齐高级特征。使用空间上不同的权值来合并相对应的邻域。为了使用时间信息，将两帧之间的差异作为输入，来预测权值。此外，还预测了偏移量来自适应的合并邻域。</p><p>本文提出的LWDN模块，使用特征加权的方式来利用时间信息。LWDN学习预测位置敏感的权重，权重被用于传播关键帧的特征到关键帧或非关键帧。此外还使用了时间一致性信息，采用brain-inspired memory mechanism来传播和更新关键帧和关键帧之间的memory特征。</p><h1 id="2-LWDN"><a href="#2-LWDN" class="headerlink" title="2.LWDN"></a>2.LWDN</h1><p>LWDN有3个重要的模块：Weight predictor network、feature correlation、aggregation unit</p><p>作者个单个视频帧的特征分为4个部分：</p><ol><li>low-level feature（the lower-part of CNN）</li><li>high-level feature（higher-part of CNN）</li><li>task feature(负责最终的检测)</li><li>memory feature（被用于在关键帧和关键帧之间传播memory feature）</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqoyws4inj312c0ri7ff.jpg" srcset="/img/loading.gif" alt=""></p><blockquote><p>图：LWDN的过程，作者将其称为：自适应位置敏感的特征传播。k表示之前的关键帧，Tk表示之前关键帧的task feature。当低级特征Lk被计算出后，Weight Predictor Network将Lk和Lk+i作为输入，产出位置敏感的kernel weights和对应的kernel offsets。然后，之前关键帧的Task特征Tk将被空间不同的变形卷积（使用预测出的kernel weights 和 offsets），传播到非关键帧。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqozd1s1wj315k0oqqfo.jpg" srcset="/img/loading.gif" alt=""></p><blockquote><p>图：Memory-Guided Propagation Networks，包括两个过程：关键帧到关键帧的推理：更新memory feature、关键帧到非关键帧的推理：传播<strong>task feature</strong>。k0、k1是关键帧，k1+i是非关键帧，+号为聚合单元，x号为LWDN操作。</p></blockquote><h4 id="关键帧到非关键帧的过程："><a href="#关键帧到非关键帧的过程：" class="headerlink" title="关键帧到非关键帧的过程："></a>关键帧到非关键帧的过程：</h4><p>​    关键帧的task特征通过LWDN操作传播到非关键帧（使用Weight Predictor Network预测出的权重，Weight Predictor Network通过两个低级特征来产生权重）。</p><h4 id="关键帧到关键帧的过程："><a href="#关键帧到关键帧的过程：" class="headerlink" title="关键帧到关键帧的过程："></a>关键帧到关键帧的过程：</h4><p>​    首先两个关键帧的低级特征，被喂给Weight Predictor Network产生位置敏感的权重和偏移。之后LWDN操作通过使用旧关键帧memory feature和权重、偏移，来生成对齐过的特征。最终对齐过的特征和新关键帧的高级特征一起被聚合为任务特征和新的memory feature（将被循环传播到下一个关键帧）</p><h2 id="1-Weight-Predictor-Network"><a href="#1-Weight-Predictor-Network" class="headerlink" title="1.Weight Predictor Network"></a>1.Weight Predictor Network</h2><p>权重预测网络用于生成空间变化的权重和偏移，输入两个低级特征，按axis=0生成串联特征。之后使用3x3的卷积将串联特征减小到256维。然后沿着axis=0切开，得到reduced low-level 特征。之后关联reduced low-level 特征，使用256通道的3x3卷积核、softmax操作，来生成位置敏感的权重。对于关键帧和关键帧之间的W1，另加一个1x1的卷积核，2xkxk个通道 来生成位置敏感的offsets。</p><h2 id="2-Feature-Correlation"><a href="#2-Feature-Correlation" class="headerlink" title="2.Feature Correlation"></a>2.Feature Correlation</h2><p>使用相关性对相邻的两个低阶特征映射进行乘法路径比较，作为Weight Predictor Network的输入。给出两个级别的特征图f1、f2、以及w、h、c（宽、高、通道数），相关层使得网络对比f1和f2的每一块，以帮助得到这两帧更好的判别器。</p><h2 id="3-聚合单元"><a href="#3-聚合单元" class="headerlink" title="3.聚合单元"></a>3.聚合单元</h2><p>特征的聚合权重通过一个质量估计网络Nq来生成。其有三个随机初始化的层：3x3x256卷积、1x1x16卷积、1x1x1卷积。输出是逐位置的原始分数图，将被应用于对应特征的每一个通道。不同特征的原始分数图将被归一化，并且通过加和来获得融合过的特征。</p><h1 id="二、THP"><a href="#二、THP" class="headerlink" title="二、THP"></a>二、THP</h1><p>本文提出了一种更快、更精确、更灵活的视频目标检测方法，主要有三个技术：</p><ol><li><p>将<strong>稀疏循环特征聚合</strong>用于保持聚合的特征质量，通过只在稀疏<strong>关键帧</strong>上操作，减少计算开销。</p><p>​    利用DFF和MEGA的互补属性，合并了DFF和MEGA的方法，速度和精度同样好。</p></li><li><p>引入空间自适应的局部特征更新，来在传播特征质量较差的非关键帧上重新计算特征。在端到端的训练中，通过一个新的公式来学习特征质量。（该技术进一步提升了识别精度）</p><p>​    将时域自适应特征计算扩展至空间域，得到空间自适应特征计算，更高效。</p></li><li><p>使用时间自适应的关键帧选择策略，来替代之前固定的关键帧选择。根据上面预测出的特征质量预测一个关键帧的使用。（使得关键帧的使用更高效）</p><p>​    提出自适应关键帧选择策略，进一步提升了特征计算效率。</p><p>​    </p></li></ol><h3 id="DFF：稀疏特征聚合"><a href="#DFF：稀疏特征聚合" class="headerlink" title="DFF：稀疏特征聚合"></a>DFF：稀疏特征聚合</h3><p>首次引入关键帧的概念，只在关键帧（每10帧）上执行特征网络。通过运动场（光流网络）将之前的关键帧的特征传播到非关键帧。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqozpllb4j30mu02e3z1.jpg" srcset="/img/loading.gif" alt=""></p><h3 id="FGFA-密集特征聚合"><a href="#FGFA-密集特征聚合" class="headerlink" title="FGFA:密集特征聚合"></a>FGFA:密集特征聚合</h3><p>首次引入时间特征聚合的概念，在所有帧上执行特征网络（每一帧都为关键帧）。将当前帧前后的所有帧的特征都加权平均到当前帧，（权重通过一个嵌入特征来计算）。由于在密集的连续帧上重复的光流估计和特征聚合，精度有所提高，但速度相比DFF慢了三倍。</p><p>DFF和FGFA都是基于相同的准则：</p><ol><li>运动估计模块不可或缺</li><li>所有模块在多帧上端到端的学习对于检测精度十分重要。</li></ol><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqozzwgk6j30u00zpwql.jpg" srcset="/img/loading.gif" alt=""></p><h2 id="1-稀疏循环特征聚合"><a href="#1-稀疏循环特征聚合" class="headerlink" title="1.稀疏循环特征聚合"></a>1.稀疏循环特征聚合</h2><p>尽管密集特征聚合（FGFA）显著提升了检测精度，但是很慢。一方面，没有必要对相邻帧执行密集的特征网络，因为他们的表观都比较相似。另一方面，需要通过多个光流场估计帧间运动，然后将多个特征图聚合，这进一步减慢了检测器。</p><p>这里提出的稀疏循环特征聚合，只在稀疏的关键帧上执行特征网络和循环特征聚合。给出两个连续的关键帧k和k’,第k‘帧聚合结果为：</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp0cgtzvj30uy07yaev.jpg" srcset="/img/loading.gif" alt=""></p><p>原则上，聚合过的关键帧特征 ${\overline{F}}_{k}$聚合了之前所有历史关键帧的丰富信息。</p><h2 id="2-空间自适应局部特征更新"><a href="#2-空间自适应局部特征更新" class="headerlink" title="2.空间自适应局部特征更新"></a>2.空间自适应局部特征更新</h2><p>尽管DFF的稀疏特征聚合通过近似真实的特征 $F_{k}$实现了显著的速度提升，但是传播后的特征图$F_{k-&gt;i}$由于临近帧之间的表观变化容易出错。</p><p>对于非关键帧，我们希望进行特征传播以高效的计算。然而等式(1)取决于传播质量的好坏。为了量化传播后的特征$F_{k-&gt;i}$是否是$Fi$的一个好的近似，作者引入了一个特征时间一致性度量$Q_{k-&gt;i}$，将其作为光流网络$N_{flow}$的一个姊妹分支，与$M_{i-&gt;k}$一起进行预测，</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp0kw5nzj30o2026751.jpg" srcset="/img/loading.gif" alt=""></p><p>如果预测出的$Q_{k-&gt;i}\leqτ$，则传播后的特征$F_{k-&gt;i}(p)$与真实特征$F_{i}(p)$不一致。也就是说$F_{k-&gt;i}(p)$是一个不好的近似，应使用实际特征$F_{i}(p)$更新。</p><h2 id="3-时间自适应关键帧选择"><a href="#3-时间自适应关键帧选择" class="headerlink" title="3.时间自适应关键帧选择"></a>3.时间自适应关键帧选择</h2><p>只在稀疏的关键帧上评估特征网络$N_{feat}$对于提升速度非常重要。使用预先固定比率的策略选择关键帧很naive(例如，每L帧选取一个关键帧)。一个更好地关键帧选择策略应当在时间域上适应动态变化。可以基于上述的特征一致指示器$Q_{k-&gt;i}$来设计关键帧选择策略。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp0v5oikj30w009odjb.jpg" srcset="/img/loading.gif" alt=""></p><p>$Q_{k-&gt;i}(p)\leqτ$表示表观变化或者运动较大，导致特征传播质量较差。$N_p$表示位置p的总数量。如果所有像素点的$Q_{k-&gt;i}(p)\leqτ$之和大于$\gamma$，则该帧为关键帧。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp154144j31540hyqgk.jpg" srcset="/img/loading.gif" alt=""></p><p>上图橘色的是关键帧，其所有像素点的$Q_{k-&gt;i}(p)\leqτ$之和大于$\gamma$，(即由于运动、表观变化导致质量低，Q小，被选为关键帧）。蓝色的为非关键帧，其表观变化较小。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp1e8he6j314o0n47je.jpg" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>LWDN</tag>
      
      <tag>THP</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（12.11)</title>
    <link href="/2020/12/11/12-11/"/>
    <url>/2020/12/11/12-11/</url>
    
    <content type="html"><![CDATA[<p>最近重读了AAAI2020的TCENet这篇文章，对其有了更深入的理解。此外阅读了一篇ICCV2019的文章、一篇NIPS2016的文章。</p><h3 id="阅读文献："><a href="#阅读文献：" class="headerlink" title="阅读文献："></a>阅读文献：</h3><p>[1] He F, Gao N, Li Q, et al. Temporal Context Enhanced Feature Aggregation for Video Object Detection[C]//AAAI. 2020: 10941-10948.</p><p>[2] Deng H, Hua Y, Song T, et al. Object guided external memory network for video object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 6678-6687.</p><p>[3] Dai J, Li Y, He K, et al. R-fcn: Object detection via region-based fully convolutional networks[J]. Advances in neural information processing systems, 2016, 29: 379-387.</p><h1 id="1-TCENet重读"><a href="#1-TCENet重读" class="headerlink" title="1.TCENet重读"></a>1.TCENet重读</h1><p>本次重读主要针对算法伪代码部分，对其有了更深入的理解。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glisfua8s0j30oa0rejwn.jpg" srcset="/img/loading.gif" alt="image-20201210152340438" style="zoom:40%;" /></p><p>上述算法的第L3-L6行对特征buffer和时间步长进行了初始化。这里假设$s_{min}=10$,$s_{max}=40$,那么算法首先将s1-s41初始化为10。算法的L10-L13进行了特征对齐，注意其中高亮的一句。这里K=1，即j取-1或1。对于1-11帧，$si=10, max(1,i-10)=1, max(1,i+10)=i+10$，即前11帧选择的是分别是：第1帧和第11帧、第1帧和第12帧……进行聚合。从第12帧开始，若s12仍然为10（可能被L17更新），则选择第2帧和第22帧进行聚合。同理，第13帧选择第3帧和第23帧……</p><p>对于视频序列最后的几帧的情况，由于算法中的i取的是无穷大，没有对这种情况进行讨论。当i接近序列结束时，第L18行计算特征图时，可能就会计算失败，因为要计算的帧可能已经越界，不存在。进而L17行的步长也无法计算，L11行也无法计算。</p><h1 id="2-OGEMN"><a href="#2-OGEMN" class="headerlink" title="2.OGEMN"></a>2.OGEMN</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>现有的视频目标检测方法通过聚合和对齐多个相邻帧的整个特征图，来将时间信息传播到退化的帧。然而，由于受到特征图的低存储效率和脆弱的内容地址分配问题，这些现有方法没有充分强调长程时间信息。本文提出了第一个目标指导的外部memory网络，用于在线视频目标检测。存储效率问题通过目标指导的hard-attention选择性的存储有价值的特征来解决，并且长程信息通过存储在可寻址的外部数据矩阵中来保护。设计出的读/写操作来精确的在目标指导下传播/分配/删除多级memory特征。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glahekr0tej30ue0u0n6o.jpg" srcset="/img/loading.gif" alt="image-20201203105724288" style="zoom:33%;" /></p><h3 id="图a）密集聚合方法（FGFA、MaNet、STSN）"><a href="#图a）密集聚合方法（FGFA、MaNet、STSN）" class="headerlink" title="图a）密集聚合方法（FGFA、MaNet、STSN）"></a>图a）密集聚合方法（FGFA、MaNet、STSN）</h3><p>memory由相邻的多个特征图组成。Reading操作包含包含对齐和聚合所有memory特征图到当前帧。聚合过的特征图用于在当前帧检测。Writing操作发生在每次检测过后，下一帧相邻的特征图被复写到当前memory。</p><h3 id="图b）循环聚合方法-Towards-High-Performance-Video-Object-Detection、STMN、Video-object-detection-with-locally-weighted-deformable-neighbors"><a href="#图b）循环聚合方法-Towards-High-Performance-Video-Object-Detection、STMN、Video-object-detection-with-locally-weighted-deformable-neighbors" class="headerlink" title="图b）循环聚合方法(Towards High Performance Video Object Detection、STMN、Video object detection with locally-weighted deformable neighbors)"></a>图b）循环聚合方法(Towards High Performance Video Object Detection、STMN、Video object detection with locally-weighted deformable neighbors)</h3><p>memory仅由一个特征图组成。Reading和Writing操作同时发生。当memory特征图被对齐和聚合到当前帧时，聚合过的特征图被用于在当前帧检测，并且成为新的memory。这些方法非常快，并且能够在线进行因果推断。</p><p>上述密集聚合方法和循环聚合方法的memory都由检测网络中<strong>全尺寸的特征图</strong>组成，其尺寸和内容地址依赖于检测网络和输入帧的行为，因此，作者将这些memory称为”internal”。这种internal memory 有一些缺点：在一些<strong>密集方法</strong>中，通常有超过20个相邻帧被存储，来提供充足的信息。由于internal memory中存储的是全尺寸的特征图，一些与检测目标无关的冗余信息也被存储和传播，导致较低的存储效率。在循环方法中，所有之前的信息被把索道一个特征图中，其中，信息的空间位置只依赖于当前帧内容的位置。因此，当当前帧内容退化、表观突变、脱离视野时，因为当前聚合过的特征图将成为新的internal memory，长期的信息容易被中断。</p><h3 id="图C-OGEMN"><a href="#图C-OGEMN" class="headerlink" title="图C) OGEMN"></a>图C) OGEMN</h3><p>为了更好的利用时间信息，本文提出了OGEMN。通过外部memory，memory的尺寸和内容地址独立于检测网络和输入帧。首先，检测产生的目标信息提供了一个自然的hard attention来提升存储效率。通过有选择的存储检测到bbox 的特征，而非所有特征图，更多时间跨度大的信息可以被存储到相同大小的存储空间中。其次，在神经图林机(NTM)和记忆网络[37,31]的驱动下，可以通过将事前的时间信息存储到一个可寻址的data矩阵中，并只删除冗余的，来解决长期的依赖和推理能力问题。之前重要的信息可以被精确的recall，没有意外遗忘的风险。外部memory的尺寸随着存储特征的数量而改变。新颖的read和write操作被用于访问这个动态的外部memory，其中attention-based read操作关注精确的传播可变尺寸的memory到当前帧，同时平衡 当前/memory 特征在聚合过的特征图中的信号强度。write操作，在目标指导的hard attention下，选择性的存储新特征、删除冗余特征。</p><h2 id="本文贡献："><a href="#本文贡献：" class="headerlink" title="本文贡献："></a>本文贡献：</h2><ul><li>提出使用目标指导的hard attention来提升存储效率、通过外部memory传播时间信息来解决视频目标检测中长程依赖问题。</li><li>设计了一个目标指导的外部memory网络，和新颖的write和read操作，来高效的存储、精确的传播多级特征。</li><li>实验表明，本文的方法是SOTA的，并且速度和精度有良好的平衡。同时，可视化了外部memory，展示了推理细节和长程依赖的重要性。</li></ul><h2 id="外部Memory"><a href="#外部Memory" class="headerlink" title="外部Memory"></a>外部Memory</h2><p>外部Memory被用于增强神经网络的序列推理能力。外部Memory通常是一个可寻址的外部数据矩阵。由一个独立的网络维持，通常包含read和write过程。信息可以通过基于注意力的全局搜索和聚合从外部memory中检索出来。新的信息被写入到特定位置的外部存储器中，因此存储器信息可以显式地长期保留。</p><h2 id="用于Memory寻址的注意力"><a href="#用于Memory寻址的注意力" class="headerlink" title="用于Memory寻址的注意力"></a>用于Memory寻址的注意力</h2><p>注意力被用作外部存储器上的一个“软”寻址机制，为了从memory中检索信息，在所有memory位置和查询该记忆的输入特征之间的相似性上计算一个注意力权重，然后根据这些注意力权重对所有记忆进行聚合。</p><h2 id="使用目标指导的外部Memory检测"><a href="#使用目标指导的外部Memory检测" class="headerlink" title="使用目标指导的外部Memory检测"></a>使用目标指导的外部Memory检测</h2><p>我们提出的框架基于单图像检测器构建，通过合并一个新奇的目标指导的外部memory网络Nmem来精确的传播/存储时间信息into/from单图像检测器，在每帧的检测上。Nmem由两个外部memory矩阵组成：Mpix Minst，分别存储pixel级别和instance级别的特征，并且有read和write操作来读写他们。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1glancqj9v0j31kb0u0h37.jpg" srcset="/img/loading.gif" alt="image-20201203142314174" style="zoom: 25%;" /></p><p>使用R-FCN作为backbone检测器，给出一个视频，我们的框架在线按序检测每一帧。假设当前检测帧为t，首先通过特征网络Nfeat产生卷积特征图F。通过read操作，Mpix被传播至F，他们被聚合为一个增强过的特征图F~，F~被用于产生位置敏感的特征图S和region proposals。之后，S经过位置敏感的roi池化，产生实例特征R。再次通过read操作，Minst被传播至R，他们被聚合为一个增强过的实例特征R~。每个R~被池化，来产生一个检测结果。第三步，使用NMS来移除重复。在第t帧检测过之后，Nmem可以在检测到的目标的信息的指导下，从F~和R~选择有价值的特征，通过write操作将其存储到外部memory中。与此同时，外部memory中的冗余特征被删除。将来自F~的特征定义为pixel级别的特征，将来自R~的特征定义为实例级别的特征，这两个级别的特征分别被存储在外部memory矩阵Mpix和Minst中。</p><h2 id="目标指导的外部Memory网络（OGEMN）"><a href="#目标指导的外部Memory网络（OGEMN）" class="headerlink" title="目标指导的外部Memory网络（OGEMN）"></a>目标指导的外部Memory网络（OGEMN）</h2><p>OGEMN（Nmem）由包含两个外部memory矩阵Mpix和Minst，以及read、write操作。注意，外部memory仅存储同一个视频的特征。</p><h3 id="外部memory矩阵"><a href="#外部memory矩阵" class="headerlink" title="外部memory矩阵"></a>外部memory矩阵</h3><p>用Mpix的每一行存储像素级别的特征，其每一行为来自卷积特征图F~的某些位置的特征向量。Mins的每一行是一个经过位置敏感roi池化的实例特征。外部数据的大小随着新特征被写入而增大，随着特征被删除而缩小。除非视频结束或写入操作明确删除了某个特征，否则外存储器中的特征矩阵将被永久存储。</p><h2 id="使用外部Memory高效检测"><a href="#使用外部Memory高效检测" class="headerlink" title="使用外部Memory高效检测"></a>使用外部Memory高效检测</h2><p>我们提出的memory网络允许我们传播memory到不同尺寸的特征图。为了时间效率，作者设置了一个关键帧体系，下采样所有非关键帧，来减少其在Nfeat上的计算时间。作者只在全尺寸关键帧上更新外部Memory，并且将高质量的特征传播到下采样过的非关键帧，来补偿由下采样引发的退化。基于注意力的read操作，与空间位置、图像尺寸无关，因此可以无缝的聚合来自不同尺寸的输入。</p><h1 id="3-R-FCN"><a href="#3-R-FCN" class="headerlink" title="3.R-FCN"></a>3.R-FCN</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>Fast/Faster R-CNN:apply a costly per-region subnetwork hundreds of times.</p><p>R-FCN:全卷积、计算在整个图像上共享</p><p>提出position-sensitive sore maps，解决图像分类的平移不变、目标检测的平移变化。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="用于目标检测的深度子网络可被分为两类："><a href="#用于目标检测的深度子网络可被分为两类：" class="headerlink" title="用于目标检测的深度子网络可被分为两类："></a>用于目标检测的深度子网络可被分为两类：</h3><ul><li>独立于ROI，共享的全卷积子网络。</li><li>RoI-wise 子网络，不共享计算。</li></ul><p>图像分类网络AlexNet和VGG，设计了两个子网络：一个卷积子网络，最后是空间池化层。紧跟一个全连接层。图像分类网络最后的空间池化层可以自然地被连接到目标检测网络的RoI池化层。</p><p>如ResNets、GoogleNets这样的图像分类网络是全卷积的。很自然的使用所有卷积层来构建共享的卷积子网络，利用RoI-wise子网络，没有隐藏层。然而，这种解决方式被证明检测精度较差，不能与分类精度匹配。为了解决这一问题，ResNet和RoI池化被不自然的插入到两个卷积层之间，这构建了一个更深的RoI-wise子网络，提升了精度，但由于没有共享每个ROI的计算，速度更慢。</p><p>本文提出了R-FCN，一个共享的，全卷积结构的FCN。为了将平移变化合并到FCN，我们使用一组专门的卷积层作为FCN的输出，构建了一组位置敏感分数图。每个分数图编码了关于相对空间位置的位置信息（例如”目标的左侧“）。在这个FCN的顶部，增加了一个位置敏感的ROI池化层，从分数图中引领信息，跟着一个没有加权的卷积或者全连接层。整个结构可被端到端的学习。所有可学习的层是卷积的，且在整张图像上共享，编码了目标检测所需的空间信息。</p><p>使用ResNet101作为backbone,在PASCAL2007上的精度达到83.6%,比Faster R-CNN快2.5-20倍。实验表明我们的方法解决了平移变化、不变之间的窘境，并且如ResNet这种全卷积图像级别的分类器可被高效的转换成全卷积目标检测器。</p><h2 id="Our-approach"><a href="#Our-approach" class="headerlink" title="Our approach"></a>Our approach</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gle5t2izglj31be0omdnn.jpg" srcset="/img/loading.gif" alt="image-20201206151822759"></p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gle5tukddzj31bk0mk79l.jpg" srcset="/img/loading.gif" alt="image-20201206151910703"></p><p>给出ROI，R-FCN结构旨在将ROI分类为目标类和背景类。在R-FCN中，所有可学习的权重层是卷积的，并且可在整个图像上计算。最后的卷积层为每一类产生<strong>k<sup>2</sup>张</strong>的位置敏感分数图<strong>（如k=3时，产生9张，分别对应9个位置）</strong>，输出层为：k<sup>2</sup>张通道数为(C+1)，（C个目标类别和1个背景类）。k<sup>2</sup>张分数图分别对应kxk网格的每个bin，描述相对位置。例如，当k=3时，9个分数图分别编码了一个目标类别的{左上角，上部中间……}</p><p>R-FCN以一个位置敏感的RoI池化层结束，这一层聚合最后一个卷积层的输出，为每一个ROI生成分数。我们的位置敏感的RoI池化是选择性的，kxk中的每一个bin只聚合一个分数图。通过端到端的训练，位置敏感的RoI层引领最后一个卷积层专门学习位置敏感的分数图。</p><h3 id="Backbone"><a href="#Backbone" class="headerlink" title="Backbone"></a>Backbone</h3><p>Backbone：ResNet101，其有100个卷积层，紧跟一个全局平均池化层、一个1000类的全连接层。我们移除平均池化层和全连接层，仅使用卷积层来计算特征图。由于ResNet101的最后一块的2048维的，因此采用一个随机初始化的1024维的1x1卷积来降低纬度。之后，使用kxk的C+1通道的卷积层来产生分数图。</p><h3 id="位置敏感的分数图和位置敏感的RoI池化"><a href="#位置敏感的分数图和位置敏感的RoI池化" class="headerlink" title="位置敏感的分数图和位置敏感的RoI池化"></a>位置敏感的分数图和位置敏感的RoI池化</h3><p>为了给每个RoI清晰地编码位置信息，采用一个均匀的网格将每个RoI矩形分成kxk的bins。</p><p><strong>每个bin只在一张分数图上的对应位置执行池化。</strong></p><p>也就是说：物体的各个部位的位置和特定的一张ROI分数图的子区域的位置是一一对应的。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gle7czfwozj311c042t9c.jpg" srcset="/img/loading.gif" alt="image-20201206161207385"></p><p>其中rc(i,j)是第(i,j)个bin对第c类的池化响应，z是一个分数图，x0,y0是RoI左上角点的坐标，ø代表多有可学习的网络参数。n为这个bin所包含的像素数。</p><p>第i,j个bin的跨越的范围是：其中w,h是RoI矩形的长和宽。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gle7f3nfiej30qy01wt8v.jpg" srcset="/img/loading.gif" alt="image-20201206161411777"></p><p>之后，k<sup>2</sup>个位置敏感的分数在RoI上进行投票，本文简化投票为分数的均值，每个RoI得到一个C+1维的向量。之后计算跨类的softmax响应，用于在训练期间评估交叉熵损失，在推理期间对RoI排序。</p><p>作者通过相似的方法解决边界框回归问题。使用4k<sup>2</sup>维的卷积层用于边界框回归，在4k<sup>2</sup>的图上执行位置敏感的ROI池化，为每一个RoI产生一个4K<sup>2</sup>的向量，然后被平均投票聚合为一个4维向量，表示（tx,ty,tw,th)。作者在这里执行了类别不可知的边界框回归，若执行指定类别的副本，则需要4k<sup>2</sup>C维的输出。</p><h3 id="可视化："><a href="#可视化：" class="headerlink" title="可视化："></a>可视化：</h3><p>图三和图四可视化了R-FCN学习到的位置敏感分数图。这些特定的分数图被期望强烈激活，在目标的一个特定相对位置。例如”顶部中心敏感“的分数图在目标的顶部中心粗略的表现出高分数。如果一个候选框签好与真实目标重合，则kxk个bin的大多数都被强烈激活，投票分数很高。相反，如果一个候选框没有正确的与一个真实目标重合，一些bin将不被激活，导致偷票分数降低。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gle8dugb6lj30ui0u0kjl.jpg" srcset="/img/loading.gif" alt="image-20201206164733647"></p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><h3 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h3><p>解决了分类网络位置不敏感性和检测网络的位置敏感性之间的矛盾。在提升检测速度的同时，利用位置敏感的分数提升了检测精度。</p><h3 id="与Faster-R-CNN的不同："><a href="#与Faster-R-CNN的不同：" class="headerlink" title="与Faster R-CNN的不同："></a>与Faster R-CNN的不同：</h3><p>Faster R-CNN可分为两部分：</p><ol><li>Fully Convolutional subnetwork before RoI Layer</li><li>RoI-wise subnetwork</li></ol><p>第一部分：普通分类网络的卷积层，用来提取共享特征。之后使用一个RoI池化层在第一部分的最后一张特征图上提取每个RoI的特征向量，之后将所有RoI的特征向量都交由第二部分进行分类和回归（对每个RoI分别有一个子网络）</p><p>第一部分是如VGG/GoogleNet/ResNet之类的基础分类网络，其是所有RoI共享的，具有”位置不敏感性“，测试时对一张图片只需进行一次前向计算即可。而第二部分的RoI-wise subnetwork，不是所有RoI共享的。</p><p>如果我们将一个分类网络比如ResNet的所有卷积层都放置在第1部分用来提取特征，而第2部分则只剩下全连接层，这样的目标检测网络是“位置不敏感的translation-invariance”，所以其检测精度会较低，并且也白白浪费了分类网络强大的分类能力（does not match the network’s superior classification accuracy）。而<strong>ResNet</strong>论文中为了解决这样的位置不敏感的缺点，做出了一点让步，即将<strong>RoI Pooling Layer</strong>不再放置在ResNet-101网络的最后一层卷积层之后而是放置在了“<strong>卷积层之间</strong>”，这样RoI Pooling Layer之前和之后都有卷积层，并且RoI Pooling Layer之后的卷积层不是共享计算的，它们是针对每个RoI进行特征提取的，所以这种网络设计，其RoI Pooling Layer之后就具有了“位置敏感性translation-variance”，但是这样做<strong>牺牲了测试速度</strong>，因为所有RoIs都要经过若干层卷积计算，测试速度会很慢</p><ul><li><strong>为什么position-sensitive score map能够具有“在含有某个类别的物体的某个部位的区域上有高响应值”</strong></li></ul><p>这种有高响应值现在只是作者自己在yy的啊，如果网络不满足这一点的话，那么前的所有分析都gg了啊。好，现在我们就大致解释一下为什么训练该网络能够让网络最终满足这一点。首先根据网络的loss公式，如果一个RoI含有人这个物体，那么该RoI通过“position-sensitive score map”+“Position-sensitive RoI pooling”得到的 <img src="https://www.zhihu.com/equation?tex=C%2B1" srcset="/img/loading.gif" alt="[公式]"> 个值中的属于人的那个值必然会在softmax损失函数的要求下“变得尽量的大”，那么如何才能使得属于人的这个值尽量的大呢？那么我们现在就要想到，属于人的这个预测值是怎么来的？在前面的分析，我们已经知道它是通过Position-sensitive RoI pooling这种池化操作的来的，那么也就是说使得 <img src="https://www.zhihu.com/equation?tex=C%2B1" srcset="/img/loading.gif" alt="[公式]"> 个值中属于人的那个值尽量大，必然会使得position-sensitive score map中属于人的那个score map上的“该RoI对应的位置区域的平均值”尽量大，从而也就是该score map上在该区域上的响应值尽量大，因为只有该区域的响应值大了，才能使得预测为人的概率大，才会降低softmax的loss。</p><p>所以这种end-to-end的训练会达到这一目的，并且从论文中的图也可以看出来。</p><p>为了减少计算量，作者将所有RoIs的loss值都计算出来后，对其进行排序，并只对“最大的128个损失值对应的RoIs”执行反向传播操作，其它的则忽略。并且训练策略也是采用的Faster R-CNN中的4-step alternating training进行训练。</p><p>FasterRCNN的RoIPooling是不区分物体部位，相当于k=1 直接获取整个物体的特征，任何含有该物体的区域都高亮，RFCN的位置敏感得分图相当于将一个物体分解，不同特征图负责不同部分的位置敏感，相当于是显示地建立位置敏感特征，并直接对特征值进行监督</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MANet</tag>
      
      <tag>R-FCN</tag>
      
      <tag>OGEMN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（12.4)</title>
    <link href="/2020/12/04/12-4/"/>
    <url>/2020/12/04/12-4/</url>
    
    <content type="html"><![CDATA[<p>最近一周阅读了2篇视频目标检测方向的论文，1篇特征提取的论文。</p><h4 id="阅读文献："><a href="#阅读文献：" class="headerlink" title="阅读文献："></a>阅读文献：</h4><p>[1] Lin T Y, Dollár P, Girshick R, et al. Feature pyramid networks for object detection[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2117-2125.</p><p>[2] Jiang Z, Liu Y, Yang C, et al. Learning where to focus for efficient video object detection[C]//European Conference on Computer Vision. Springer, Cham, 2020: 18-34.</p><p>[3] Shvets M, Liu W, Berg A C. Leveraging long-range temporal relationships between proposals for video object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 9756-9764.</p><h1 id="1-FPN"><a href="#1-FPN" class="headerlink" title="1.FPN"></a>1.FPN</h1><p>主要解决了目标检测而在处理多尺度变化问题时的不足。提出特征金字塔网络结构，能在极少增加计算量的情况下，处理目标检测的多尺度变化问题。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp4tybi8j30ka0koafl.jpg" srcset="/img/loading.gif" alt=""></p><p>识别尺寸差异大的目标现有方法：</p><h4 id="a-通过图像金字塔来构建不同尺度的特征金字塔"><a href="#a-通过图像金字塔来构建不同尺度的特征金字塔" class="headerlink" title="a)通过图像金字塔来构建不同尺度的特征金字塔"></a>a)通过图像金字塔来构建不同尺度的特征金字塔</h4><ul><li>优点：对不同尺度的图像进行特征提取，能产生多尺度特征表示，所有等级特征图都具有较强语义信息，甚至包括一些高分辨率的特征图。</li><li>缺点：<ul><li>推理时间长，内存占用大</li></ul></li></ul><h4 id="b-利用单个高层特征图"><a href="#b-利用单个高层特征图" class="headerlink" title="b)利用单个高层特征图"></a>b)利用单个高层特征图</h4><p>Faster-RCNN中的RPN层就是利用单个高层特征图进行目标分类和bbox回归。</p><h4 id="c-金字塔形特征层级"><a href="#c-金字塔形特征层级" class="headerlink" title="c)金字塔形特征层级"></a>c)金字塔形特征层级</h4><p>SSD为了避免使用低层特征图，放弃使用已经计算的层，从网络的高层开始构建金字塔（在VGG网络的conv4之后，再添加几个新的卷积层）。因此SSD错过了重用高分辨率特征图的机会，没有充分利用到低层特征图中的空间信息（这些信息对于检测小目标非常重要）。</p><h4 id="d-特征金字塔"><a href="#d-特征金字塔" class="headerlink" title="d)特征金字塔"></a>d)特征金字塔</h4><p>FPN是一种具有侧向连接的自上而下的网络结构，用来构建不同尺寸的具有高级语义信息的才特征图。</p><p>能够在较少增加计算量的前提下融合低分辨率但语义信息较强的特征图(高层) 和 高分辨率但语义信息较弱的空间信息丰富的特征图。</p><h2 id="FPN网络"><a href="#FPN网络" class="headerlink" title="FPN网络"></a>FPN网络</h2><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp544w0fj30ke0g2n00.jpg" srcset="/img/loading.gif" alt=""></p><h4 id="（1）Bottom-up-pathway"><a href="#（1）Bottom-up-pathway" class="headerlink" title="（1）Bottom-up pathway"></a>（1）Bottom-up pathway</h4><p>前馈Backbone的一部分，每一级往上用step=2降采样。输出尺寸相同的叫做一级（stage)，选择每一级的最后一层特征图经过1x1卷积后作为逐元素相加的参考。</p><h4 id="（2）Top-down-pathway-and-lateral-connections"><a href="#（2）Top-down-pathway-and-lateral-connections" class="headerlink" title="（2）Top-down pathway and lateral connections"></a>（2）Top-down pathway and lateral connections</h4><p>自顶向下的过程通过“上采样”(up-sampling)将顶层的小特征图放大到与上一级的特征图一样的大小。</p><p>上采样的方法是最近邻插值法：</p><p><img src="https://pic4.zhimg.com/v2-3a1a33dab4980683b5ebe73d43c7396b_r.jpg" srcset="/img/loading.gif" alt="preview" style="zoom: 25%;" /></p><p>对于使用最近邻插值法的个人思考：使用最近邻值插值法，可以在上采样的过程中最大程度地保留特征图的语义信息(有利于分类)，从而与bottom-up 过程中相应的具有丰富的空间信息(高分辨率，有利于定位)的特征图进行融合，从而得到既有良好的空间信息又有较强烈的语义信息的特征图。</p><p><img src="https://pic3.zhimg.com/v2-fe85fb352b9c212fb6d5416330fad9d2_r.jpg" srcset="/img/loading.gif" alt="preview" style="zoom: 67%;" /></p><p>具体过程为：C5层先经过1 x 1卷积，改变特征图的通道数(文章中设置d=256，与Faster R-CNN中RPN层的维数相同便于分类与回归)。M5通过上采样，再加上(特征图中每一个相同位置元素直接相加)C4经过1 x 1卷积后的特征图，得到M4。这个过程再做两次，分别得到M3，M2。M层特征图再经过3 x 3卷积(减轻最近邻近插值带来的混叠影响，周围的数都相同)，得到最终的P2，P3，P4，P5层特征。</p><p>另外，和传统的图像金字塔方式一样，所有M层的通道数都设计成一样的，本文都用d=256。</p><h2 id="FPN应用于RPN层"><a href="#FPN应用于RPN层" class="headerlink" title="FPN应用于RPN层"></a>FPN应用于RPN层</h2><p>Faster RCNN中的RPN是通过最后一层的特征来做的。最后一层的特征经过3x3卷积，得到256个channel的卷积层，再分别经过两个1x1卷积得到类别得分和边框回归结果。这里将特征层之后的RPN子网络称之为网络头部（network head）。对于特征层上的每一个点，作者用anchor的方式预设了9个框。这些框本身包含不同的尺度和不同的长款比例。</p><p>FPN针对RPN的改进是将网络头部应用到每一个P层。由于每个P层相对于原始图片具有不同的尺度信息，因此作者将原始RPN中的尺度信息分离，让每个P层只处理单一的尺度信息。具体的，对{32^2、64^2、128^2、256^2、512^2}这五种尺度的anchor，分别对应到{P2、P3、P4、P5、P6}这五个特征层上。每个特征层都处理1:1、1:2、2:1三种长宽比例的候选框。P6是专门为了RPN网络而设计的，用来处理512大小的候选框。它由P5经过下采样得到。</p><p>另外，上述5个网络头部的参数是共享的。作者通过实验发现，网络头部参数共享和不共享两种设置得到的结果几乎没有差别。这说明不同层级之间的特征有相似的语义层次。这和特征金字塔网络的原理一致。</p><h1 id="2-LSTS"><a href="#2-LSTS" class="headerlink" title="2.LSTS"></a>2.LSTS</h1><h2 id="主要贡献："><a href="#主要贡献：" class="headerlink" title="主要贡献："></a>主要贡献：</h2><p>1.提出LSTS来传播跨帧的高级特征，其可以精确的计算时空对应。LSTS(Learnable Spatio-Temporal Sampling)可以精确的的学习相邻帧特征之间的语义级别的对应。<strong>采样位置首先被随机初始化，然后在检测监督的指导下迭代更新，以找到更好地空间对应。</strong> LSTS将采样位置的偏移看作参数，在bbox和类别监督的指导下，通过反向传播来学习最优的偏移量。</p><p>2.提出了SRFU(Sparsely Recursive Feature Updating)模块，对时间关系建模，加速密集的低级特征。</p><p>3.提出了DFA(Dense Feature Aggregation)模块，增强逐帧特征，增强非关键帧特征表示。</p><h2 id="之前利用时间信息的方法可被分为两类："><a href="#之前利用时间信息的方法可被分为两类：" class="headerlink" title="之前利用时间信息的方法可被分为两类："></a>之前利用时间信息的方法可被分为两类：</h2><p>1.利用时间信息进行后处理，保证目标检测结果的一致性。这些方法通常使用图像检测器来获得检测结果，然后通过box-level的匹配（如跟踪器或现成的光流），对结果进行联合。基于后处理的方法速度较慢。</p><p>2.利用特征表示的时间信息，主要通过聚合相邻帧的特征来提升检测精度，或者传播特征来避免密集的特征提取，以提升速度。</p><h2 id="当在帧间传播特征时，基于光流的扭曲方法有一些缺点："><a href="#当在帧间传播特征时，基于光流的扭曲方法有一些缺点：" class="headerlink" title="当在帧间传播特征时，基于光流的扭曲方法有一些缺点："></a>当在帧间传播特征时，基于光流的扭曲方法有一些缺点：</h2><p>1.光流大量增加了模型的参数量，使得对嵌入式设备的应用不友好。</p><p>2.光流不能准确地表示高级特征之间的对应关系。由于网络的感受野增加，高级语义特征的微小漂移，总是对应图像级别的大运动。</p><p>3.光流提取耗费时间。</p><p>为了解决上述问题，LSTS模块被提出来传播帧间的高级特征。该模块可以在整个数据集上学习其自身的跨帧空间对应。此外，没有额外的光流，也显著加速了传播过程。</p><p>给出两帧It和It+k，及提取出的特征Ft和Ft+k，LSTS模块将首先采样特定的位置，然后计算采样到的位置在特征图Ft和Ft+k上的相似性。最终，采样位置将在最后的检测监督的指导下被迭代更新，这使得我们可以更精确的传播和对齐跨帧的高级特征。</p><p>对比基于光流扭曲的特征传播方法，所提出的方法更轻量级，可以在特征级别精确的建模跨帧对应。</p><h2 id="方法："><a href="#方法：" class="headerlink" title="方法："></a>方法：</h2><p>为了将低计算复杂度，对关键帧使用大的特征提取器H，对非关键帧使用轻量级的特征提取器L。为了利用帧间的关系，一个memory feature F<sup>memory</sup>在关键帧上维持，其通过SRFU模块逐渐更新。此外，由于在非关键帧使用了轻量级的特征提取网络，非关键帧的低级特征不能包含好的检测结果。因此使用DFA模块来提升非关键帧的低级特征，通过在关键帧使用F<sup>memory</sup>。</p><h4 id=""><a href="#" class="headerlink" title=""></a><center><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkz5xtu5vgj318k0qa4ei.jpg" srcset="/img/loading.gif" alt="image-20201123155929982" style="zoom:35%;" /></center></h4><blockquote><p>为了简化，t0、t1(关键帧)将被喂给高级特征提取器H，t1+k(非关键帧)将被喂给低级特征提取器L。通过SRFU来捕捉时间信息，以维持F<sup>memory</sup>，F<sup>memory</sup>在关键帧迭代更新。与此同时，DFA传播关键帧的F<sup>memory</sup>来增强和丰富非关键帧的低级信息。LSTS被嵌入到SRFU和DFA中，来景区的传播和对齐跨帧特征。最终，SFRU和DFA的输出被未给任务网络来进行最终的预测。</p></blockquote><h2 id="LSTS"><a href="#LSTS" class="headerlink" title="LSTS"></a>LSTS</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkz6amegumj317y0s2e09.jpg" srcset="/img/loading.gif" alt="image-20201123161150186" style="zoom: 33%;" /></p><blockquote><p>LSTS由4步组成：1）在特征图Ft上随机采样一些位置。2）通过相似性对比计算亲和力权重。相关相似性权重通过嵌入的特征f(Ft)和g(Ft+k)计算，f和g是嵌入的函数，旨在减少特征Ft和Ft+k的通道数，节约计算花费。3）特征Ft和计算出的权重一起被合并为一个被传播的特征F‘t+k。4）在训练期间位置通过反向传播迭代更新。训练后，最终学习到的采样位置将被用于推理。</p><p>黄色方格表示第t+k帧特定的网格位置，蓝色方格表示第t帧采样的相关位置。</p></blockquote><p>在给出特征图Ft，Ft+k之后，LSTS允许我们计算对应的相似性权重。</p><h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><p>为了精确的将特征从Ft传播到Ft+k，我们需要两帧之间精确的空间对应。首先在邻域内初始化一些采样位置，提供了粗相关。通过学习LSTS可以转换和缩放采样位置的分布，逐渐建立一个精确的空间对应。</p><h3 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h3><p>有了对应的位置，就可以计算其相似性。为了减少计算开销，Ft和Ft+k被嵌入化为f(Ft)和g(Ft+k)。然后，基于嵌入化的特征f(Ft)和g(Ft+k)计算对应的相似性权重。</p><h3 id="聚合"><a href="#聚合" class="headerlink" title="聚合"></a>聚合</h3><p>计算出第t帧的pn位置与第t+k帧p0位置相似性权重后，即可估计第t+k帧p0位置的值。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkz7kj03f0j30iu04sdg3.jpg" srcset="/img/loading.gif" alt="image-20201123165557637" style="zoom:33%;" /></p><h3 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h3><p>为了学习对应的ground truth分布，在训练期间采样位置pn通过反向传播更新（根据最终检测loss）。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkz7rzs63zj30r804kmxp.jpg" srcset="/img/loading.gif" alt="image-20201123170308684" style="zoom: 33%;" /></p><p>在训练之后，最终学习到的采样位置可被用于在推理过程中传播和对齐帧间特征。LSTA是SRFU和DFA的核心。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkzbeg15cij31as0gkwii.jpg" srcset="/img/loading.gif" alt="image-20201123190830258"></p><blockquote><p>图4:SRFU和DFA的结构，+是聚合单元。Transform由一些卷积组成，用于提升非关键帧的低级特征。对于SRFU，使用LSTS模块传播最后一个<strong>关键帧</strong>特征F<sup>memory</sup><sub>t0</sub>到当前<strong>关键帧</strong>F<sup>memory</sup><sub>t1</sub> 。而对于DFA，使用LSTS模块将<strong>关键帧</strong>特征F<sup>memory</sup><sub>t1</sub>传播到<strong>非关键帧</strong>t1+k，来提升特征质量，获得更好地检测结果。</p></blockquote><h2 id="SRFU"><a href="#SRFU" class="headerlink" title="SRFU"></a>SRFU</h2><p>SRFU模块允许利用内在的时间线索内的视频传播和聚集稀疏关键帧在整个视频的高层特征。具体来说，SRFU模块在整个视频上维护和递归更新一个F<sup>memory</sup>。在图4的过程中，直接使用新的关键帧特征Ft1更新memory feature是次优的，因为t0和t1之间存在运动偏差。因此，使用LSTS模块，估计运动，生成一个对齐的特征Ft1 align。之后，使用一个聚合单元来生成更新过的特征Ft1 memory和关键帧的task特征(用于检测和定位）。</p><h2 id="DFA"><a href="#DFA" class="headerlink" title="DFA"></a>DFA</h2><p>考虑到计算复杂性，作者在非关键帧使用轻量级的特征提取网络，将会提取到低级特征。因此DFA模块可以重用密集的关键帧的高级特征来提升非关键帧的质量。具体来说，如图4b所示，非关键帧特征F<sup>low</sup><sub>t1+k</sub>先被喂给一个Transform单元（计算量很小），预测一个语义级别的特征F<sup>high</sup><sub>t1+k</sub>。由于第t1帧和t1+k帧之间存在运动偏差，将LSTS模块用于Ft1 memory来生成一个对齐的特征Ft1+k align。之后使用一个聚合单元来生成非关键帧上的task特征Ft1+k task。相比于低级特征F<sup>low</sup><sub>t1+k</sub>,F<sup>task</sup><sub>t1+k</sub>包含更多语义级别的信息，允许我们获得更好地检测结果。</p><h1 id="3-LLTR"><a href="#3-LLTR" class="headerlink" title="3.LLTR"></a>3.LLTR</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>本文对单帧检测器进行了轻量化修改，可以解决视频中任意长的依赖性。显著提升了单帧检测器的精度，计算开销可以忽略不计。本文方法的新颖之处是：在目标proposal上操作的时间关系模块，其学习不同帧之间proposal的相似性，从之前或（和）之后帧选择proposal来支持当前帧的proposal。</p><p>本文的方法基于聚合视频相关部分的特征，以便就推定的检测是否正确做出更好的决策。这涉及确定视频的哪些部分与潜在检测相关，这些部分的特征的加权平均以及对那些更新的新特征的最终检测决策。考虑FPN中的region proposal作为潜在的检测和视频潜在的关系部分。也可以使用快速、高召回率的单步检测器的输出。无论如何，考虑长期时间关系的一个重大挑战是存在许多可能性。本文方法的一个关键方面是：将哪些proposal应该支持检测的问题分解，合并结果。</p><p>作者展示了如何使用关系模块来建模帧间对目标proposal的依赖，并且引入直接监督来学习表观关系。此外，证明了我们的方法从聚合长程时间关系中收益，当尝试考虑相隔1秒以上的帧时，用于竞争的模块效果会下降。这些优点来自于基于表观特征学习到了哪些proposal可能与潜在的检测相关，忽略了位置和时间上的差异。</p><h2 id="主要贡献：-1"><a href="#主要贡献：-1" class="headerlink" title="主要贡献："></a>主要贡献：</h2><ul><li>提出了一个新奇的proposal-level的时间关系块，来学习表观相似性，使用支持帧的特征来丰富目标帧的特征。</li><li>提出了一个使用时间关系模块来从视频中多个支持帧合并长程依赖的方法。</li><li>online推理、速度与单帧baseline检测器相当</li><li>在学习过程中增加图监督</li></ul><p>pixel-level的特征，如D&amp;T、FGFA、STSN，随着时间快速退化。本文采用细粒度的instance-level的表观特征，不随时间退化，并且故意忽略的实例的空间位置，来释放更大的时间跨度的使用和跨多个时间跨度的聚集。我们的模块在每帧数百个proposal上运行，可以使用历史信息聚合特征，可以高效的端到端的反向传播信息至RPN和backbone网络。</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>作者的目标是学习更新instance-level的特征，以在线方式更新检测器的预测，无需任何后处理步骤。为了实现这个目标，需要学习目标proposal间基于表观的关系。</p><h3 id="基于表观的关系模块"><a href="#基于表观的关系模块" class="headerlink" title="基于表观的关系模块"></a>基于表观的关系模块</h3><p>假设在D维空间中，目标帧有N个proposal，支持帧有M个proposal。构建一个注意力机制来用支持帧特征的加权平均来更新目标帧的特征。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp5hw4jkj30ko0fw0u4.jpg" srcset="/img/loading.gif" style="zoom:50%;" /></p><p>目标特征和支持特征<strong>首先使用一个linear层进行embedded</strong>。之后这两个矩阵经过<strong>特征归一化</strong>，这与以前的工作不同，在先前的工作中，外观关系被构建为嵌入特征的直接关联。我们注意到，如果不这样做，相关性主要由特征数量决定而不是由实际的关系决定。这在ROI-level的特征检测中非常重要，因为这些特征被认为是有偏差的并且具有较高的量级。<strong>第三步是构建表观关系矩阵G（</strong>图中的x)，通过在所有目标特征对之间执行简单的相关。矩阵G被进一步用于在支持proposal上计算注意力权重，如果在视频中xi和xj与同一个实例相关，那么Gij的值就高，否则不相关的话，Gi,j的值就低。为了加强约束，给图G增加了一个监督损失。鼓励相同实例间的距离小，不同实例间的距离大。</p><p>在描述性外观关系矩阵G被构造之后，它被用作支持特征的注意机制中的权重。使用softmax对G进行归一化。可以想象，如果目标实例I具有来自j1、j2、.。。，jk的强support关系，然后Gij1，Gij2，.。。，Gijk的值高，只有这些值些会在softmax操作后“存活”。另一方面，如果i在支持帧中没有相应的proposal，那么第i行中的所有值都将是低的，并且softmax将不支持任何support实例，因此目标实例将接收不携带相关信号的简单平均support。</p><p>G^被用于聚合embedded支持特征，使用矩阵相乘层。在一个额外的linear层后，形成最终的聚合过的support特征矩阵，与输入的目标特征矩阵的大小相同，（最后一个线性层的输出维度仍然是D）。因此，使用逐元素相加来更新目标特征。</p><p>到目前为止，我们只讨论了如何根据support proposals更新目标特征。</p><h4 id="Causal-and-Symmetric-modes"><a href="#Causal-and-Symmetric-modes" class="headerlink" title="Causal and Symmetric modes"></a>Causal and Symmetric modes</h4><p>为了给定的目标帧选择支持帧，我们引入了时间内核（temporal kernel）的概念。时间核是一个元组(K，s)，表示大小和步距。大小K是关系推理中包含的帧数。因此，对于一个选定的目标帧，有k-1个支持帧。步幅s限制以s帧的统一时间间隔对K帧进行采样。</p><p>我们为给定的目标帧定义了两种核模式：Causal和Symmetric。在Causal模式下，支持帧仅从之前的帧中选取：t-s,t-2s,t-(K-1)s.在Symmetric模式下，支持帧从之前和之后选取：t-s,t+s。</p><h3 id="Single-model-and-Frozen-Support-Model-settings"><a href="#Single-model-and-Frozen-Support-Model-settings" class="headerlink" title="Single model and Frozen Support Model settings"></a>Single model and Frozen Support Model settings</h3><p>由于效率高，在单个模型中计算目标和支持特征是一个有吸引力的选择。在我们的单一模型设置中，目标和支持特征都是从同一层pool的ROI特征，并且网络是经过端到端训练的。这存在两个潜在的问题，首先，当核尺寸被设为K，训练时至少有K帧在一个设备上处理。若给定一个较大分辨率的图像用于检测，在标准的GPU存储器中只可能防止几张图像，因此，内核大小是有限的。第二，支持特征分布在检测器的训练期间是变化的，因为它是根据与目标特征相同的参数集计算的。</p><p>为了解决这些问题，我们引入了冻结支持模型设置，其中使用单帧预训练检测器(冻结支持模型)来提取实例级支持特征，同时主检测器一次运行单个图像，还接收固定支持特征作为附加输入。请注意，在训练期间，冻结支持模型不需要分配大的内存，因为不需要反向传播，这使得在训练期间可以使用更大的内核大小。在我们的实验部分，我们证明了事实上从同一个模型中学习这两个特性是有益的，并且我们在不同层次上的聚合降低了对大内核的需求。</p><h3 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h3><p>正如第2节所讨论的，实例之间的几何关系被认为会随着时间而退化，即使使用了复杂的运动传播方法[1，3，26]。在我们的模型中，box之间的几何关系是有意不使用的，以便能够在推理过程中捕获长期依赖关系。我们证明了应用具有大时间步长s的核来捕捉长期关系的可能性。为此，我们训练一个关系模块，使其不可知于时间内核步长，这使得在推理过程中能够跨不同的时间步长进行聚合。</p><p>因此，对于训练和测试来说，核大小K作为模型的参数是固定的，但是时间步长是动态随机选择的。训练时，s选一次。请注意，由于上面提到的内存限制，模型是用单个s训练的，但是由于随机采样，关系模块在推理过程中是不可知的。这实现了多步聚合。在推理过程中，对几个不同的步长多次应用内核，并执行特征聚合。我们表明，延长时间跨步长(使用高达s = 256)不仅不会降低性能，而且显示了准确性的持续提高。</p><p><img src="https://tva1.sinaimg.cn/large/008eGmZEly1goqp5ss666j316c0kqe4x.jpg" srcset="/img/loading.gif" alt=""></p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FPN</tag>
      
      <tag>LSTS</tag>
      
      <tag>LLTR</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java学习笔记2：面向对象编程</title>
    <link href="/2020/12/03/Java-2/"/>
    <url>/2020/12/03/Java-2/</url>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://www.liaoxuefeng.com/wiki/1252599548343744" target="_blank" rel="noopener">廖雪峰的Java教程</a></p><h1 id="一、面向对象基础"><a href="#一、面向对象基础" class="headerlink" title="一、面向对象基础"></a>一、面向对象基础</h1><p>面向对象编程，是一种通过对象的方式，把现实世界映射到计算机模型的一种编程方法。</p><h3 id="定义class"><a href="#定义class" class="headerlink" title="定义class"></a>定义class</h3><p>在Java中，创建一个类，例如，给这个类命名为<code>Person</code>，就是定义一个<code>class</code>：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">public</span> String name;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> age;&#125;</code></pre><p>一个<code>class</code>可以包含多个字段（<code>field</code>），字段用来描述一个类的特征。<code>public</code>是用来修饰字段的，它表示这个字段可以被外部访问。</p><h3 id="创建实例"><a href="#创建实例" class="headerlink" title="创建实例"></a>创建实例</h3><p>定义了class，只是定义了对象模版，而要根据对象模版创建出真正的对象实例，必须用new操作符。</p><p>new操作符可以创建一个实例，然后，我们需要定义一个引用类型的变量来指向这个实例：</p><pre><code class="hljs java">Person ming = <span class="hljs-keyword">new</span> Person();</code></pre><p>上述代码创建了一个Person类型的实例，并通过变量<code>ming</code>指向它。</p><p>注意区分<code>Person ming</code>是定义<code>Person</code>类型的变量<code>ming</code>，而<code>new Person()</code>是创建<code>Person</code>实例。</p><p>访问实例变量可以用<code>变量.字段</code></p><pre><code class="hljs java">ming.name = <span class="hljs-string">"Xiao Ming"</span>; <span class="hljs-comment">// 对字段name赋值</span>ming.age = <span class="hljs-number">12</span>; <span class="hljs-comment">// 对字段age赋值</span>System.out.println(ming.name); <span class="hljs-comment">// 访问字段name</span>Person hong = <span class="hljs-keyword">new</span> Person();hong.name = <span class="hljs-string">"Xiao Hong"</span>;hong.age = <span class="hljs-number">15</span>;</code></pre><h2 id="1-方法："><a href="#1-方法：" class="headerlink" title="1.方法："></a>1.方法：</h2><p>为了避免外部代码直接去访问<code>field</code>，我们可以用<code>private</code>修饰<code>field</code>，拒绝外部访问：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;&#125;</code></pre><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">class_test</span> </span>&#123;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;Person hou = <span class="hljs-keyword">new</span> Person();hou.setAge(<span class="hljs-number">22</span>);hou.setName(<span class="hljs-string">"侯天翔"</span>);System.out.println(hou.getName() + <span class="hljs-string">","</span> + hou.getAge());&#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;<span class="hljs-keyword">private</span> String name;<span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;<span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.name;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setName</span><span class="hljs-params">(String name)</span> </span>&#123;<span class="hljs-keyword">this</span>.name = name;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123;<span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.age;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> age)</span> </span>&#123;<span class="hljs-keyword">if</span> (age &lt; <span class="hljs-number">0</span> || age &gt; <span class="hljs-number">120</span>) &#123;<span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> IllegalArgumentException(<span class="hljs-string">"Invalid age value"</span>);&#125; <span class="hljs-keyword">else</span> &#123;<span class="hljs-keyword">this</span>.age = age;&#125;&#125;&#125;</code></pre><h3 id="可变参数"><a href="#可变参数" class="headerlink" title="可变参数"></a>可变参数</h3><p>可变参数用<code>类型...</code>定义，可变参数相当于数组类型：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Group</span> </span>&#123;    <span class="hljs-keyword">private</span> String[] names;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setNames</span><span class="hljs-params">(String... names)</span> </span>&#123;        <span class="hljs-keyword">this</span>.names = names;    &#125;&#125;</code></pre><h3 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h3><p>基本类型参数的传递，是调用方值的复制。双方各自的后续修改，互不影响。</p><p>引用类型参数的传递，调用方的变量，和接收方的参数变量，指向的是同一个对象。双方任意一方对这个对象的修改，都会影响对方（因为指向同一个对象嘛）。</p><h2 id="2-构造方法"><a href="#2-构造方法" class="headerlink" title="2.构造方法"></a>2.构造方法</h2><p>构造方法的名称就是类名。构造方法的参数没有限制，在方法内部，也可以编写任意语句。但是，和普通方法相比，构造方法没有返回值（也没有<code>void</code>），调用构造方法，必须用<code>new</code>操作符。</p><p>如果一个类没有定义构造方法，编译器会自动为我们生成一个默认构造方法，它没有参数，也没有执行语句.要特别注意的是，如果我们自定义了一个构造方法，那么，编译器就<em>不再</em>自动创建默认构造方法</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Person p = <span class="hljs-keyword">new</span> Person(<span class="hljs-string">"Xiao Ming"</span>, <span class="hljs-number">15</span>);        System.out.println(p.getName());        System.out.println(p.getAge());    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Person</span><span class="hljs-params">(String name, <span class="hljs-keyword">int</span> age)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;        <span class="hljs-keyword">this</span>.age = age;    &#125;        <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.name;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.age;    &#125;&#125;</code></pre><h2 id="3-方法重载"><a href="#3-方法重载" class="headerlink" title="3.方法重载"></a>3.方法重载</h2><p>在一个类中，我们可以定义多个方法。如果有一系列方法，它们的功能都是类似的，只有参数有所不同，那么，可以把这一组方法名做成<em>同名</em>方法。例如，在<code>Hello</code>类中，定义多个<code>hello()</code>方法：这种方法名相同，但各自的参数不同，称为方法重载（<code>Overload</code>）。</p><p>注意：方法重载的返回值类型通常都是相同的。</p><p>方法重载的目的是，功能类似的方法使用同一名字，更容易记住，因此，调用起来更简单。</p><h2 id="4-继承"><a href="#4-继承" class="headerlink" title="4.继承"></a>4.继承</h2><p>Java使用<code>extends</code>关键字来实现继承：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;...&#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setName</span><span class="hljs-params">(String name)</span> </span>&#123;...&#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123;...&#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> age)</span> </span>&#123;...&#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-comment">// 不要重复name和age字段/方法,</span>    <span class="hljs-comment">// 只需要定义新增score字段/方法:</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> score;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getScore</span><span class="hljs-params">()</span> </span>&#123; … &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setScore</span><span class="hljs-params">(<span class="hljs-keyword">int</span> score)</span> </span>&#123; … &#125;&#125;</code></pre><p>注意：子类自动获得了父类的所有字段，严禁定义与父类重名的字段！</p><p>在OOP的术语中，我们把<code>Person</code>称为超类（super class），父类（parent class），基类（base class），把<code>Student</code>称为子类（subclass），扩展类（extended class）。</p><h3 id="继承树"><a href="#继承树" class="headerlink" title="继承树"></a>继承树</h3><p>注意到我们在定义<code>Person</code>的时候，没有写<code>extends</code>。在Java中，没有明确写<code>extends</code>的类，编译器会自动加上<code>extends Object</code>。所以，任何类，除了<code>Object</code>，都会继承自某个类。下图是<code>Person</code>、<code>Student</code>的继承树：</p><pre><code class="hljs ascii">┌───────────┐│  Object   │└───────────┘      ▲      │┌───────────┐│  Person   │└───────────┘      ▲      │┌───────────┐│  Student  │└───────────┘</code></pre><p>Java只允许一个class继承自一个类，因此，一个类有且仅有一个父类。只有<code>Object</code>特殊，它没有父类。</p><h3 id="protected"><a href="#protected" class="headerlink" title="protected"></a>protected</h3><p>继承有个特点，就是子类无法访问父类的<code>private</code>字段或者<code>private</code>方法。例如，<code>Student</code>类就无法访问<code>Person</code>类的<code>name</code>和<code>age</code>字段：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"Hello, "</span> + name; <span class="hljs-comment">// 编译错误：无法访问name字段</span>    &#125;&#125;</code></pre><p>这使得继承的作用被削弱了。为了让子类可以访问父类的字段，我们需要把<code>private</code>改为<code>protected</code>。<strong>用<code>protected</code>修饰的字段可以被子类访问：</strong></p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> String name;    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">int</span> age;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"Hello, "</span> + name; <span class="hljs-comment">// OK!</span>    &#125;&#125;</code></pre><h3 id="super"><a href="#super" class="headerlink" title="super"></a>super</h3><p><code>super</code>关键字表示父类（超类）。子类引用父类的字段时，可以用<code>super.fieldName</code>。例如：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    public <span class="hljs-type">String</span> hello() &#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"Hello, "</span> + <span class="hljs-keyword">super</span>.name;    &#125;&#125;</code></pre><p>实际上，这里使用<code>super.name</code>，或者<code>this.name</code>，或者<code>name</code>，效果都是一样的。编译器会自动定位到父类的<code>name</code>字段。</p><p>但是，在某些时候，就必须使用<code>super</code>。我们来看一个例子：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Student s = <span class="hljs-keyword">new</span> Student(<span class="hljs-string">"Xiao Ming"</span>, <span class="hljs-number">12</span>, <span class="hljs-number">89</span>);    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> String name;    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Person</span><span class="hljs-params">(String name, <span class="hljs-keyword">int</span> age)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;        <span class="hljs-keyword">this</span>.age = age;    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">int</span> score;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Student</span><span class="hljs-params">(String name, <span class="hljs-keyword">int</span> age, <span class="hljs-keyword">int</span> score)</span> </span>&#123;        <span class="hljs-keyword">this</span>.score = score;    &#125;&#125;</code></pre><p>运行上面的代码，会得到一个编译错误，大意是在<code>Student</code>的构造方法中，无法调用<code>Person</code>的构造方法。</p><p><strong>这是因为在Java中，任何<code>class</code>的构造方法，第一行语句必须是调用父类的构造方法。</strong>如果没有明确地调用父类的构造方法，编译器会帮我们自动加一句<code>super();</code>，所以，<code>Student</code>类的构造方法实际上是这样：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> int score;    public <span class="hljs-type">Student</span>(<span class="hljs-type">String</span> name, int age, int score) &#123;        <span class="hljs-keyword">super</span>(); <span class="hljs-comment">// 自动调用父类的构造方法</span>        <span class="hljs-keyword">this</span>.score = score;    &#125;&#125;</code></pre><p>但是，<code>Person</code>类并没有无参数的构造方法，因此，编译失败。</p><p>解决方法是调用<code>Person</code>类存在的某个构造方法。例如：</p><pre><code class="hljs scala"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> int score;    public <span class="hljs-type">Student</span>(<span class="hljs-type">String</span> name, int age, int score) &#123;        <span class="hljs-keyword">super</span>(name, age); <span class="hljs-comment">// 调用父类的构造方法Person(String, int)</span>        <span class="hljs-keyword">this</span>.score = score;    &#125;&#125;</code></pre><p>这样就可以正常编译了！</p><p>因此我们得出结论：如果父类没有默认的构造方法，子类就必须显式调用<code>super()</code>并给出参数以便让编译器定位到父类的一个合适的构造方法。</p><p>这里还顺带引出了另一个问题：即子类<em>不会继承</em>任何父类的构造方法。子类默认的构造方法是编译器自动生成的，不是继承的。</p><h3 id="向上转型："><a href="#向上转型：" class="headerlink" title="向上转型："></a>向上转型：</h3><p>这种把一个子类类型安全地变为父类类型的赋值，被称为向上转型（upcasting）。</p><p>向上转型实际上是把一个子类型安全地变为更加抽象的父类型：</p><pre><code class="hljs java">Student s = <span class="hljs-keyword">new</span> Student();Person p = s; <span class="hljs-comment">// upcasting, ok</span>Object o1 = p; <span class="hljs-comment">// upcasting, ok</span>Object o2 = s; <span class="hljs-comment">// upcasting, ok</span></code></pre><p>注意到继承树是<code>Student &gt; Person &gt; Object</code>，所以，可以把<code>Student</code>类型转型为<code>Person</code>，或者更高层次的<code>Object</code>。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>继承是面向对象编程的一种强大的代码复用方式；</li><li>Java只允许单继承，所有类最终的根类是<code>Object</code>；</li><li><code>protected</code>允许子类访问父类的字段和方法；</li><li>子类的构造方法可以通过<code>super()</code>调用父类的构造方法；</li><li>可以安全地向上转型为更抽象的类型；</li><li>可以强制向下转型，最好借助<code>instanceof</code>判断；</li><li>子类和父类的关系是is，has关系不能用继承。</li></ul><h2 id="5-多态"><a href="#5-多态" class="headerlink" title="5.多态"></a>5.多态</h2><p>在继承关系中，子类如果定义了一个与父类方法签名完全相同的方法，被称为覆写（Override）。</p><p>例如，在<code>Person</code>类中，我们定义了<code>run()</code>方法：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"Person.run"</span>);    &#125;&#125;</code></pre><p>在子类<code>Student</code>中，覆写这个<code>run()</code>方法：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"Student.run"</span>);    &#125;&#125;</code></pre><p>Override和Overload不同的是，如果方法签名如果不同，就是Overload，Overload方法是一个新方法；如果方法签名相同，并且返回值也相同，就是<code>Override</code>。</p><p>加上<code>@Override</code>可以让编译器帮助检查是否进行了正确的覆写。希望进行覆写，但是不小心写错了方法签名，编译器会报错。</p><p>但是<code>@Override</code>不是必需的。</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;&#125;&#125;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-meta">@Override</span> <span class="hljs-comment">// Compile error!</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">(String s)</span> </span>&#123;&#125;&#125;</code></pre><p>Java的实例方法调用是基于运行时的实际类型的动态调用，而非变量的声明类型。</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Person p = <span class="hljs-keyword">new</span> Student();        p.run(); <span class="hljs-comment">// 应该打印Person.run还是Student.run?答案：Student.run</span>    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"Person.run"</span>);    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"Student.run"</span>);    &#125;&#125;</code></pre><p>这个非常重要的特性在面向对象编程中称之为多态。它的英文拼写非常复杂：Polymorphic。</p><h3 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h3><p>多态是指，针对某个类型的方法调用，其真正执行的方法取决于运行时期实际类型的方法。</p><p>多态具有一个非常强大的功能，就是允许添加更多类型的子类实现功能扩展，却不需要修改基于父类的代码。</p><h3 id="覆写Object方法"><a href="#覆写Object方法" class="headerlink" title="覆写Object方法"></a>覆写Object方法</h3><p>因为所有的<code>class</code>最终都继承自<code>Object</code>，而<code>Object</code>定义了几个重要的方法：</p><ul><li><code>toString()</code>：把instance输出为<code>String</code>；</li><li><code>equals()</code>：判断两个instance是否逻辑相等；</li><li><code>hashCode()</code>：计算一个instance的哈希值。</li></ul><h3 id="调用super"><a href="#调用super" class="headerlink" title="调用super"></a>调用super</h3><p>在子类的覆写方法中，如果要调用父类的被覆写的方法，可以通过<code>super</code>来调用。例如：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> String name;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"Hello, "</span> + name;    &#125;&#125;Student extends Person &#123;    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">// 调用父类的hello()方法:</span>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">super</span>.hello() + <span class="hljs-string">"!"</span>;    &#125;&#125;</code></pre><h3 id="final"><a href="#final" class="headerlink" title="final"></a>final</h3><p>继承可以允许子类覆写父类的方法。如果一个父类不允许子类对它的某个方法进行覆写，可以把该方法标记为<code>final</code>。用<code>final</code>修饰的方法不能被<code>Override</code>：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> String name;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> String <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-string">"Hello, "</span> + name;    &#125;&#125;Student extends Person &#123;    <span class="hljs-comment">// compile error: 不允许覆写</span>    <span class="hljs-meta">@Override</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>如果一个类不希望任何其他类继承自它，那么可以把这个类本身标记为<code>final</code>。用<code>final</code>修饰的类不能被继承：</p><pre><code class="hljs java"><span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">protected</span> String name;&#125;<span class="hljs-comment">// compile error: 不允许继承自Person</span>Student extends Person &#123;&#125;</code></pre><p>对于一个类的实例字段，同样可以用<code>final</code>修饰。用<code>final</code>修饰的字段在初始化后不能被修改。例如：</p><pre><code class="hljs java"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> String name = <span class="hljs-string">"Unamed"</span>;&#125;</code></pre><p>对<code>final</code>字段重新赋值会报错：</p><pre><code class="hljs haxe">Person p = <span class="hljs-keyword">new</span> <span class="hljs-type">Person</span>();p.name = <span class="hljs-string">"New Name"</span>; <span class="hljs-comment">// compile error!</span></code></pre><p>可以在构造方法中初始化final字段：</p><pre><code class="hljs arduino"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> &#123;</span>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">String</span> name;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Person</span><span class="hljs-params">(<span class="hljs-keyword">String</span> name)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;    &#125;&#125;</code></pre><p>这种方法更为常用，因为可以保证实例一旦创建，其<code>final</code>字段就不可修改。</p><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><ul><li>子类可以覆写父类的方法（Override），覆写在子类中改变了父类方法的行为；</li><li>Java的方法调用总是作用于运行期对象的实际类型，这种行为称为多态；</li><li><code>final</code>修饰符有多种作用：<ul><li><code>final</code>修饰的方法可以阻止被覆写；</li><li><code>final</code>修饰的class可以阻止被继承；</li><li><code>final</code>修饰的field必须在创建对象时初始化，随后不可修改。</li></ul></li></ul><h2 id="6-抽象类"><a href="#6-抽象类" class="headerlink" title="6.抽象类"></a>6.抽象类</h2><p>定义普通方法的时候，必须实现方法的语句。不能省略父类方法的定义。</p><p>如果父类的方法本身不需要实现任何功能，仅仅是为了定义方法签名，目的是让子类去覆写它，那么，可以把父类的方法声明为抽象方法：</p><pre><code class="hljs cs"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> &#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span>(<span class="hljs-params"></span>)</span>;&#125;</code></pre><p>把一个方法声明为<code>abstract</code>，表示它是一个抽象方法，本身没有实现任何方法语句。因为这个抽象方法本身是无法执行的，所以，<code>Person</code>类也无法被实例化。编译器会告诉我们，无法编译<code>Person</code>类，因为它包含抽象方法。</p><p>必须把<code>Person</code>类本身也声明为<code>abstract</code>，才能正确编译它：</p><pre><code class="hljs cs"><span class="hljs-keyword">abstract</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> &#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span>(<span class="hljs-params"></span>)</span>;&#125;</code></pre><h3 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h3><p>如果一个<code>class</code>定义了方法，但没有具体执行代码，这个方法就是抽象方法，抽象方法用<code>abstract</code>修饰。</p><p>因为无法执行抽象方法，因此这个类也必须申明为抽象类（abstract class）。</p><p>使用<code>abstract</code>修饰的类就是抽象类。我们无法实例化一个抽象类：</p><pre><code class="hljs haxe">Person p = <span class="hljs-keyword">new</span> <span class="hljs-type">Person</span>(); <span class="hljs-comment">// 编译错误</span></code></pre><p>无法实例化的抽象类有什么用？</p><p>因为抽象类本身被设计成只能用于被继承，因此，抽象类可以强迫子类实现其定义的抽象方法，否则编译会报错。因此，抽象方法实际上相当于定义了“规范”。</p><h3 id="面向抽象编程"><a href="#面向抽象编程" class="headerlink" title="面向抽象编程"></a>面向抽象编程</h3><p>当我们定义了抽象类<code>Person</code>，以及具体的<code>Student</code>、<code>Teacher</code>子类的时候，我们可以通过抽象类<code>Person</code>类型去引用具体的子类的实例：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">Person s</span> = new Student();<span class="hljs-attribute">Person t</span> = new Teacher();</code></pre><p>这种引用抽象类的好处在于，我们对其进行方法调用，并不关心<code>Person</code>类型变量的具体子类型：</p><pre><code class="hljs dockerfile">// 不关心Person变量的具体子类型:s.<span class="hljs-keyword">run</span><span class="bash">();</span>t.<span class="hljs-keyword">run</span><span class="bash">();</span></code></pre><p>同样的代码，如果引用的是一个新的子类，我们仍然不关心具体类型：</p><pre><code class="hljs dockerfile">// 同样不关心新的子类是如何实现<span class="hljs-keyword">run</span><span class="bash">()方法的：</span>Person e = new Employee();e.<span class="hljs-keyword">run</span><span class="bash">();</span></code></pre><p>这种尽量引用高层类型，避免引用实际子类型的方式，称之为<strong>面向抽象编程</strong>。</p><p>面向抽象编程的本质就是：</p><ul><li>上层代码只定义规范（例如：<code>abstract class Person</code>）；</li><li>不需要子类就可以实现业务逻辑（正常编译）；</li><li>具体的业务逻辑由不同的子类实现，调用者并不关心。</li></ul><h2 id="7-接口"><a href="#7-接口" class="headerlink" title="7.接口"></a>7.接口</h2><p>在抽象类中，抽象方法本质上是定义接口规范：即规定高层类的接口，从而保证所有子类都有相同的接口实现，这样，多态就能发挥出威力。</p><p>如果一个抽象类没有字段，所有方法全部都是抽象方法：</p><pre><code class="hljs cs"><span class="hljs-keyword">abstract</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> &#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span>(<span class="hljs-params"></span>)</span>;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">abstract</span> String <span class="hljs-title">getName</span>(<span class="hljs-params"></span>)</span>;&#125;</code></pre><p>就可以把该抽象类改写为接口：<strong><code>interface</code>。</strong></p><p>在Java中，使用<code>interface</code>可以声明一个接口：</p><pre><code class="hljs cs"><span class="hljs-keyword">interface</span> <span class="hljs-title">Person</span> &#123;    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">run</span>(<span class="hljs-params"></span>)</span>;    <span class="hljs-function">String <span class="hljs-title">getName</span>(<span class="hljs-params"></span>)</span>;&#125;</code></pre><p>所谓<code>interface</code>，就是比抽象类还要抽象的纯抽象接口，因为它连字段都不能有。因为接口定义的所有方法<strong>默认都是<code>public abstract</code>的</strong>，所以这两个修饰符不需要写出来（写不写效果都一样）。</p><p>当一个具体的<code>class</code>去实现一个<code>interface</code>时，需要使用<code>implements</code>关键字。举个例子：</p><pre><code class="hljs arduino"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-title">implements</span> <span class="hljs-title">Person</span> &#123;</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">String</span> name;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Student</span><span class="hljs-params">(<span class="hljs-keyword">String</span> name)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;    &#125;    @Override    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        System.out.<span class="hljs-built_in">println</span>(<span class="hljs-keyword">this</span>.name + <span class="hljs-string">" run"</span>);    &#125;    @Override    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">String</span> <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.name;    &#125;&#125;</code></pre><h3 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h3><p>注意区分术语：</p><p>Java的接口特指<code>interface</code>的定义，表示一个接口类型和一组方法签名，而编程接口泛指接口规范，如方法签名，数据格式，网络协议等。</p><p>抽象类和接口的对比如下：</p><div class="table-container"><table><thead><tr><th style="text-align:left"></th><th style="text-align:left">abstract class</th><th style="text-align:left">interface</th></tr></thead><tbody><tr><td style="text-align:left">继承</td><td style="text-align:left">只能extends一个class</td><td style="text-align:left">可以implements多个interface</td></tr><tr><td style="text-align:left">字段</td><td style="text-align:left">可以定义实例字段</td><td style="text-align:left">不能定义实例字段</td></tr><tr><td style="text-align:left">抽象方法</td><td style="text-align:left">可以定义抽象方法</td><td style="text-align:left">可以定义抽象方法</td></tr><tr><td style="text-align:left">非抽象方法</td><td style="text-align:left">可以定义非抽象方法</td><td style="text-align:left">可以定义default方法</td></tr></tbody></table></div><h3 id="接口继承"><a href="#接口继承" class="headerlink" title="接口继承"></a>接口继承</h3><p>一个<code>interface</code>可以继承自另一个<code>interface</code>。<code>interface</code>继承自<code>interface</code>使用<code>extends</code>，它相当于扩展了接口的方法。例如：</p><pre><code class="hljs routeros">interface Hello &#123;    void hello();&#125;interface Person extends Hello &#123;    void <span class="hljs-builtin-name">run</span>();    String getName();&#125;</code></pre><p>此时，<code>Person</code>接口继承自<code>Hello</code>接口，因此，<code>Person</code>接口现在实际上有3个抽象方法签名，其中一个来自继承的<code>Hello</code>接口。</p><h3 id="default方法"><a href="#default方法" class="headerlink" title="default方法"></a>default方法</h3><p>在接口中，可以定义<code>default</code>方法。例如，把<code>Person</code>接口的<code>run()</code>方法改为<code>default</code>方法：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Person p = <span class="hljs-keyword">new</span> Student(<span class="hljs-string">"Xiao Ming"</span>);        p.run();    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function">String <span class="hljs-title">getName</span><span class="hljs-params">()</span></span>;    <span class="hljs-function"><span class="hljs-keyword">default</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(getName() + <span class="hljs-string">" run"</span>);    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Student</span> <span class="hljs-keyword">implements</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Student</span><span class="hljs-params">(String name)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.name;    &#125;&#125;</code></pre><p>实现类可以不必覆写<code>default</code>方法。<code>default</code>方法的目的是，当我们需要给接口新增一个方法时，会涉及到修改全部子类。如果新增的是<code>default</code>方法，那么子类就不必全部修改，只需要在需要覆写的地方去覆写新增方法。</p><p><code>default</code>方法和抽象类的普通方法是有所不同的。因为<code>interface</code>没有字段，<code>default</code>方法无法访问字段，而抽象类的普通方法可以访问实例字段。</p><h2 id="8-静态字段和静态方法"><a href="#8-静态字段和静态方法" class="headerlink" title="8.静态字段和静态方法"></a>8.静态字段和静态方法</h2><p>在一个<code>class</code>中定义的字段，我们称之为实例字段。实例字段的特点是，每个实例都有独立的字段，各个实例的同名字段互不影响。</p><p>还有一种字段，是用<code>static</code>修饰的字段，称为静态字段：<code>static field</code>。</p><p>实例字段在每个实例中都有自己的一个独立“空间”，但是<strong>静态字段只有一个共享“空间”，所有实例都会共享该字段。</strong>举个例子：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Person ming = <span class="hljs-keyword">new</span> Person(<span class="hljs-string">"Xiao Ming"</span>, <span class="hljs-number">12</span>);        Person hong = <span class="hljs-keyword">new</span> Person(<span class="hljs-string">"Xiao Hong"</span>, <span class="hljs-number">15</span>);        ming.number = <span class="hljs-number">88</span>;        System.out.println(hong.number);<span class="hljs-comment">//88</span>        hong.number = <span class="hljs-number">99</span>;        System.out.println(ming.number);<span class="hljs-comment">//99</span>    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">public</span> String name;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> number;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Person</span><span class="hljs-params">(String name, <span class="hljs-keyword">int</span> age)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;        <span class="hljs-keyword">this</span>.age = age;    &#125;&#125;</code></pre><p>对于静态字段，无论修改哪个实例的静态字段，效果都是一样的：所有实例的静态字段都被修改了，原因是静态字段并不属于实例：</p><pre><code class="hljs ascii">        ┌──────────────────┐ming ──&gt;│Person instance   │        ├──────────────────┤        │name &#x3D; &quot;Xiao Ming&quot;│        │age &#x3D; 12          │        │number ───────────┼──┐    ┌─────────────┐        └──────────────────┘  │    │Person class │                              │    ├─────────────┤                              ├───&gt;│number &#x3D; 99  │        ┌──────────────────┐  │    └─────────────┘hong ──&gt;│Person instance   │  │        ├──────────────────┤  │        │name &#x3D; &quot;Xiao Hong&quot;│  │        │age &#x3D; 15          │  │        │number ───────────┼──┘        └──────────────────┘</code></pre><p>虽然实例可以访问静态字段，但是它们指向的其实都是<code>Person class</code>的静态字段。所以，<strong>所有实例共享一个静态字段。</strong></p><p>因此，不推荐用<code>实例变量.静态字段</code>去访问静态字段，因为在Java程序中，实例对象并没有静态字段。在代码中，实例对象能访问静态字段只是因为编译器可以根据实例类型自动转换为<code>类名.静态字段</code>来访问静态对象。</p><p>推荐用类名来访问静态字段。可以把静态字段理解为描述<code>class</code>本身的字段（非实例字段）。对于上面的代码，更好的写法是：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>number = <span class="hljs-number">99</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>number);</code></pre><h3 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h3><p>有静态字段，就有静态方法。用<code>static</code>修饰的方法称为静态方法。</p><p>调用实例方法必须通过一个实例变量，而调用静态方法则不需要实例变量，通过类名就可以调用。静态方法类似其它编程语言的函数。例如：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Person.setNumber(<span class="hljs-number">99</span>);        System.out.println(Person.number);    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">int</span> number;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setNumber</span><span class="hljs-params">(<span class="hljs-keyword">int</span> value)</span> </span>&#123;        number = value;    &#125;&#125;</code></pre><p>因为静态方法属于<code>class</code>而不属于实例，因此，静态方法内部，无法访问<code>this</code>变量，也无法访问实例字段，它只能访问静态字段。<strong>【静态方法只能访问静态字段】</strong></p><p>通过实例变量也可以调用静态方法，但这只是编译器自动帮我们把实例改写成类名而已。</p><p>通常情况下，通过实例变量访问静态字段和静态方法，会得到一个编译警告。</p><p>静态方法经常用于工具类。例如：</p><ul><li>Arrays.sort()</li><li>Math.random()</li></ul><p>静态方法也经常用于辅助方法。注意到Java程序的入口<code>main()</code>也是静态方法。</p><h3 id="接口的静态字段"><a href="#接口的静态字段" class="headerlink" title="接口的静态字段"></a>接口的静态字段</h3><p>因为<code>interface</code>是一个纯抽象类，所以它不能定义实例字段。但是，<code>interface</code>是可以有静态字段的，并且静态字段必须为<code>final</code>类型：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> MALE = <span class="hljs-number">1</span>;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> FEMALE = <span class="hljs-number">2</span>;&#125;</code></pre><p>实际上，因为<code>interface</code>的字段只能是<code>public static final</code>类型，所以我们可以把这些修饰符都去掉，上述代码可以简写为：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">interface</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-comment">// 编译器会自动加上public statc final:</span>    <span class="hljs-keyword">int</span> MALE = <span class="hljs-number">1</span>;    <span class="hljs-keyword">int</span> FEMALE = <span class="hljs-number">2</span>;&#125;</code></pre><p>编译器会自动把该字段变为<code>public static final</code>类型。</p><h2 id="9-包"><a href="#9-包" class="headerlink" title="9.包"></a>9.包</h2><p>在前面的代码中，我们把类和接口命名为<code>Person</code>、<code>Student</code>、<code>Hello</code>等简单名字。</p><p>在现实中，如果小明写了一个<code>Person</code>类，小红也写了一个<code>Person</code>类，现在，小白既想用小明的<code>Person</code>，也想用小红的<code>Person</code>，怎么办？</p><p>如果小军写了一个<code>Arrays</code>类，恰好JDK也自带了一个<code>Arrays</code>类，如何解决类名冲突？</p><p>在Java中，我们使用<code>package</code>来解决名字冲突。</p><p>Java定义了一种名字空间，称之为包：<code>package</code>。一个类总是属于某个包，类名（比如<code>Person</code>）只是一个简写，真正的完整类名是<code>包名.类名</code>。</p><p>例如：</p><p>小明的<code>Person</code>类存放在包<code>ming</code>下面，因此，完整类名是<code>ming.Person</code>；</p><p>小红的<code>Person</code>类存放在包<code>hong</code>下面，因此，完整类名是<code>hong.Person</code>；</p><p>小军的<code>Arrays</code>类存放在包<code>mr.jun</code>下面，因此，完整类名是<code>mr.jun.Arrays</code>；</p><p>JDK的<code>Arrays</code>类存放在包<code>java.util</code>下面，因此，完整类名是<code>java.util.Arrays</code>。</p><p>在定义<code>class</code>的时候，我们需要在第一行声明这个<code>class</code>属于哪个包。</p><p>小明的<code>Person.java</code>文件：</p><pre><code class="hljs angelscript">package ming; <span class="hljs-comment">// 申明包名ming</span><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Person</span> &#123;&#125;</code></pre><p>小军的<code>Arrays.java</code>文件：</p><pre><code class="hljs angelscript">package mr.jun; <span class="hljs-comment">// 申明包名mr.jun</span><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Arrays</span> &#123;&#125;</code></pre><p>在Java虚拟机执行的时候，JVM只看完整类名，因此，只要包名不同，类就不同。</p><p>包可以是多层结构，用<code>.</code>隔开。例如：<code>java.util</code>。</p><p> 要特别注意：包没有父子关系。java.util和java.util.zip是不同的包，两者没有任何继承关系。</p><p>没有定义包名的<code>class</code>，它使用的是默认包，非常容易引起名字冲突，因此，不推荐不写包名的做法。</p><p>我们还需要按照包结构把上面的Java文件组织起来。假设以<code>package_sample</code>作为根目录，<code>src</code>作为源码目录，那么所有文件结构就是：</p><pre><code class="hljs ascii">package_sample└─ src    ├─ hong    │  └─ Person.java    │  ming    │  └─ Person.java    └─ mr       └─ jun          └─ Arrays.java</code></pre><p>即所有Java文件对应的目录层次要和包的层次一致。</p><p>编译后的<code>.class</code>文件也需要按照包结构存放。如果使用IDE，把编译后的<code>.class</code>文件放到<code>bin</code>目录下，那么，编译的文件结构就是：</p><pre><code class="hljs ascii">package_sample└─ bin   ├─ hong   │  └─ Person.class   │  ming   │  └─ Person.class   └─ mr      └─ jun         └─ Arrays.class</code></pre><p>编译的命令相对比较复杂，我们需要在<code>src</code>目录下执行<code>javac</code>命令：</p><pre><code class="hljs reasonml">javac -d ../bin ming/<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>java hong/<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>java mr/jun/<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Arrays</span>.</span></span>java</code></pre><p>在IDE中，会自动根据包结构编译所有Java源码，所以不必担心使用命令行编译的复杂命令。</p><h3 id="包作用域"><a href="#包作用域" class="headerlink" title="包作用域"></a>包作用域</h3><p>位于同一个包的类，可以访问包作用域的字段和方法。不用<code>public</code>、<code>protected</code>、<code>private</code>修饰的字段和方法就是包作用域。例如，<code>Person</code>类定义在<code>hello</code>包下面：</p><pre><code class="hljs arduino">package hello;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> &#123;</span>    <span class="hljs-comment">// 包作用域:</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        System.out.<span class="hljs-built_in">println</span>(<span class="hljs-string">"Hello!"</span>);    &#125;&#125;</code></pre><p><code>Main</code>类也定义在<code>hello</code>包下面：</p><pre><code class="hljs arduino">package hello;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        Person p = <span class="hljs-keyword">new</span> Person();        p.hello(); <span class="hljs-comment">// 可以调用，因为Main和Person在同一个包</span>    &#125;&#125;</code></pre><h3 id="import"><a href="#import" class="headerlink" title="import"></a>import</h3><p>在一个<code>class</code>中，我们总会引用其他的<code>class</code>。例如，小明的<code>ming.Person</code>类，如果要引用小军的<code>mr.jun.Arrays</code>类，他有三种写法：</p><p>第一种，直接写出完整类名，例如：</p><pre><code class="hljs java"><span class="hljs-comment">// Person.java</span><span class="hljs-keyword">package</span> ming;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        mr.jun.Arrays arrays = <span class="hljs-keyword">new</span> mr.jun.Arrays();    &#125;&#125;</code></pre><p>很显然，每次写完整类名比较痛苦。</p><p>因此，第二种写法是用<code>import</code>语句，导入小军的<code>Arrays</code>，然后写简单类名：</p><pre><code class="hljs java"><span class="hljs-comment">// Person.java</span><span class="hljs-keyword">package</span> ming;<span class="hljs-comment">// 导入完整类名:</span><span class="hljs-keyword">import</span> mr.jun.Arrays;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        Arrays arrays = <span class="hljs-keyword">new</span> Arrays();    &#125;&#125;</code></pre><p>在写<code>import</code>的时候，可以使用<code>*</code>，表示把这个包下面的所有<code>class</code>都导入进来（但不包括子包的<code>class</code>）：</p><pre><code class="hljs java"><span class="hljs-comment">// Person.java</span><span class="hljs-keyword">package</span> ming;<span class="hljs-comment">// 导入mr.jun包的所有class:</span><span class="hljs-keyword">import</span> mr.jun.*;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">run</span><span class="hljs-params">()</span> </span>&#123;        Arrays arrays = <span class="hljs-keyword">new</span> Arrays();    &#125;&#125;</code></pre><p>我们一般不推荐这种写法，因为在导入了多个包后，很难看出<code>Arrays</code>类属于哪个包。</p><p>还有一种<code>import static</code>的语法，它可以导入可以导入一个类的静态字段和静态方法：</p><pre><code class="hljs arduino">package main;<span class="hljs-comment">// 导入System类的所有静态字段和静态方法:</span><span class="hljs-keyword">import</span> <span class="hljs-keyword">static</span> java.lang.System.*;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        <span class="hljs-comment">// 相当于调用System.out.println(…)</span>        out.<span class="hljs-built_in">println</span>(<span class="hljs-string">"Hello, world!"</span>);    &#125;&#125;</code></pre><p><code>import static</code>很少使用。</p><p>Java编译器最终编译出的<code>.class</code>文件只使用<em>完整类名</em>，因此，在代码中，当编译器遇到一个<code>class</code>名称时：</p><ul><li>如果是完整类名，就直接根据完整类名查找这个<code>class</code>；</li><li>如果是简单类名，按下面的顺序依次查找：<ul><li>查找当前<code>package</code>是否存在这个<code>class</code>；</li><li>查找<code>import</code>的包是否包含这个<code>class</code>；</li><li>查找<code>java.lang</code>包是否包含这个<code>class</code>。</li></ul></li></ul><p>如果按照上面的规则还无法确定类名，则编译报错。</p><p>我们来看一个例子：</p><pre><code class="hljs arduino"><span class="hljs-comment">// Main.java</span>package test;<span class="hljs-keyword">import</span> java.<span class="hljs-built_in">text</span>.Format;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        java.util.List <span class="hljs-built_in">list</span>; <span class="hljs-comment">// ok，使用完整类名 -&gt; java.util.List</span>        Format format = null; <span class="hljs-comment">// ok，使用import的类 -&gt; java.text.Format</span>        <span class="hljs-keyword">String</span> s = <span class="hljs-string">"hi"</span>; <span class="hljs-comment">// ok，使用java.lang包的String -&gt; java.lang.String</span>        System.out.<span class="hljs-built_in">println</span>(s); <span class="hljs-comment">// ok，使用java.lang包的System -&gt; java.lang.System</span>        MessageFormat mf = null; <span class="hljs-comment">// 编译错误：无法找到MessageFormat: MessageFormat cannot be resolved to a type</span>    &#125;&#125;</code></pre><p>因此，编写class的时候，编译器会自动帮我们做两个import动作：</p><ul><li>默认自动<code>import</code>当前<code>package</code>的其他<code>class</code>；</li><li><p>默认自动<code>import java.lang.*</code>。</p><p>注意：自动导入的是java.lang包，但类似java.lang.reflect这些包仍需要手动导入。</p></li></ul><p>如果有两个<code>class</code>名称相同，例如，<code>mr.jun.Arrays</code>和<code>java.util.Arrays</code>，那么只能<code>import</code>其中一个，另一个必须写完整类名。</p><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p>Java内建的<code>package</code>机制是为了避免<code>class</code>命名冲突；</p><p>JDK的核心类使用<code>java.lang</code>包，编译器会自动导入；</p><p>JDK的其它常用类定义在<code>java.util.*</code>，<code>java.math.*</code>，<code>java.text.*</code>，……；</p><p>包名推荐使用倒置的域名，例如<code>org.apache</code>。</p><h2 id="10-作用域"><a href="#10-作用域" class="headerlink" title="10.作用域"></a>10.作用域</h2><p>在Java中，我们经常看到<code>public</code>、<code>protected</code>、<code>private</code>这些修饰符。在Java中，这些修饰符可以用来限定访问作用域。</p><h3 id="public"><a href="#public" class="headerlink" title="public"></a>public</h3><p>定义为<code>public</code>的<code>class</code>、<code>interface</code>可以被其他任何类访问：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>上面的<code>Hello</code>是<code>public</code>，因此，可以被其他包的类访问：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> xyz;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">foo</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">// Main可以访问Hello</span>        Hello h = <span class="hljs-keyword">new</span> Hello();    &#125;&#125;</code></pre><p>定义为<code>public</code>的<code>field</code>、<code>method</code>可以被其他类访问，前提是首先有访问<code>class</code>的权限：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>上面的<code>hi()</code>方法是<code>public</code>，可以被其他类调用，前提是首先要能访问<code>Hello</code>类：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> xyz;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">foo</span><span class="hljs-params">()</span> </span>&#123;        Hello h = <span class="hljs-keyword">new</span> Hello();        h.hi();    &#125;&#125;</code></pre><h3 id="private"><a href="#private" class="headerlink" title="private"></a>private</h3><p>定义为<code>private</code>的<code>field</code>、<code>method</code>无法被其他类访问：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-comment">// 不能被其他类调用:</span>    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">this</span>.hi();    &#125;&#125;</code></pre><p>实际上，确切地说，<code>private</code>访问权限被限定在<code>class</code>的内部，而且与方法声明顺序<em>无关</em>。推荐把<code>private</code>方法放到后面，因为<code>public</code>方法定义了类对外提供的功能，阅读代码的时候，应该先关注<code>public</code>方法：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">this</span>.hi();    &#125;    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>由于Java支持嵌套类，如果一个类内部还定义了嵌套类，那么，嵌套类拥有访问<code>private</code>的权限：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Inner i = <span class="hljs-keyword">new</span> Inner();        i.hi();    &#125;    <span class="hljs-comment">// private方法:</span>    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;        System.out.println(<span class="hljs-string">"private hello!"</span>);    &#125;    <span class="hljs-comment">// 静态内部类:</span>    <span class="hljs-keyword">static</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inner</span> </span>&#123;        <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;            Main.hello();        &#125;    &#125;&#125;</code></pre><p>定义在一个<code>class</code>内部的<code>class</code>称为嵌套类（<code>nested class</code>），Java支持好几种嵌套类。</p><h3 id="protected-1"><a href="#protected-1" class="headerlink" title="protected"></a>protected</h3><p><code>protected</code>作用于继承关系。定义为<code>protected</code>的字段和方法可以被子类访问，以及子类的子类：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-comment">// protected方法:</span>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>上面的<code>protected</code>方法可以被继承的类访问：</p><pre><code class="hljs scala"><span class="hljs-keyword">package</span> xyz;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Hello</span> </span>&#123;    void foo() &#123;        <span class="hljs-comment">// 可以访问protected方法:</span>        hi();    &#125;&#125;</code></pre><h3 id="package"><a href="#package" class="headerlink" title="package"></a>package</h3><p>最后，<strong>包作用域</strong>是指一个类允许访问同一个<code>package</code>的没有<code>public</code>、<code>private</code>修饰的<code>class</code>，以及没有<code>public</code>、<code>protected</code>、<code>private</code>修饰的字段和方法。</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-comment">// package权限的类:</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-comment">// package权限的方法:</span>    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>只要在同一个包，就可以访问<code>package</code>权限的<code>class</code>、<code>field</code>和<code>method</code>：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">foo</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-comment">// 可以访问package权限的类:</span>        Hello h = <span class="hljs-keyword">new</span> Hello();        <span class="hljs-comment">// 可以调用package权限的方法:</span>        h.hi();    &#125;&#125;</code></pre><p>注意，包名必须完全一致，包没有父子关系，<code>com.apache</code>和<code>com.apache.abc</code>是不同的包。</p><h3 id="局部变量"><a href="#局部变量" class="headerlink" title="局部变量"></a>局部变量</h3><p>在方法内部定义的变量称为局部变量，局部变量作用域从变量声明处开始到对应的块结束。方法参数也是局部变量。</p><pre><code class="hljs angelscript">package abc;<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Hello</span> &#123;    <span class="hljs-built_in">void</span> hi(String name) &#123; <span class="hljs-comment">// ①</span>        String s = name.toLowerCase(); <span class="hljs-comment">// ②</span>        <span class="hljs-built_in">int</span> len = s.length(); <span class="hljs-comment">// ③</span>        <span class="hljs-keyword">if</span> (len &lt; <span class="hljs-number">10</span>) &#123; <span class="hljs-comment">// ④</span>            <span class="hljs-built_in">int</span> p = <span class="hljs-number">10</span> - len; <span class="hljs-comment">// ⑤</span>            <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i=<span class="hljs-number">0</span>; i&lt;<span class="hljs-number">10</span>; i++) &#123; <span class="hljs-comment">// ⑥</span>                System.<span class="hljs-keyword">out</span>.println(); <span class="hljs-comment">// ⑦</span>            &#125; <span class="hljs-comment">// ⑧</span>        &#125; <span class="hljs-comment">// ⑨</span>    &#125; <span class="hljs-comment">// ⑩</span>&#125;</code></pre><p>我们观察上面的<code>hi()</code>方法代码：</p><ul><li>方法参数name是局部变量，它的作用域是整个方法，即①～⑩；</li><li>变量s的作用域是定义处到方法结束，即②～⑩；</li><li>变量len的作用域是定义处到方法结束，即③～⑩；</li><li>变量p的作用域是定义处到if块结束，即⑤～⑨；</li><li>变量i的作用域是for循环，即⑥～⑧。</li></ul><p>使用局部变量时，应该尽可能把局部变量的作用域缩小，尽可能延后声明局部变量。</p><h3 id="final-1"><a href="#final-1" class="headerlink" title="final"></a>final</h3><p>Java还提供了一个<code>final</code>修饰符。<code>final</code>与访问权限不冲突，它有很多作用。</p><p>用<code>final</code>修饰<code>class</code>可以阻止被继承：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-comment">// 无法被继承:</span><span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> n = <span class="hljs-number">0</span>;    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">(<span class="hljs-keyword">int</span> t)</span> </span>&#123;        <span class="hljs-keyword">long</span> i = t;    &#125;&#125;</code></pre><p>用<code>final</code>修饰<code>method</code>可以阻止被子类覆写：</p><pre><code class="hljs java"><span class="hljs-keyword">package</span> abc;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;    <span class="hljs-comment">// 无法被覆写:</span>    <span class="hljs-function"><span class="hljs-keyword">protected</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">void</span> <span class="hljs-title">hi</span><span class="hljs-params">()</span> </span>&#123;    &#125;&#125;</code></pre><p>用<code>final</code>修饰<code>field</code>可以阻止被重新赋值：</p><pre><code class="hljs angelscript">package abc;<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Hello</span> &#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> n = <span class="hljs-number">0</span>;    <span class="hljs-keyword">protected</span> <span class="hljs-built_in">void</span> hi() &#123;        <span class="hljs-keyword">this</span>.n = <span class="hljs-number">1</span>; <span class="hljs-comment">// error!</span>    &#125;&#125;</code></pre><p>用<code>final</code>修饰局部变量可以阻止被重新赋值：</p><pre><code class="hljs angelscript">package abc;<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Hello</span> &#123;    <span class="hljs-keyword">protected</span> <span class="hljs-built_in">void</span> hi(<span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> t) &#123;        t = <span class="hljs-number">1</span>; <span class="hljs-comment">// error!</span>    &#125;&#125;</code></pre><h3 id="最佳实践"><a href="#最佳实践" class="headerlink" title="最佳实践"></a>最佳实践</h3><p>如果不确定是否需要<code>public</code>，就不声明为<code>public</code>，即尽可能少地暴露对外的字段和方法。</p><p>把方法定义为<code>package</code>权限有助于测试，因为测试类和被测试类只要位于同一个<code>package</code>，测试代码就可以访问被测试类的<code>package</code>权限方法。</p><p><strong>一个<code>.java</code>文件只能包含一个<code>public</code>类，但可以包含多个非<code>public</code>类。如果有<code>public</code>类，文件名必须和<code>public</code>类的名字相同。</strong></p><h3 id="小结-3"><a href="#小结-3" class="headerlink" title="小结"></a>小结</h3><p>Java内建的访问权限包括<code>public</code>、<code>protected</code>、<code>private</code>和<code>package</code>权限；</p><p>Java在方法内部定义的变量是局部变量，局部变量的作用域从变量声明开始，到一个块结束；</p><p><code>final</code>修饰符不是访问权限，它可以修饰<code>class</code>、<code>field</code>和<code>method</code>；</p><p>一个<code>.java</code>文件只能包含一个<code>public</code>类，但可以包含多个非<code>public</code>类。</p><h2 id="11-内部类"><a href="#11-内部类" class="headerlink" title="11.内部类"></a>11.内部类</h2><h3 id="Inner-Class"><a href="#Inner-Class" class="headerlink" title="Inner Class"></a>Inner Class</h3><p>如果一个类定义在另一个类的内部，这个类就是Inner Class：</p><pre><code class="hljs angelscript"><span class="hljs-keyword">class</span> <span class="hljs-symbol">Outer</span> &#123;    <span class="hljs-keyword">class</span> <span class="hljs-symbol">Inner</span> &#123;        <span class="hljs-comment">// 定义了一个Inner Class</span>    &#125;&#125;</code></pre><p>上述定义的<code>Outer</code>是一个普通类，而<code>Inner</code>是一个Inner Class，它与普通类有个最大的不同，就是Inner Class的实例不能单独存在，必须依附于一个Outer Class的实例。示例代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Outer outer = <span class="hljs-keyword">new</span> Outer(<span class="hljs-string">"Nested"</span>); <span class="hljs-comment">// 实例化一个Outer</span>        Outer.Inner inner = outer.<span class="hljs-keyword">new</span> Inner(); <span class="hljs-comment">// 实例化一个Inner</span>        inner.hello();    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Outer</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    Outer(String name) &#123;        <span class="hljs-keyword">this</span>.name = name;    &#125;    <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Inner</span> </span>&#123;        <span class="hljs-function"><span class="hljs-keyword">void</span> <span class="hljs-title">hello</span><span class="hljs-params">()</span> </span>&#123;            System.out.println(<span class="hljs-string">"Hello, "</span> + Outer.<span class="hljs-keyword">this</span>.name);        &#125;    &#125;&#125;</code></pre><p>观察上述代码，要实例化一个<code>Inner</code>，我们必须首先创建一个<code>Outer</code>的实例，然后，调用<code>Outer</code>实例的<code>new</code>来创建<code>Inner</code>实例：</p><pre><code class="hljs pgsql"><span class="hljs-keyword">Outer</span>.<span class="hljs-keyword">Inner</span> <span class="hljs-keyword">inner</span> = <span class="hljs-keyword">outer</span>.<span class="hljs-built_in">new</span> <span class="hljs-keyword">Inner</span>();</code></pre><p>这是因为Inner Class除了有一个<code>this</code>指向它自己，还隐含地持有一个Outer Class实例，可以用<code>Outer.this</code>访问这个实例。所以，实例化一个Inner Class不能脱离Outer实例。</p><p>Inner Class和普通Class相比，除了能引用Outer实例外，还有一个额外的“特权”，就是可以修改Outer Class的<code>private</code>字段，因为Inner Class的作用域在Outer Class内部，所以能访问Outer Class的<code>private</code>字段和方法。</p><p>观察Java编译器编译后的<code>.class</code>文件可以发现，<code>Outer</code>类被编译为<code>Outer.class</code>，而<code>Inner</code>类被编译为<code>Outer$Inner.class</code>。</p><h2 id="12-classpath-和-jar"><a href="#12-classpath-和-jar" class="headerlink" title="12.classpath 和 jar"></a>12.classpath 和 jar</h2><p><code>classpath</code>是JVM用到的一个环境变量，它用来指示JVM如何搜索<code>class</code>。</p><p>因为Java是编译型语言，源码文件是<code>.java</code>，而<strong>编译后的<code>.class</code>文件</strong>才是真正可以被JVM执行的字节码。因此，JVM需要知道，如果要加载一个<code>abc.xyz.Hello</code>的类，应该去哪搜索对应的<code>Hello.class</code>文件。</p><p>所以，<code>classpath</code>就是一组目录的集合，</p><p>如果JVM在某个路径下找到了对应的<code>class</code>文件，就不再往后继续搜索。如果所有路径下都没有找到，就报错。</p><p><code>classpath</code>的设定方法有两种：</p><p>在系统环境变量中设置<code>classpath</code>环境变量，不推荐；</p><p>在启动JVM时设置<code>classpath</code>变量，推荐。</p><p>我们强烈<em>不推荐</em>在系统环境变量中设置<code>classpath</code>，那样会污染整个系统环境。在启动JVM时设置<code>classpath</code>才是推荐的做法。实际上就是给<code>java</code>命令传入<code>-classpath</code>或<code>-cp</code>参数：</p><pre><code class="hljs taggerscript">java -classpath .;C:<span class="hljs-symbol">\w</span>ork<span class="hljs-symbol">\p</span>roject1<span class="hljs-symbol">\b</span>in;C:<span class="hljs-symbol">\s</span>hared abc.xyz.Hello</code></pre><p>或者使用<code>-cp</code>的简写：</p><pre><code class="hljs taggerscript">java -cp .;C:<span class="hljs-symbol">\w</span>ork<span class="hljs-symbol">\p</span>roject1<span class="hljs-symbol">\b</span>in;C:<span class="hljs-symbol">\s</span>hared abc.xyz.Hello</code></pre><p>没有设置系统环境变量，也没有传入<code>-cp</code>参数，那么JVM默认的<code>classpath</code>为<code>.</code>，即当前目录：</p><pre><code class="hljs css"><span class="hljs-selector-tag">java</span> <span class="hljs-selector-tag">abc</span><span class="hljs-selector-class">.xyz</span><span class="hljs-selector-class">.Hello</span></code></pre><p>上述命令告诉JVM只在当前目录搜索<code>Hello.class</code>。</p><h3 id="jar包"><a href="#jar包" class="headerlink" title="jar包"></a>jar包</h3><p>如果有很多<code>.class</code>文件，散落在各层目录中，肯定不便于管理。如果能把目录打一个包，变成一个文件，就方便多了。</p><p>jar包就是用来干这个事的，它可以把<code>package</code>组织的目录层级，以及各个目录下的所有文件（包括<code>.class</code>文件和其他文件）都打成一个jar文件，这样一来，无论是备份，还是发给客户，就简单多了。</p><p>jar包实际上就是一个zip格式的压缩文件，而jar包相当于目录。如果我们要执行一个jar包的<code>class</code>，就可以把jar包放到<code>classpath</code>中：</p><pre><code class="hljs stylus">java -cp ./hello<span class="hljs-selector-class">.jar</span> abc<span class="hljs-selector-class">.xyz</span>.Hello</code></pre><p>这样JVM会自动在<code>hello.jar</code>文件里去搜索某个类。</p><p>那么问题来了：如何创建jar包？</p><p>因为jar包就是zip包，所以，直接在资源管理器中，找到正确的目录，点击右键，在弹出的快捷菜单中选择“发送到”，“压缩(zipped)文件夹”，就制作了一个zip文件。然后，把后缀从<code>.zip</code>改为<code>.jar</code>，一个jar包就创建成功。</p><p>假设编译输出的目录结构是这样：</p><pre><code class="hljs ascii">package_sample└─ bin   ├─ hong   │  └─ Person.class   │  ming   │  └─ Person.class   └─ mr      └─ jun         └─ Arrays.class</code></pre><p>这里需要特别注意的是，jar包里的第一层目录，不能是<code>bin</code>，而应该是<code>hong</code>、<code>ming</code>、<code>mr</code>。如果在Windows的资源管理器中看，应该长这样：</p><p><img src="https://www.liaoxuefeng.com/files/attachments/1261393208671488/l" srcset="/img/loading.gif" alt="hello.zip.ok"></p><p>jar包还可以包含一个特殊的<code>/META-INF/MANIFEST.MF</code>文件，<code>MANIFEST.MF</code>是纯文本，可以指定<code>Main-Class</code>和其它信息。JVM会自动读取这个<code>MANIFEST.MF</code>文件，如果存在<code>Main-Class</code>，我们就不必在命令行指定启动的类名，而是用更方便的命令：</p><pre><code class="hljs mipsasm"><span class="hljs-keyword">java </span>-<span class="hljs-keyword">jar </span>hello.<span class="hljs-keyword">jar</span></code></pre><p>jar包还可以包含其它jar包，这个时候，就需要在<code>MANIFEST.MF</code>文件里配置<code>classpath</code>了。</p><p>在大型项目中，不可能手动编写<code>MANIFEST.MF</code>文件，再手动创建zip包。Java社区提供了大量的开源构建工具，例如<a href="https://www.liaoxuefeng.com/wiki/1252599548343744/1255945359327200" target="_blank" rel="noopener">Maven</a>，可以非常方便地创建jar包。</p><h3 id="小结-4"><a href="#小结-4" class="headerlink" title="小结"></a>小结</h3><p>JVM通过环境变量<code>classpath</code>决定搜索<code>class</code>的路径和顺序；</p><p>不推荐设置系统环境变量<code>classpath</code>，始终建议通过<code>-cp</code>命令传入；</p><p>jar包相当于目录，可以包含很多<code>.class</code>文件，方便下载和使用；</p><p><code>MANIFEST.MF</code>文件可以提供jar包的信息，如<code>Main-Class</code>，这样可以直接运行jar包。</p><h2 id="13-模块"><a href="#13-模块" class="headerlink" title="13.模块"></a>13.模块</h2><p>从Java 9开始，JDK又引入了模块（Module）。</p><p>什么是模块？这要从Java 9之前的版本说起。</p><p>我们知道，<code>.class</code>文件是JVM看到的最小可执行文件，而一个大型程序需要编写很多Class，并生成一堆<code>.class</code>文件，很不便于管理，所以，<code>jar</code>文件就是<code>class</code>文件的容器。</p><p>在Java 9之前，一个大型Java程序会生成自己的jar文件，同时引用依赖的第三方jar文件，而JVM自带的Java标准库，实际上也是以jar文件形式存放的，这个文件叫<code>rt.jar</code>，一共有60多M。</p><p>如果是自己开发的程序，除了一个自己的<code>app.jar</code>以外，还需要一堆第三方的jar包，运行一个Java程序，一般来说，命令行写这样：</p><pre><code class="hljs bash">java -cp app.jar:a.jar:b.jar:c.jar com.liaoxuefeng.sample.Main</code></pre><p> 注意：JVM自带的标准库rt.jar不要写到classpath中，写了反而会干扰JVM的正常运行。</p><p>如果漏写了某个运行时需要用到的jar，那么在运行期极有可能抛出<code>ClassNotFoundException</code>。</p><p>所以，jar只是用于存放class的容器，它并不关心class之间的依赖。</p><p>从Java 9开始引入的模块，主要是为了解决“依赖”这个问题。如果<code>a.jar</code>必须依赖另一个<code>b.jar</code>才能运行，那我们应该给<code>a.jar</code>加点说明啥的，让程序在编译和运行的时候能自动定位到<code>b.jar</code>，这种自带“依赖关系”的class容器就是模块。</p><p>为了表明Java模块化的决心，从Java 9开始，原有的Java标准库已经由一个单一巨大的<code>rt.jar</code>分拆成了几十个模块，这些模块以<code>.jmod</code>扩展名标识，可以在<code>$JAVA_HOME/jmods</code>目录下找到它们：</p><ul><li>java.base.jmod</li><li>java.compiler.jmod</li><li>java.datatransfer.jmod</li><li>java.desktop.jmod</li><li>…</li></ul><p>这些<code>.jmod</code>文件每一个都是一个模块，模块名就是文件名。例如：模块<code>java.base</code>对应的文件就是<code>java.base.jmod</code>。模块之间的依赖关系已经被写入到模块内的<code>module-info.class</code>文件了。所有的模块都直接或间接地依赖<code>java.base</code>模块，只有<code>java.base</code>模块不依赖任何模块，它可以被看作是“根模块”，好比所有的类都是从<code>Object</code>直接或间接继承而来。</p><h3 id="编写模块"><a href="#编写模块" class="headerlink" title="编写模块"></a>编写模块</h3><p>那么，我们应该如何编写模块呢？还是以具体的例子来说。首先，创建模块和原有的创建Java项目是完全一样的，以<code>oop-module</code>工程为例，它的目录结构如下：</p><pre><code class="hljs ascii">oop-module├── bin├── build.sh└── src    ├── com    │   └── itranswarp    │       └── sample    │           ├── Greeting.java    │           └── Main.java    └── module-info.java</code></pre><p>其中，<code>bin</code>目录存放编译后的class文件，<code>src</code>目录存放源码，按包名的目录结构存放，仅仅在<code>src</code>目录下多了一个<code>module-info.java</code>这个文件，这就是模块的描述文件。在这个模块中，它长这样：</p><pre><code class="hljs cpp"><span class="hljs-keyword">module</span> hello.world &#123;<span class="hljs-keyword">requires</span> java.base; <span class="hljs-comment">// 可不写，任何模块都会自动引入java.base</span><span class="hljs-keyword">requires</span> java.xml;&#125;</code></pre><p>其中，<code>module</code>是关键字，后面的<code>hello.world</code>是模块的名称，它的命名规范与包一致。花括号的<code>requires xxx;</code>表示这个模块需要引用的其他模块名。除了<code>java.base</code>可以被自动引入外，这里我们引入了一个<code>java.xml</code>的模块。</p><p>当我们使用模块声明了依赖关系后，才能使用引入的模块。例如，<code>Main.java</code>代码如下：</p><pre><code class="hljs arduino">package com.itranswarp.sample;<span class="hljs-comment">// 必须引入java.xml模块后才能使用其中的类:</span><span class="hljs-keyword">import</span> javax.xml.XMLConstants;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;Greeting g = <span class="hljs-keyword">new</span> Greeting();System.out.<span class="hljs-built_in">println</span>(g.hello(XMLConstants.XML_NS_PREFIX));&#125;&#125;</code></pre><p>如果把<code>requires java.xml;</code>从<code>module-info.java</code>中去掉，编译将报错。可见，模块的重要作用就是声明依赖关系。</p><p>下面，我们用JDK提供的命令行工具来编译并创建模块。</p><p>首先，我们把工作目录切换到<code>oop-module</code>，在当前目录下编译所有的<code>.java</code>文件，并存放到<code>bin</code>目录下，命令如下：</p><pre><code class="hljs bash">$ javac -d bin src/module-info.java src/com/itranswarp/sample/*.java</code></pre><p>如果编译成功，现在项目结构如下：</p><pre><code class="hljs ascii">oop-module├── bin│   ├── com│   │   └── itranswarp│   │       └── sample│   │           ├── Greeting.class│   │           └── Main.class│   └── module-info.class└── src    ├── com    │   └── itranswarp    │       └── sample    │           ├── Greeting.java    │           └── Main.java    └── module-info.java</code></pre><p>注意到<code>src</code>目录下的<code>module-info.java</code>被编译到<code>bin</code>目录下的<code>module-info.class</code>。</p><p>下一步，我们需要把bin目录下的所有class文件先打包成jar，在打包的时候，注意传入<code>--main-class</code>参数，让这个jar包能自己定位<code>main</code>方法所在的类：</p><pre><code class="hljs bash">$ jar --create --file hello.jar --main-class com.itranswarp.sample.Main -C bin .</code></pre><p>现在我们就在当前目录下得到了<code>hello.jar</code>这个jar包，它和普通jar包并无区别，可以直接使用命令<code>java -jar hello.jar</code>来运行它。但是我们的目标是创建模块，所以，继续使用JDK自带的<code>jmod</code>命令把一个jar包转换成模块：</p><pre><code class="hljs bash">$ jmod create --class-path hello.jar hello.jmod</code></pre><p>于是，在当前目录下我们又得到了<code>hello.jmod</code>这个模块文件，这就是最后打包出来的传说中的模块！</p><h3 id="运行模块"><a href="#运行模块" class="headerlink" title="运行模块"></a>运行模块</h3><p>要运行一个jar，我们使用<code>java -jar xxx.jar</code>命令。要运行一个模块，我们只需要指定模块名。试试：</p><pre><code class="hljs bash">$ java --module-path hello.jmod --module hello.world</code></pre><p>结果是一个错误：</p><pre><code class="hljs bash">Error occurred during initialization of boot layerjava.lang.module.FindException: JMOD format not supported at execution time: hello.jmod</code></pre><p>原因是<code>.jmod</code>不能被放入<code>--module-path</code>中。换成<code>.jar</code>就没问题了：</p><pre><code class="hljs bash">$ java --module-path hello.jar --module hello.worldHello, xml!</code></pre><p>那我们辛辛苦苦创建的<code>hello.jmod</code>有什么用？答案是我们可以用它来打包JRE。</p><h3 id="打包JRE"><a href="#打包JRE" class="headerlink" title="打包JRE"></a>打包JRE</h3><p>前面讲了，为了支持模块化，Java 9首先带头把自己的一个巨大无比的<code>rt.jar</code>拆成了几十个<code>.jmod</code>模块，原因就是，运行Java程序的时候，实际上我们用到的JDK模块，并没有那么多。不需要的模块，完全可以删除。</p><p>过去发布一个Java应用程序，要运行它，必须下载一个完整的JRE，再运行jar包。而完整的JRE块头很大，有100多M。怎么给JRE瘦身呢？</p><p>现在，JRE自身的标准库已经分拆成了模块，只需要带上程序用到的模块，其他的模块就可以被裁剪掉。怎么裁剪JRE呢？并不是说把系统安装的JRE给删掉部分模块，而是“复制”一份JRE，但只带上用到的模块。为此，JDK提供了<code>jlink</code>命令来干这件事。命令如下：</p><pre><code class="hljs bash">$ jlink --module-path hello.jmod --add-modules java.base,java.xml,hello.world --output jre/</code></pre><p>我们在<code>--module-path</code>参数指定了我们自己的模块<code>hello.jmod</code>，然后，在<code>--add-modules</code>参数中指定了我们用到的3个模块<code>java.base</code>、<code>java.xml</code>和<code>hello.world</code>，用<code>,</code>分隔。最后，在<code>--output</code>参数指定输出目录。</p><p>现在，在当前目录下，我们可以找到<code>jre</code>目录，这是一个完整的并且带有我们自己<code>hello.jmod</code>模块的JRE。试试直接运行这个JRE：</p><pre><code class="hljs bash">$ jre/bin/java --module hello.worldHello, xml!</code></pre><p>要分发我们自己的Java应用程序，只需要把这个<code>jre</code>目录打个包给对方发过去，对方直接运行上述命令即可，既不用下载安装JDK，也不用知道如何配置我们自己的模块，极大地方便了分发和部署。</p><h3 id="访问权限"><a href="#访问权限" class="headerlink" title="访问权限"></a>访问权限</h3><p>前面我们讲过，Java的class访问权限分为public、protected、private和默认的包访问权限。引入模块后，这些访问权限的规则就要稍微做些调整。</p><p>确切地说，class的这些访问权限只在一个模块内有效，模块和模块之间，例如，a模块要访问b模块的某个class，必要条件是b模块明确地导出了可以访问的包。</p><p>举个例子：我们编写的模块<code>hello.world</code>用到了模块<code>java.xml</code>的一个类<code>javax.xml.XMLConstants</code>，我们之所以能直接使用这个类，是因为模块<code>java.xml</code>的<code>module-info.java</code>中声明了若干导出：</p><pre><code class="hljs lasso">module java.<span class="hljs-built_in">xml</span> &#123;    exports java.<span class="hljs-built_in">xml</span>;    exports javax.<span class="hljs-built_in">xml</span>.catalog;    exports javax.<span class="hljs-built_in">xml</span>.datatype;    <span class="hljs-params">...</span>&#125;</code></pre><p>只有它声明的导出的包，外部代码才被允许访问。换句话说，如果外部代码想要访问我们的<code>hello.world</code>模块中的<code>com.itranswarp.sample.Greeting</code>类，我们必须将其导出：</p><pre><code class="hljs java"><span class="hljs-keyword">module</span> hello.world &#123;    <span class="hljs-keyword">exports</span> com.itranswarp.sample;    <span class="hljs-keyword">requires</span> java.base;<span class="hljs-keyword">requires</span> java.xml;&#125;</code></pre><p>因此，模块进一步隔离了代码的访问权限。</p><h3 id="小结-5"><a href="#小结-5" class="headerlink" title="小结"></a>小结</h3><p>Java 9引入的模块目的是为了管理依赖；</p><p>使用模块可以按需打包JRE；</p><p>使用模块对类的访问权限有了进一步限制。</p><h1 id="二、java核心类"><a href="#二、java核心类" class="headerlink" title="二、java核心类"></a>二、java核心类</h1><h2 id="1-字符串和编码"><a href="#1-字符串和编码" class="headerlink" title="1.字符串和编码"></a>1.字符串和编码</h2><h3 id="String"><a href="#String" class="headerlink" title="String"></a>String</h3><p>在Java中，<code>String</code>是一个引用类型，它本身也是一个<code>class</code>。但是，Java编译器对<code>String</code>有特殊处理，即可以直接用<code>&quot;...&quot;</code>来表示一个字符串：</p><pre><code class="hljs armasm"><span class="hljs-keyword">String </span><span class="hljs-built_in">s1</span> = <span class="hljs-string">"Hello!"</span><span class="hljs-comment">;</span></code></pre><p>实际上字符串在<code>String</code>内部是通过一个<code>char[]</code>数组表示的，因此，按下面的写法也是可以的：</p><pre><code class="hljs arduino"><span class="hljs-keyword">String</span> s2 = <span class="hljs-keyword">new</span> <span class="hljs-keyword">String</span>(<span class="hljs-keyword">new</span> <span class="hljs-keyword">char</span>[] &#123;<span class="hljs-string">'H'</span>, <span class="hljs-string">'e'</span>, <span class="hljs-string">'l'</span>, <span class="hljs-string">'l'</span>, <span class="hljs-string">'o'</span>, <span class="hljs-string">'!'</span>&#125;);</code></pre><p>因为<code>String</code>太常用了，所以Java提供了<code>&quot;...&quot;</code>这种字符串字面量表示方法。</p><p>Java字符串的一个重要特点就是字符串<em>不可变</em>。这种不可变性是通过内部的<code>private final char[]</code>字段，以及没有任何修改<code>char[]</code>的方法实现的。</p><h3 id="字符串比较"><a href="#字符串比较" class="headerlink" title="字符串比较"></a>字符串比较</h3><p>当我们想要比较两个字符串是否相同时，要特别注意，我们实际上是想比较字符串的内容是否相同。必须使用<code>equals()</code>方法而不能用<code>==</code>。</p><p>结论：两个字符串比较，必须总是使用<code>equals()</code>方法。</p><p>要忽略大小写比较，使用<code>equalsIgnoreCase()</code>方法。</p><p><code>String</code>类还提供了多种方法来搜索子串、提取子串。常用的方法有：</p><pre><code class="hljs jboss-cli"><span class="hljs-string">//</span> 是否包含子串:<span class="hljs-string">"Hello"</span><span class="hljs-string">.contains</span><span class="hljs-params">("ll")</span>; <span class="hljs-string">//</span> <span class="hljs-literal">true</span></code></pre><p>注意到<code>contains()</code>方法的参数是<code>CharSequence</code>而不是<code>String</code>，因为<code>CharSequence</code>是<code>String</code>的父类。</p><p>搜索子串的更多的例子：</p><pre><code class="hljs reasonml"><span class="hljs-string">"Hello"</span>.index<span class="hljs-constructor">Of(<span class="hljs-string">"l"</span>)</span>; <span class="hljs-comment">// 2</span><span class="hljs-string">"Hello"</span>.last<span class="hljs-constructor">IndexOf(<span class="hljs-string">"l"</span>)</span>; <span class="hljs-comment">// 3</span><span class="hljs-string">"Hello"</span>.starts<span class="hljs-constructor">With(<span class="hljs-string">"He"</span>)</span>; <span class="hljs-comment">// true</span><span class="hljs-string">"Hello"</span>.ends<span class="hljs-constructor">With(<span class="hljs-string">"lo"</span>)</span>; <span class="hljs-comment">// true</span></code></pre><p>提取子串的例子：</p><pre><code class="hljs jboss-cli"><span class="hljs-string">"Hello"</span><span class="hljs-string">.substring</span><span class="hljs-params">(2)</span>; <span class="hljs-string">//</span> <span class="hljs-string">"llo"</span><span class="hljs-string">"Hello"</span><span class="hljs-string">.substring</span><span class="hljs-params">(2, 4)</span>; <span class="hljs-string">"ll"</span></code></pre><p>注意索引号是从<code>0</code>开始的。</p><h3 id="去除首尾空白字符"><a href="#去除首尾空白字符" class="headerlink" title="去除首尾空白字符"></a>去除首尾空白字符</h3><p>使用<code>trim()</code>方法可以移除字符串首尾空白字符。空白字符包括空格，<code>\t</code>，<code>\r</code>，<code>\n</code>：</p><pre><code class="hljs jboss-cli"><span class="hljs-string">"  \tHello\r\n "</span><span class="hljs-string">.trim</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-string">"Hello"</span></code></pre><p>注意：<code>trim()</code>并没有改变字符串的内容，而是返回了一个新字符串。</p><p>另一个<code>strip()</code>方法也可以移除字符串首尾空白字符。它和<code>trim()</code>不同的是，类似中文的空格字符<code>\u3000</code>也会被移除：</p><pre><code class="hljs jboss-cli"><span class="hljs-string">"\u3000Hello\u3000"</span><span class="hljs-string">.strip</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-string">"Hello"</span><span class="hljs-string">" Hello "</span><span class="hljs-string">.stripLeading</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-string">"Hello "</span><span class="hljs-string">" Hello "</span><span class="hljs-string">.stripTrailing</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-string">" Hello"</span></code></pre><p><code>String</code>还提供了<code>isEmpty()</code>和<code>isBlank()</code>来判断字符串是否为空和空白字符串：</p><pre><code class="hljs jboss-cli"><span class="hljs-string">""</span><span class="hljs-string">.isEmpty</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-literal">true</span>，因为字符串长度为0<span class="hljs-string">"  "</span><span class="hljs-string">.isEmpty</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-literal">false</span>，因为字符串长度不为0<span class="hljs-string">"  \n"</span><span class="hljs-string">.isBlank</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-literal">true</span>，因为只包含空白字符<span class="hljs-string">" Hello "</span><span class="hljs-string">.isBlank</span><span class="hljs-params">()</span>; <span class="hljs-string">//</span> <span class="hljs-literal">false</span>，因为包含非空白字符</code></pre><h3 id="替换子串"><a href="#替换子串" class="headerlink" title="替换子串"></a>替换子串</h3><p>要在字符串中替换子串，有两种方法。一种是根据字符或字符串替换：</p><pre><code class="hljs awk">String s = <span class="hljs-string">"hello"</span>;s.replace(<span class="hljs-string">'l'</span>, <span class="hljs-string">'w'</span>); <span class="hljs-regexp">//</span> <span class="hljs-string">"hewwo"</span>，所有字符<span class="hljs-string">'l'</span>被替换为<span class="hljs-string">'w'</span>s.replace(<span class="hljs-string">"ll"</span>, <span class="hljs-string">"~~"</span>); <span class="hljs-regexp">//</span> <span class="hljs-string">"he~~o"</span>，所有子串<span class="hljs-string">"ll"</span>被替换为<span class="hljs-string">"~~"</span></code></pre><p>另一种是通过正则表达式替换：</p><pre><code class="hljs lasso"><span class="hljs-built_in">String</span> s = <span class="hljs-string">"A,,B;C ,D"</span>;s.replaceAll(<span class="hljs-string">"[\\,\\;\\s]+"</span>, <span class="hljs-string">","</span>); <span class="hljs-comment">// "A,B,C,D"</span></code></pre><p>上面的代码通过正则表达式，把匹配的子串统一替换为<code>&quot;,&quot;</code>。关于正则表达式的用法我们会在后面详细讲解。</p><h3 id="分割字符串"><a href="#分割字符串" class="headerlink" title="分割字符串"></a>分割字符串</h3><p>要分割字符串，使用<code>split()</code>方法，并且传入的也是正则表达式：</p><pre><code class="hljs cmake"><span class="hljs-keyword">String</span> s = <span class="hljs-string">"A,B,C,D"</span>;<span class="hljs-keyword">String</span>[] ss = s.split(<span class="hljs-string">"\\,"</span>); // &#123;<span class="hljs-string">"A"</span>, <span class="hljs-string">"B"</span>, <span class="hljs-string">"C"</span>, <span class="hljs-string">"D"</span>&#125;</code></pre><h3 id="拼接字符串"><a href="#拼接字符串" class="headerlink" title="拼接字符串"></a>拼接字符串</h3><p>拼接字符串使用静态方法<code>join()</code>，它用指定的字符串连接字符串数组：</p><pre><code class="hljs processing"><span class="hljs-keyword">String</span>[] arr = &#123;<span class="hljs-string">"A"</span>, <span class="hljs-string">"B"</span>, <span class="hljs-string">"C"</span>&#125;;<span class="hljs-keyword">String</span> s = <span class="hljs-keyword">String</span>.<span class="hljs-built_in">join</span>(<span class="hljs-string">"***"</span>, arr); <span class="hljs-comment">// "A***B***C"</span></code></pre><h3 id="格式化字符串"><a href="#格式化字符串" class="headerlink" title="格式化字符串"></a>格式化字符串</h3><p>字符串提供了<code>formatted()</code>方法和<code>format()</code>静态方法，可以传入其他参数，替换占位符，然后生成新的字符串：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        <span class="hljs-keyword">String</span> s = <span class="hljs-string">"Hi %s, your score is %d!"</span>;        System.out.<span class="hljs-built_in">println</span>(s.formatted(<span class="hljs-string">"Alice"</span>, <span class="hljs-number">80</span>));        System.out.<span class="hljs-built_in">println</span>(<span class="hljs-keyword">String</span>.format(<span class="hljs-string">"Hi %s, your score is %.2f!"</span>, <span class="hljs-string">"Bob"</span>, <span class="hljs-number">59.5</span>));    &#125;&#125;</code></pre><p>有几个占位符，后面就传入几个参数。参数类型要和占位符一致。我们经常用这个方法来格式化信息。常用的占位符有：</p><ul><li><code>%s</code>：显示字符串；</li><li><code>%d</code>：显示整数；</li><li><code>%x</code>：显示十六进制整数；</li><li><code>%f</code>：显示浮点数。</li></ul><p>占位符还可以带格式，例如<code>%.2f</code>表示显示两位小数。如果你不确定用啥占位符，那就始终用<code>%s</code>，因为<code>%s</code>可以显示任何数据类型。要查看完整的格式化语法，请参考<a href="https://docs.oracle.com/en/java/javase/14/docs/api/java.base/java/util/Formatter.html#syntax" target="_blank" rel="noopener">JDK文档</a>。</p><h3 id="类型转换"><a href="#类型转换" class="headerlink" title="类型转换"></a>类型转换</h3><p>要把任意基本类型或引用类型转换为字符串，可以使用静态方法<code>valueOf()</code>。这是一个重载方法，编译器会根据参数自动选择合适的方法：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">String</span>.</span></span>value<span class="hljs-constructor">Of(123)</span>; <span class="hljs-comment">// "123"</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">String</span>.</span></span>value<span class="hljs-constructor">Of(45.67)</span>; <span class="hljs-comment">// "45.67"</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">String</span>.</span></span>value<span class="hljs-constructor">Of(<span class="hljs-params">true</span>)</span>; <span class="hljs-comment">// "true"</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">String</span>.</span></span>value<span class="hljs-constructor">Of(<span class="hljs-params">new</span> Object()</span>); <span class="hljs-comment">// 类似java.lang.Object@636be97c</span></code></pre><p>要把字符串转换为其他类型，就需要根据情况。例如，把字符串转换为<code>int</code>类型：</p><pre><code class="hljs reasonml"><span class="hljs-built_in">int</span> n1 = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span>parse<span class="hljs-constructor">Int(<span class="hljs-string">"123"</span>)</span>; <span class="hljs-comment">// 123</span><span class="hljs-built_in">int</span> n2 = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span>parse<span class="hljs-constructor">Int(<span class="hljs-string">"ff"</span>, 16)</span>; <span class="hljs-comment">// 按十六进制转换，255</span></code></pre><p>把字符串转换为<code>boolean</code>类型：</p><pre><code class="hljs reasonml">boolean b1 = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Boolean</span>.</span></span>parse<span class="hljs-constructor">Boolean(<span class="hljs-string">"true"</span>)</span>; <span class="hljs-comment">// true</span>boolean b2 = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Boolean</span>.</span></span>parse<span class="hljs-constructor">Boolean(<span class="hljs-string">"FALSE"</span>)</span>; <span class="hljs-comment">// false</span></code></pre><p>要特别注意，<code>Integer</code>有个<code>getInteger(String)</code>方法，它不是将字符串转换为<code>int</code>，而是把该字符串对应的系统变量转换为<code>Integer</code>：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span>get<span class="hljs-constructor">Integer(<span class="hljs-string">"java.version"</span>)</span>; <span class="hljs-comment">// 版本号，11</span></code></pre><h3 id="转换为char"><a href="#转换为char" class="headerlink" title="转换为char[]"></a>转换为char[]</h3><p><code>String</code>和<code>char[]</code>类型可以互相转换，方法是：</p><pre><code class="hljs reasonml"><span class="hljs-built_in">char</span><span class="hljs-literal">[]</span> cs = <span class="hljs-string">"Hello"</span>.<span class="hljs-keyword">to</span><span class="hljs-constructor">CharArray()</span>; <span class="hljs-comment">// String -&gt; char[]</span>String s = <span class="hljs-keyword">new</span> <span class="hljs-constructor">String(<span class="hljs-params">cs</span>)</span>; <span class="hljs-comment">// char[] -&gt; String</span></code></pre><h3 id="字符编码"><a href="#字符编码" class="headerlink" title="字符编码"></a>字符编码</h3><p>在早期的计算机系统中，为了给字符编码，美国国家标准学会（American National Standard Institute：ANSI）制定了一套英文字母、数字和常用符号的编码，它占用一个字节，编码范围从<code>0</code>到<code>127</code>，最高位始终为<code>0</code>，称为<code>ASCII</code>编码。例如，字符<code>&#39;A&#39;</code>的编码是<code>0x41</code>，字符<code>&#39;1&#39;</code>的编码是<code>0x31</code>。</p><p>如果要把汉字也纳入计算机编码，很显然一个字节是不够的。<code>GB2312</code>标准使用两个字节表示一个汉字，其中第一个字节的最高位始终为<code>1</code>，以便和<code>ASCII</code>编码区分开。例如，汉字<code>&#39;中&#39;</code>的<code>GB2312</code>编码是<code>0xd6d0</code>。</p><p>类似的，日文有<code>Shift_JIS</code>编码，韩文有<code>EUC-KR</code>编码，这些编码因为标准不统一，同时使用，就会产生冲突。</p><p>为了统一全球所有语言的编码，全球统一码联盟发布了<code>Unicode</code>编码，它把世界上主要语言都纳入同一个编码，这样，中文、日文、韩文和其他语言就不会冲突。</p><p><code>Unicode</code>编码需要两个或者更多字节表示，我们可以比较中英文字符在<code>ASCII</code>、<code>GB2312</code>和<code>Unicode</code>的编码：</p><p>英文字符<code>&#39;A&#39;</code>的<code>ASCII</code>编码和<code>Unicode</code>编码：</p><pre><code class="hljs ascii">         ┌────┐ASCII:   │ 41 │         └────┘         ┌────┬────┐Unicode: │ 00 │ 41 │         └────┴────┘</code></pre><p>英文字符的<code>Unicode</code>编码就是简单地在前面添加一个<code>00</code>字节。</p><p>中文字符<code>&#39;中&#39;</code>的<code>GB2312</code>编码和<code>Unicode</code>编码：</p><pre><code class="hljs ascii">         ┌────┬────┐GB2312:  │ d6 │ d0 │         └────┴────┘         ┌────┬────┐Unicode: │ 4e │ 2d │         └────┴────┘</code></pre><p>那我们经常使用的<code>UTF-8</code>又是什么编码呢？因为英文字符的<code>Unicode</code>编码高字节总是<code>00</code>，包含大量英文的文本会浪费空间，所以，出现了<code>UTF-8</code>编码，它是一种变长编码，用来把固定长度的<code>Unicode</code>编码变成1～4字节的变长编码。通过<code>UTF-8</code>编码，英文字符<code>&#39;A&#39;</code>的<code>UTF-8</code>编码变为<code>0x41</code>，正好和<code>ASCII</code>码一致，而中文<code>&#39;中&#39;</code>的<code>UTF-8</code>编码为3字节<code>0xe4b8ad</code>。</p><p><code>UTF-8</code>编码的另一个好处是容错能力强。如果传输过程中某些字符出错，不会影响后续字符，因为<code>UTF-8</code>编码依靠高字节位来确定一个字符究竟是几个字节，它经常用来作为传输编码。</p><p>在Java中，<code>char</code>类型实际上就是两个字节的<code>Unicode</code>编码。如果我们要手动把字符串转换成其他编码，可以这样做：</p><pre><code class="hljs mipsasm"><span class="hljs-keyword">byte[] </span><span class="hljs-keyword">b1 </span>= <span class="hljs-string">"Hello"</span>.getBytes()<span class="hljs-comment">; // 按系统默认编码转换，不推荐</span><span class="hljs-keyword">byte[] </span><span class="hljs-keyword">b2 </span>= <span class="hljs-string">"Hello"</span>.getBytes(<span class="hljs-string">"UTF-8"</span>)<span class="hljs-comment">; // 按UTF-8编码转换</span><span class="hljs-keyword">byte[] </span><span class="hljs-keyword">b2 </span>= <span class="hljs-string">"Hello"</span>.getBytes(<span class="hljs-string">"GBK"</span>)<span class="hljs-comment">; // 按GBK编码转换</span><span class="hljs-keyword">byte[] </span><span class="hljs-keyword">b3 </span>= <span class="hljs-string">"Hello"</span>.getBytes(StandardCharsets.UTF_8)<span class="hljs-comment">; // 按UTF-8编码转换</span></code></pre><p>注意：转换编码后，就不再是<code>char</code>类型，而是<code>byte</code>类型表示的数组。</p><p>如果要把已知编码的<code>byte[]</code>转换为<code>String</code>，可以这样做：</p><pre><code class="hljs arduino"><span class="hljs-keyword">byte</span>[] b = ...<span class="hljs-keyword">String</span> s1 = <span class="hljs-keyword">new</span> <span class="hljs-keyword">String</span>(b, <span class="hljs-string">"GBK"</span>); <span class="hljs-comment">// 按GBK转换</span><span class="hljs-keyword">String</span> s2 = <span class="hljs-keyword">new</span> <span class="hljs-keyword">String</span>(b, StandardCharsets.UTF_8); <span class="hljs-comment">// 按UTF-8转换</span></code></pre><p>始终牢记：<strong>Java的<code>String</code>和<code>char</code>在内存中总是以Unicode编码表示。</strong></p><h3 id="小结-6"><a href="#小结-6" class="headerlink" title="小结"></a>小结</h3><ul><li>Java字符串<code>String</code>是不可变对象；</li><li>字符串操作不改变原字符串内容，而是返回新字符串；</li><li>常用的字符串操作：提取子串、查找、替换、大小写转换等；</li><li>Java使用Unicode编码表示<code>String</code>和<code>char</code>；</li><li>转换编码就是将<code>String</code>和<code>byte[]</code>转换，需要指定编码；</li><li>转换为<code>byte[]</code>时，始终优先考虑<code>UTF-8</code>编码。</li></ul><h2 id="2-String-Builder"><a href="#2-String-Builder" class="headerlink" title="2.String Builder"></a>2.String Builder</h2><p>Java编译器对<code>String</code>做了特殊处理，使得我们可以直接用<code>+</code>拼接字符串。</p><p>考察下面的循环代码：</p><pre><code class="hljs matlab">String s = <span class="hljs-string">""</span>;<span class="hljs-keyword">for</span> (int <span class="hljs-built_in">i</span> = <span class="hljs-number">0</span>; <span class="hljs-built_in">i</span> &lt; <span class="hljs-number">1000</span>; <span class="hljs-built_in">i</span>++) &#123;    s = s + <span class="hljs-string">","</span> + <span class="hljs-built_in">i</span>;&#125;</code></pre><p>虽然可以直接拼接字符串，但是，在循环中，每次循环都会创建新的字符串对象，然后扔掉旧的字符串。这样，绝大部分字符串都是临时对象，不但浪费内存，还会影响GC效率。</p><p>为了能高效拼接字符串，Java标准库提供了<code>StringBuilder</code>，它是一个可变对象，可以预分配缓冲区，这样，往<code>StringBuilder</code>中新增字符时，不会创建新的临时对象：</p><pre><code class="hljs armasm"><span class="hljs-keyword">StringBuilder </span><span class="hljs-built_in">sb</span> = new <span class="hljs-keyword">StringBuilder(1024);</span><span class="hljs-keyword">for </span>(int i = <span class="hljs-number">0</span><span class="hljs-comment">; i &lt; 1000; i++) &#123;</span>    <span class="hljs-built_in">sb</span>.append(<span class="hljs-string">','</span>)<span class="hljs-comment">;</span>    <span class="hljs-built_in">sb</span>.append(i)<span class="hljs-comment">;</span>&#125;<span class="hljs-keyword">String </span>s = <span class="hljs-built_in">sb</span>.toString()<span class="hljs-comment">;</span></code></pre><p><code>StringBuilder</code>还可以进行链式操作：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        var sb = <span class="hljs-keyword">new</span> StringBuilder(<span class="hljs-number">1024</span>);        sb.append(<span class="hljs-string">"Mr "</span>)          .append(<span class="hljs-string">"Bob"</span>)          .append(<span class="hljs-string">"!"</span>)          .insert(<span class="hljs-number">0</span>, <span class="hljs-string">"Hello, "</span>);        System.out.<span class="hljs-built_in">println</span>(sb.toString());    &#125;&#125;</code></pre><p>注意：对于普通的字符串<code>+</code>操作，并不需要我们将其改写为<code>StringBuilder</code>，因为Java编译器在编译时就自动把多个连续的<code>+</code>操作编码为<code>StringConcatFactory</code>的操作。在运行期，<code>StringConcatFactory</code>会自动把字符串连接操作优化为数组复制或者<code>StringBuilder</code>操作。</p><p>你可能还听说过<code>StringBuffer</code>，这是Java早期的一个<code>StringBuilder</code>的线程安全版本，它通过同步来保证多个线程操作<code>StringBuffer</code>也是安全的，但是同步会带来执行速度的下降。</p><p><code>StringBuilder</code>和<code>StringBuffer</code>接口完全相同，现在完全没有必要使用<code>StringBuffer</code>。</p><h2 id="3-StringJoiner"><a href="#3-StringJoiner" class="headerlink" title="3.StringJoiner"></a>3.StringJoiner</h2><p>要高效拼接字符串，应该使用<code>StringBuilder</code>。</p><p>很多时候，我们拼接的字符串像这样：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        <span class="hljs-keyword">String</span>[] names = &#123;<span class="hljs-string">"Bob"</span>, <span class="hljs-string">"Alice"</span>, <span class="hljs-string">"Grace"</span>&#125;;        var sb = <span class="hljs-keyword">new</span> StringBuilder();        sb.append(<span class="hljs-string">"Hello "</span>);        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">String</span> name : names) &#123;            sb.append(name).append(<span class="hljs-string">", "</span>);        &#125;        <span class="hljs-comment">// 注意去掉最后的", ":</span>        sb.<span class="hljs-keyword">delete</span>(sb.length() - <span class="hljs-number">2</span>, sb.length());        sb.append(<span class="hljs-string">"!"</span>);        System.out.<span class="hljs-built_in">println</span>(sb.toString());    &#125;&#125;</code></pre><p>类似用分隔符拼接数组的需求很常见，所以Java标准库还提供了一个<code>StringJoiner</code>来干这个事：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        <span class="hljs-keyword">String</span>[] names = &#123;<span class="hljs-string">"Bob"</span>, <span class="hljs-string">"Alice"</span>, <span class="hljs-string">"Grace"</span>&#125;;        var sj = <span class="hljs-keyword">new</span> StringJoiner(<span class="hljs-string">", "</span>, <span class="hljs-string">"Hello "</span>, <span class="hljs-string">"!"</span>);<span class="hljs-comment">//指定分隔符、开头、结尾</span>        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">String</span> name : names) &#123;            sj.add(name);        &#125;        System.out.<span class="hljs-built_in">println</span>(sj.toString());    &#125;&#125;</code></pre><h3 id="String-join"><a href="#String-join" class="headerlink" title="String.join()"></a>String.join()</h3><p><code>String</code>还提供了一个静态方法<code>join()</code>，这个方法在内部使用了<code>StringJoiner</code>来拼接字符串，在不需要指定“开头”和“结尾”的时候，用<code>String.join()</code>更方便：</p><pre><code class="hljs dart"><span class="hljs-built_in">String</span>[] names = &#123;<span class="hljs-string">"Bob"</span>, <span class="hljs-string">"Alice"</span>, <span class="hljs-string">"Grace"</span>&#125;;<span class="hljs-keyword">var</span> s = <span class="hljs-built_in">String</span>.join(<span class="hljs-string">", "</span>, names);</code></pre><h2 id="4-包装类型"><a href="#4-包装类型" class="headerlink" title="4.包装类型"></a>4.包装类型</h2><p>我们已经知道，Java的数据类型分两种：</p><ul><li>基本类型：<code>byte</code>，<code>short</code>，<code>int</code>，<code>long</code>，<code>boolean</code>，<code>float</code>，<code>double</code>，<code>char</code></li><li>引用类型：所有<code>class</code>和<code>interface</code>类型</li></ul><p>引用类型可以赋值为<code>null</code>，表示空，但基本类型不能赋值为<code>null</code>：</p><pre><code class="hljs dart"><span class="hljs-built_in">String</span> s = <span class="hljs-keyword">null</span>;<span class="hljs-built_in">int</span> n = <span class="hljs-keyword">null</span>; <span class="hljs-comment">// compile error!</span></code></pre><p>那么，如何把一个基本类型视为对象（引用类型）？</p><p>比如，想要把<code>int</code>基本类型变成一个引用类型，我们可以定义一个<code>Integer</code>类，它只包含一个实例字段<code>int</code>，这样，<code>Integer</code>类就可以视为<code>int</code>的包装类（Wrapper Class）：</p><pre><code class="hljs cs"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title">Integer</span> &#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> <span class="hljs-keyword">value</span>;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Integer</span>(<span class="hljs-params"><span class="hljs-keyword">int</span> <span class="hljs-keyword">value</span></span>)</span> &#123;        <span class="hljs-keyword">this</span>.<span class="hljs-keyword">value</span> = <span class="hljs-keyword">value</span>;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">intValue</span>(<span class="hljs-params"></span>)</span> &#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.<span class="hljs-keyword">value</span>;    &#125;&#125;</code></pre><p>定义好了<code>Integer</code>类，我们就可以把<code>int</code>和<code>Integer</code>互相转换：</p><pre><code class="hljs pgsql"><span class="hljs-type">Integer</span> n = <span class="hljs-keyword">null</span>;<span class="hljs-type">Integer</span> n2 = <span class="hljs-built_in">new</span> <span class="hljs-type">Integer</span>(<span class="hljs-number">99</span>);<span class="hljs-type">int</span> n3 = n2.intValue();</code></pre><p>实际上，因为包装类型非常有用，Java核心库为每种基本类型都提供了对应的包装类型：</p><div class="table-container"><table><thead><tr><th style="text-align:left">基本类型</th><th style="text-align:left">对应的引用类型</th></tr></thead><tbody><tr><td style="text-align:left">boolean</td><td style="text-align:left">java.lang.Boolean</td></tr><tr><td style="text-align:left">byte</td><td style="text-align:left">java.lang.Byte</td></tr><tr><td style="text-align:left">short</td><td style="text-align:left">java.lang.Short</td></tr><tr><td style="text-align:left">int</td><td style="text-align:left">java.lang.Integer</td></tr><tr><td style="text-align:left">long</td><td style="text-align:left">java.lang.Long</td></tr><tr><td style="text-align:left">float</td><td style="text-align:left">java.lang.Float</td></tr><tr><td style="text-align:left">double</td><td style="text-align:left">java.lang.Double</td></tr><tr><td style="text-align:left">char</td><td style="text-align:left">java.lang.Character</td></tr></tbody></table></div><p>我们可以直接使用，并不需要自己去定义。</p><h3 id="Auto-Boxing"><a href="#Auto-Boxing" class="headerlink" title="Auto Boxing"></a>Auto Boxing</h3><p>因为<code>int</code>和<code>Integer</code>可以互相转换：</p><pre><code class="hljs reasonml"><span class="hljs-built_in">int</span> i = <span class="hljs-number">100</span>;Integer n = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span>value<span class="hljs-constructor">Of(<span class="hljs-params">i</span>)</span>;<span class="hljs-built_in">int</span> x = n.<span class="hljs-built_in">int</span><span class="hljs-constructor">Value()</span>;</code></pre><p>所以，Java编译器可以帮助我们自动在<code>int</code>和<code>Integer</code>之间转型：</p><pre><code class="hljs fortran"><span class="hljs-keyword">Integer</span> n = <span class="hljs-number">100</span>; // 编译器自动使用<span class="hljs-keyword">Integer</span>.valueOf(<span class="hljs-built_in">int</span>)<span class="hljs-built_in">int</span> x = n; // 编译器自动使用<span class="hljs-keyword">Integer</span>.intValue()</code></pre><p>这种直接把<code>int</code>变为<code>Integer</code>的赋值写法，称为自动装箱（Auto Boxing），反过来，把<code>Integer</code>变为<code>int</code>的赋值写法，称为自动拆箱（Auto Unboxing）。</p><h3 id="不变类"><a href="#不变类" class="headerlink" title="不变类"></a>不变类</h3><p>所有的包装类型都是不变类。我们查看<code>Integer</code>的源码可知，它的核心代码如下：</p><pre><code class="hljs angelscript"><span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Integer</span> &#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> value;&#125;</code></pre><p>因此，一旦创建了<code>Integer</code>对象，该对象就是不变的。</p><p>对两个<code>Integer</code>实例进行比较要特别注意：绝对不能用<code>==</code>比较，因为<code>Integer</code>是引用类型，必须使用<code>equals()</code>比较。</p><p>因为<code>Integer.valueOf()</code>可能始终返回同一个<code>Integer</code>实例，因此，在我们自己创建<code>Integer</code>的时候，以下两种方法：</p><ul><li>方法1：<code>Integer n = new Integer(100);</code></li><li>方法2：<code>Integer n = Integer.valueOf(100);</code></li></ul><p>方法2更好，因为方法1总是创建新的<code>Integer</code>实例，方法2把内部优化留给<code>Integer</code>的实现者去做，即使在当前版本没有优化，也有可能在下一个版本进行优化。</p><p>我们把能创建“新”对象的静态方法称为静态工厂方法。<code>Integer.valueOf()</code>就是静态工厂方法，它尽可能地返回缓存的实例以节省内存。</p><p> 创建新对象时，优先选用静态工厂方法而不是new操作符。</p><p>如果我们考察<code>Byte.valueOf()</code>方法的源码，可以看到，标准库返回的<code>Byte</code>实例全部是缓存实例，但调用者并不关心静态工厂方法以何种方式创建新实例还是直接返回缓存的实例。</p><h3 id="进制转换"><a href="#进制转换" class="headerlink" title="进制转换"></a>进制转换</h3><p><code>Integer</code>类本身还提供了大量方法，例如，最常用的静态方法<code>parseInt()</code>可以把字符串解析成一个整数：</p><pre><code class="hljs reasonml"><span class="hljs-built_in">int</span> x1 = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span>parse<span class="hljs-constructor">Int(<span class="hljs-string">"100"</span>)</span>; <span class="hljs-comment">// 100</span><span class="hljs-built_in">int</span> x2 = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span>parse<span class="hljs-constructor">Int(<span class="hljs-string">"100"</span>, 16)</span>; <span class="hljs-comment">// 256,因为按16进制解析</span></code></pre><p><code>Integer</code>还可以把整数格式化为指定进制的字符串：</p><pre><code class="hljs reasonml">public <span class="hljs-keyword">class</span> Main &#123;    public static void main(String<span class="hljs-literal">[]</span> args) &#123;        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">String(100)</span>); <span class="hljs-comment">// "100",表示为10进制</span>        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">String(100, 36)</span>); <span class="hljs-comment">// "2s",表示为36进制</span>        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">HexString(100)</span>); <span class="hljs-comment">// "64",表示为16进制</span>        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">OctalString(100)</span>); <span class="hljs-comment">// "144",表示为8进制</span>        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Integer</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">BinaryString(100)</span>); <span class="hljs-comment">// "1100100",表示为2进制</span>    &#125;&#125;</code></pre><p>注意：上述方法的输出都是<code>String</code>，在计算机内存中，只用二进制表示，不存在十进制或十六进制的表示方法。<code>int n = 100</code>在内存中总是以4字节的二进制表示：</p><pre><code class="hljs ascii">┌────────┬────────┬────────┬────────┐│00000000│00000000│00000000│01100100│└────────┴────────┴────────┴────────┘</code></pre><p>我们经常使用的<code>System.out.println(n);</code>是依靠核心库自动把整数格式化为10进制输出并显示在屏幕上，使用<code>Integer.toHexString(n)</code>则通过核心库自动把整数格式化为16进制。</p><p>这里我们注意到程序设计的一个重要原则：数据的存储和显示要分离。</p><p>Java的包装类型还定义了一些有用的静态变量</p><pre><code class="hljs gradle"><span class="hljs-comment">// boolean只有两个值true/false，其包装类型只需要引用Boolean提供的静态字段:</span><span class="hljs-keyword">Boolean</span> t = <span class="hljs-keyword">Boolean</span>.<span class="hljs-keyword">TRUE</span>;<span class="hljs-keyword">Boolean</span> f = <span class="hljs-keyword">Boolean</span>.<span class="hljs-keyword">FALSE</span>;<span class="hljs-comment">// int可表示的最大/最小值:</span><span class="hljs-keyword">int</span> max = Integer.MAX_VALUE; <span class="hljs-comment">// 2147483647</span><span class="hljs-keyword">int</span> min = Integer.MIN_VALUE; <span class="hljs-comment">// -2147483648</span><span class="hljs-comment">// long类型占用的bit和byte数量:</span><span class="hljs-keyword">int</span> sizeOfLong = <span class="hljs-keyword">Long</span>.<span class="hljs-keyword">SIZE</span>; <span class="hljs-comment">// 64 (bits)</span><span class="hljs-keyword">int</span> bytesOfLong = <span class="hljs-keyword">Long</span>.BYTES; <span class="hljs-comment">// 8 (bytes)</span></code></pre><p>最后，所有的整数和浮点数的包装类型都继承自<code>Number</code>，因此，可以非常方便地直接通过包装类型获取各种基本类型：</p><pre><code class="hljs reasonml"><span class="hljs-comment">// 向上转型为Number:</span>Number num = <span class="hljs-keyword">new</span> <span class="hljs-constructor">Integer(999)</span>;<span class="hljs-comment">// 获取byte, int, long, float, double:</span>byte b = num.byte<span class="hljs-constructor">Value()</span>;<span class="hljs-built_in">int</span> n = num.<span class="hljs-built_in">int</span><span class="hljs-constructor">Value()</span>;long ln = num.long<span class="hljs-constructor">Value()</span>;<span class="hljs-built_in">float</span> f = num.<span class="hljs-built_in">float</span><span class="hljs-constructor">Value()</span>;double d = num.double<span class="hljs-constructor">Value()</span>;</code></pre><h3 id="处理无符号整型"><a href="#处理无符号整型" class="headerlink" title="处理无符号整型"></a>处理无符号整型</h3><p>在Java中，并没有无符号整型（Unsigned）的基本数据类型。<code>byte</code>、<code>short</code>、<code>int</code>和<code>long</code>都是带符号整型，最高位是符号位。而C语言则提供了CPU支持的全部数据类型，包括无符号整型。无符号整型和有符号整型的转换在Java中就需要借助包装类型的静态方法完成。</p><p>例如，byte是有符号整型，范围是<code>-128</code>~<code>+127</code>，但如果把<code>byte</code>看作无符号整型，它的范围就是<code>0</code>~<code>255</code>。我们把一个负的<code>byte</code>按无符号整型转换为<code>int</code>：</p><pre><code class="hljs reasonml">public <span class="hljs-keyword">class</span> Main &#123;    public static void main(String<span class="hljs-literal">[]</span> args) &#123;        byte x = -<span class="hljs-number">1</span>;        byte y = <span class="hljs-number">127</span>;        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Byte</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">UnsignedInt(<span class="hljs-params">x</span>)</span>); <span class="hljs-comment">// 255</span>        <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Byte</span>.</span></span><span class="hljs-keyword">to</span><span class="hljs-constructor">UnsignedInt(<span class="hljs-params">y</span>)</span>); <span class="hljs-comment">// 127</span>    &#125;&#125;</code></pre><p>因为<code>byte</code>的<code>-1</code>的二进制表示是<code>11111111</code>，以无符号整型转换后的<code>int</code>就是<code>255</code>。</p><p>类似的，可以把一个<code>short</code>按unsigned转换为<code>int</code>，把一个<code>int</code>按unsigned转换为<code>long</code>。</p><h3 id="小结-7"><a href="#小结-7" class="headerlink" title="小结"></a>小结</h3><p>Java核心库提供的包装类型可以把基本类型包装为<code>class</code>；</p><p>自动装箱和自动拆箱都是在编译期完成的（JDK&gt;=1.5）；</p><p>装箱和拆箱会影响执行效率，且拆箱时可能发生<code>NullPointerException</code>；</p><p>包装类型的比较必须使用<code>equals()</code>；</p><p>整数和浮点数的包装类型都继承自<code>Number</code>；</p><p>包装类型提供了大量实用方法。</p><h2 id="5-JavaBean"><a href="#5-JavaBean" class="headerlink" title="5.JavaBean"></a>5.JavaBean</h2><p>在Java中，有很多<code>class</code>的定义都符合这样的规范：</p><ul><li>若干<code>private</code>实例字段；</li><li>通过<code>public</code>方法来读写实例字段。</li></ul><p>例如：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> &#123;</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">String</span> name;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">String</span> <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123; <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.name; &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setName</span><span class="hljs-params">(<span class="hljs-keyword">String</span> name)</span> </span>&#123; <span class="hljs-keyword">this</span>.name = name; &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123; <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.age; &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> age)</span> </span>&#123; <span class="hljs-keyword">this</span>.age = age; &#125;&#125;</code></pre><p>如果读写方法符合以下这种命名规范：</p><pre><code class="hljs lasso"><span class="hljs-comment">// 读方法:</span><span class="hljs-keyword">public</span> <span class="hljs-keyword">Type</span> getXyz()<span class="hljs-comment">// 写方法:</span><span class="hljs-keyword">public</span> <span class="hljs-literal">void</span> setXyz(<span class="hljs-keyword">Type</span> value)</code></pre><p>那么这种<code>class</code>被称为<code>JavaBean</code>：</p><p>上面的字段是<code>xyz</code>，那么读写方法名分别以<code>get</code>和<code>set</code>开头，并且后接大写字母开头的字段名<code>Xyz</code>，因此两个读写方法名分别是<code>getXyz()</code>和<code>setXyz()</code>。</p><p><code>boolean</code>字段比较特殊，它的读方法一般命名为<code>isXyz()</code>：</p><pre><code class="hljs angelscript"><span class="hljs-comment">// 读方法:</span><span class="hljs-keyword">public</span> <span class="hljs-built_in">bool</span>ean isChild()<span class="hljs-comment">// 写方法:</span><span class="hljs-keyword">public</span> <span class="hljs-built_in">void</span> setChild(<span class="hljs-built_in">bool</span>ean value)</code></pre><p>我们通常把一组对应的读方法（<code>getter</code>）和写方法（<code>setter</code>）称为属性（<code>property</code>）。例如，<code>name</code>属性：</p><ul><li>对应的读方法是<code>String getName()</code></li><li>对应的写方法是<code>setName(String)</code></li></ul><p>只有<code>getter</code>的属性称为只读属性（read-only），例如，定义一个age只读属性：</p><ul><li>对应的读方法是<code>int getAge()</code></li><li>无对应的写方法<code>setAge(int)</code></li></ul><p>类似的，只有<code>setter</code>的属性称为只写属性（write-only）。</p><p>很明显，只读属性很常见，只写属性不常见。</p><h3 id="JavaBean的作用"><a href="#JavaBean的作用" class="headerlink" title="JavaBean的作用"></a>JavaBean的作用</h3><p>JavaBean主要用来传递数据，即把一组数据组合成一个JavaBean便于传输。此外，JavaBean可以方便地被IDE工具分析，生成读写属性的代码，主要用在图形界面的可视化设计中。</p><p>通过IDE，可以快速生成<code>getter</code>和<code>setter</code>。例如，在Eclipse中，先输入以下代码：</p><pre><code class="hljs angelscript"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Person</span> &#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> <span class="hljs-built_in">int</span> age;&#125;</code></pre><p>然后，点击右键，在弹出的菜单中选择“Source”，“Generate Getters and Setters”，在弹出的对话框中选中需要生成<code>getter</code>和<code>setter</code>方法的字段，点击确定即可由IDE自动完成所有方法代码。</p><h3 id="枚举JavaBean属性"><a href="#枚举JavaBean属性" class="headerlink" title="枚举JavaBean属性"></a>枚举JavaBean属性</h3><p>要枚举一个JavaBean的所有属性，可以直接使用Java核心库提供的<code>Introspector</code>：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> <span class="hljs-keyword">throws</span> Exception </span>&#123;        BeanInfo info = Introspector.getBeanInfo(Person<span class="hljs-class">.<span class="hljs-keyword">class</span>)</span>;        <span class="hljs-keyword">for</span> (PropertyDescriptor pd : info.getPropertyDescriptors()) &#123;            System.out.println(pd.getName());            System.out.println(<span class="hljs-string">"  "</span> + pd.getReadMethod());            System.out.println(<span class="hljs-string">"  "</span> + pd.getWriteMethod());        &#125;    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span> </span>&#123;    <span class="hljs-keyword">private</span> String name;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">int</span> age;    <span class="hljs-function"><span class="hljs-keyword">public</span> String <span class="hljs-title">getName</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> name;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setName</span><span class="hljs-params">(String name)</span> </span>&#123;        <span class="hljs-keyword">this</span>.name = name;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">getAge</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> age;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title">setAge</span><span class="hljs-params">(<span class="hljs-keyword">int</span> age)</span> </span>&#123;        <span class="hljs-keyword">this</span>.age = age;    &#125;&#125;</code></pre><p>运行结果：</p><pre><code class="hljs reasonml">age   public <span class="hljs-built_in">int</span> <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>get<span class="hljs-constructor">Age()</span>   public void <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>set<span class="hljs-constructor">Age(<span class="hljs-params">int</span>)</span> <span class="hljs-keyword">class</span>   public final native java.lang.Class java.lang.<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Object</span>.</span></span>get<span class="hljs-constructor">Class()</span>   null name   public java.lang.String <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>get<span class="hljs-constructor">Name()</span>   public void <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Person</span>.</span></span>set<span class="hljs-constructor">Name(<span class="hljs-params">java</span>.<span class="hljs-params">lang</span>.String)</span></code></pre><p>运行上述代码，可以列出所有的属性，以及对应的读写方法。注意<code>class</code>属性是从<code>Object</code>继承的<code>getClass()</code>方法带来的。</p><h3 id="小结-8"><a href="#小结-8" class="headerlink" title="小结"></a>小结</h3><p>JavaBean是一种符合命名规范的<code>class</code>，它通过<code>getter</code>和<code>setter</code>来定义属性；</p><p>属性是一种通用的叫法，并非Java语法规定；</p><p>可以利用IDE快速生成<code>getter</code>和<code>setter</code>；</p><p>使用<code>Introspector.getBeanInfo()</code>可以获取属性列表。</p><h2 id="6-枚举类"><a href="#6-枚举类" class="headerlink" title="6.枚举类"></a>6.枚举类</h2><p>在Java中，我们可以通过<code>static final</code>来定义常量。例如，我们希望定义周一到周日这7个常量，可以用7个不同的<code>int</code>表示：</p><pre><code class="hljs angelscript"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Weekday</span> &#123;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> SUN = <span class="hljs-number">0</span>;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> MON = <span class="hljs-number">1</span>;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> TUE = <span class="hljs-number">2</span>;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> WED = <span class="hljs-number">3</span>;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> THU = <span class="hljs-number">4</span>;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> FRI = <span class="hljs-number">5</span>;    <span class="hljs-keyword">public</span> static <span class="hljs-keyword">final</span> <span class="hljs-built_in">int</span> SAT = <span class="hljs-number">6</span>;&#125;</code></pre><p>使用常量的时候，可以这么引用：</p><pre><code class="hljs arcade"><span class="hljs-keyword">if</span> (day == <span class="hljs-built_in">Weekday</span>.SAT || day == <span class="hljs-built_in">Weekday</span>.SUN) &#123;    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span> work at home</span>&#125;</code></pre><p>也可以把常量定义为字符串类型，例如，定义3种颜色的常量：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Color</span> &#123;</span>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">String</span> RED = <span class="hljs-string">"r"</span>;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">String</span> GREEN = <span class="hljs-string">"g"</span>;    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">String</span> BLUE = <span class="hljs-string">"b"</span>;&#125;</code></pre><p>使用常量的时候，可以这么引用：</p><pre><code class="hljs lasso"><span class="hljs-built_in">String</span> color = <span class="hljs-params">...</span><span class="hljs-keyword">if</span> (Color.RED.<span class="hljs-keyword">equals</span>(color)) &#123;    <span class="hljs-comment">// <span class="hljs-doctag">TODO:</span></span>&#125;</code></pre><p>无论是<code>int</code>常量还是<code>String</code>常量，使用这些常量来表示一组枚举值的时候，有一个严重的问题就是，编译器无法检查每个值的合理性。例如：</p><pre><code class="hljs jboss-cli"><span class="hljs-keyword">if</span> <span class="hljs-params">(<span class="hljs-attr">weekday</span> == 6 || <span class="hljs-attr">weekday</span> == 7)</span> &#123;    <span class="hljs-keyword">if</span> <span class="hljs-params">(<span class="hljs-attr">tasks</span> == Weekday.MON)</span> &#123;        <span class="hljs-string">//</span> TODO:    &#125;&#125;</code></pre><p>上述代码编译和运行均不会报错，但存在两个问题：</p><ul><li>注意到<code>Weekday</code>定义的常量范围是<code>0</code>~<code>6</code>，并不包含<code>7</code>，编译器无法检查不在枚举中的<code>int</code>值；</li><li>定义的常量仍可与其他变量比较，但其用途并非是枚举星期值。</li></ul><h3 id="enum"><a href="#enum" class="headerlink" title="enum"></a>enum</h3><p>为了让编译器能自动检查某个值在枚举的集合内，并且，不同用途的枚举需要不同的类型来标记，不能混用，我们可以使用<code>enum</code>来定义枚举类：</p><pre><code class="hljs crystal">public <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    public static void main(String[] args) &#123;        Weekday day = Weekday.FRI;        <span class="hljs-keyword">if</span> (day == Weekday.SAT || day == Weekday.SUN) &#123;            System.<span class="hljs-keyword">out</span>.println(<span class="hljs-string">"Work at home!"</span>);        &#125; <span class="hljs-keyword">else</span> &#123;            System.<span class="hljs-keyword">out</span>.println(<span class="hljs-string">"Work at office!"</span>);        &#125;    &#125;&#125;<span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">Weekday</span> &#123;</span>    SUN, MON, TUE, WED, THU, FRI, SAT;&#125;</code></pre><p>注意到定义枚举类是通过关键字<code>enum</code>实现的，我们只需依次列出枚举的常量名。</p><p>和<code>int</code>定义的常量相比，使用<code>enum</code>定义枚举有如下好处：</p><p>首先，<code>enum</code>常量本身带有类型信息，即<code>Weekday.SUN</code>类型是<code>Weekday</code>，编译器会自动检查出类型错误。例如，下面的语句不可能编译通过：</p><pre><code class="hljs pgsql"><span class="hljs-type">int</span> day = <span class="hljs-number">1</span>;<span class="hljs-keyword">if</span> (day == Weekday.SUN) &#123; // Compile error: bad operand <span class="hljs-keyword">types</span> <span class="hljs-keyword">for</span> binary <span class="hljs-keyword">operator</span> <span class="hljs-string">'=='</span>&#125;</code></pre><p>其次，不可能引用到非枚举的值，因为无法通过编译。</p><p>最后，不同类型的枚举不能互相比较或者赋值，因为类型不符。例如，不能给一个<code>Weekday</code>枚举类型的变量赋值为<code>Color</code>枚举类型的值：</p><pre><code class="hljs arcade"><span class="hljs-built_in">Weekday</span> x = <span class="hljs-built_in">Weekday</span>.SUN; <span class="hljs-comment">// ok!</span><span class="hljs-built_in">Weekday</span> y = Color.RED; <span class="hljs-comment">// Compile error: incompatible types</span></code></pre><p>这就使得编译器可以在编译期自动检查出所有可能的潜在错误。</p><h3 id="enum的比较"><a href="#enum的比较" class="headerlink" title="enum的比较"></a>enum的比较</h3><p>使用<code>enum</code>定义的枚举类是一种引用类型。前面我们讲到，引用类型比较，要使用<code>equals()</code>方法，如果使用<code>==</code>比较，它比较的是两个引用类型的变量是否是同一个对象。因此，引用类型比较，要始终使用<code>equals()</code>方法，但<code>enum</code>类型可以例外。</p><p>这是因为<code>enum</code>类型的每个常量在JVM中只有一个唯一实例，所以可以直接用<code>==</code>比较：</p><pre><code class="hljs arcade"><span class="hljs-keyword">if</span> (day == <span class="hljs-built_in">Weekday</span>.FRI) &#123; <span class="hljs-comment">// ok!</span>&#125;<span class="hljs-keyword">if</span> (day.equals(<span class="hljs-built_in">Weekday</span>.SUN)) &#123; <span class="hljs-comment">// ok, but more code!</span>&#125;</code></pre><h3 id="enum类型"><a href="#enum类型" class="headerlink" title="enum类型"></a>enum类型</h3><p>通过<code>enum</code>定义的枚举类，和其他的<code>class</code>有什么区别？</p><p>答案是没有任何区别。<code>enum</code>定义的类型就是<code>class</code>，只不过它有以下几个特点：</p><ul><li>定义的<code>enum</code>类型总是继承自<code>java.lang.Enum</code>，且无法被继承；</li><li>只能定义出<code>enum</code>的实例，而无法通过<code>new</code>操作符创建<code>enum</code>的实例；</li><li>定义的每个实例都是引用类型的唯一实例；</li><li>可以将<code>enum</code>类型用于<code>switch</code>语句。</li></ul><p>例如，我们定义的<code>Color</code>枚举类：</p><pre><code class="hljs crystal">public <span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">Color</span> &#123;</span>    RED, GREEN, BLUE;&#125;</code></pre><p>编译器编译出的<code>class</code>大概就像这样：</p><pre><code class="hljs scala">public <span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Color</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Enum</span> </span>&#123; <span class="hljs-comment">// 继承自Enum，标记为final class</span>    <span class="hljs-comment">// 每个实例均为全局唯一:</span>    public static <span class="hljs-keyword">final</span> <span class="hljs-type">Color</span> <span class="hljs-type">RED</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Color</span>();    public static <span class="hljs-keyword">final</span> <span class="hljs-type">Color</span> <span class="hljs-type">GREEN</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Color</span>();    public static <span class="hljs-keyword">final</span> <span class="hljs-type">Color</span> <span class="hljs-type">BLUE</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Color</span>();    <span class="hljs-comment">// private构造方法，确保外部无法调用new操作符:</span>    <span class="hljs-keyword">private</span> <span class="hljs-type">Color</span>() &#123;&#125;&#125;</code></pre><p>所以，编译后的<code>enum</code>类和普通<code>class</code>并没有任何区别。但是我们自己无法按定义普通<code>class</code>那样来定义<code>enum</code>，必须使用<code>enum</code>关键字，这是Java语法规定的。</p><p>因为<code>enum</code>是一个<code>class</code>，每个枚举的值都是<code>class</code>实例，因此，这些实例有一些方法：</p><h4 id="name"><a href="#name" class="headerlink" title="name()"></a>name()</h4><p>返回常量名，例如：</p><pre><code class="hljs reasonml">String s = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Weekday</span>.</span><span class="hljs-module"><span class="hljs-identifier">SUN</span>.</span></span>name<span class="hljs-literal">()</span>; <span class="hljs-comment">// "SUN"</span></code></pre><h4 id="ordinal"><a href="#ordinal" class="headerlink" title="ordinal()"></a>ordinal()</h4><p>返回定义的常量的顺序，从0开始计数，例如：</p><pre><code class="hljs reasonml"><span class="hljs-built_in">int</span> n = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Weekday</span>.</span><span class="hljs-module"><span class="hljs-identifier">MON</span>.</span></span>ordinal<span class="hljs-literal">()</span>; <span class="hljs-comment">// 1</span></code></pre><p>改变枚举常量定义的顺序就会导致<code>ordinal()</code>返回值发生变化。例如：</p><pre><code class="hljs crystal">public <span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">Weekday</span> &#123;</span>    SUN, MON, TUE, WED, THU, FRI, SAT;&#125;</code></pre><p>和</p><pre><code class="hljs crystal">public <span class="hljs-class"><span class="hljs-keyword">enum</span> <span class="hljs-title">Weekday</span> &#123;</span>    MON, TUE, WED, THU, FRI, SAT, SUN;&#125;</code></pre><p>的<code>ordinal</code>就是不同的。如果在代码中编写了类似<code>if(x.ordinal()==1)</code>这样的语句，就要保证<code>enum</code>的枚举顺序不能变。新增的常量必须放在最后。</p><p>有些童鞋会想，<code>Weekday</code>的枚举常量如果要和<code>int</code>转换，使用<code>ordinal()</code>不是非常方便？比如这样写：</p><pre><code class="hljs reasonml">String task = <span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Weekday</span>.</span><span class="hljs-module"><span class="hljs-identifier">MON</span>.</span></span>ordinal<span class="hljs-literal">()</span> + <span class="hljs-string">"/ppt"</span>;save<span class="hljs-constructor">ToFile(<span class="hljs-params">task</span>)</span>;</code></pre><p>但是，如果不小心修改了枚举的顺序，编译器是无法检查出这种逻辑错误的。要编写健壮的代码，就不要依靠<code>ordinal()</code>的返回值。因为<code>enum</code>本身是<code>class</code>，所以我们可以定义<code>private</code>的构造方法，并且，给每个枚举常量添加字段：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Weekday day = Weekday.SUN;        <span class="hljs-keyword">if</span> (day.dayValue == <span class="hljs-number">6</span> || day.dayValue == <span class="hljs-number">0</span>) &#123;            System.out.println(<span class="hljs-string">"Work at home!"</span>);        &#125; <span class="hljs-keyword">else</span> &#123;            System.out.println(<span class="hljs-string">"Work at office!"</span>);        &#125;    &#125;&#125;<span class="hljs-keyword">enum</span> Weekday &#123;    MON(<span class="hljs-number">1</span>), TUE(<span class="hljs-number">2</span>), WED(<span class="hljs-number">3</span>), THU(<span class="hljs-number">4</span>), FRI(<span class="hljs-number">5</span>), SAT(<span class="hljs-number">6</span>), SUN(<span class="hljs-number">0</span>);    <span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> dayValue;    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-title">Weekday</span><span class="hljs-params">(<span class="hljs-keyword">int</span> dayValue)</span> </span>&#123;        <span class="hljs-keyword">this</span>.dayValue = dayValue;    &#125;&#125;</code></pre><p>这样就无需担心顺序的变化，新增枚举常量时，也需要指定一个<code>int</code>值。</p><p> 注意：枚举类的字段也可以是非final类型，即可以在运行期修改，但是不推荐这样做！</p><p>默认情况下，对枚举常量调用<code>toString()</code>会返回和<code>name()</code>一样的字符串。但是，<code>toString()</code>可以被覆写，而<code>name()</code>则不行。我们可以给<code>Weekday</code>添加<code>toString()</code>方法：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        Weekday day = Weekday.SUN;        <span class="hljs-keyword">if</span> (day.dayValue == <span class="hljs-number">6</span> || day.dayValue == <span class="hljs-number">0</span>) &#123;            System.out.<span class="hljs-built_in">println</span>(<span class="hljs-string">"Today is "</span> + day + <span class="hljs-string">". Work at home!"</span>);        &#125; <span class="hljs-keyword">else</span> &#123;            System.out.<span class="hljs-built_in">println</span>(<span class="hljs-string">"Today is "</span> + day + <span class="hljs-string">". Work at office!"</span>);        &#125;    &#125;&#125;<span class="hljs-keyword">enum</span> Weekday &#123;    MON(<span class="hljs-number">1</span>, <span class="hljs-string">"星期一"</span>), TUE(<span class="hljs-number">2</span>, <span class="hljs-string">"星期二"</span>), WED(<span class="hljs-number">3</span>, <span class="hljs-string">"星期三"</span>), THU(<span class="hljs-number">4</span>, <span class="hljs-string">"星期四"</span>), FRI(<span class="hljs-number">5</span>, <span class="hljs-string">"星期五"</span>), SAT(<span class="hljs-number">6</span>, <span class="hljs-string">"星期六"</span>), SUN(<span class="hljs-number">0</span>, <span class="hljs-string">"星期日"</span>);    <span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> dayValue;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">String</span> chinese;    <span class="hljs-function"><span class="hljs-keyword">private</span> <span class="hljs-title">Weekday</span><span class="hljs-params">(<span class="hljs-keyword">int</span> dayValue, <span class="hljs-keyword">String</span> chinese)</span> </span>&#123;        <span class="hljs-keyword">this</span>.dayValue = dayValue;        <span class="hljs-keyword">this</span>.chinese = chinese;    &#125;    @Override    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">String</span> <span class="hljs-title">toString</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.chinese;    &#125;&#125;</code></pre><p>覆写<code>toString()</code>的目的是在输出时更有可读性。</p><p> 注意：判断枚举常量的名字，要始终使用name()方法，绝不能调用toString()！</p><h3 id="switch"><a href="#switch" class="headerlink" title="switch"></a>switch</h3><p>最后，枚举类可以应用在<code>switch</code>语句中。因为枚举类天生具有类型信息和有限个枚举常量，所以比<code>int</code>、<code>String</code>类型更适合用在<code>switch</code>语句中：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Weekday day = Weekday.SUN;        <span class="hljs-keyword">switch</span>(day) &#123;        <span class="hljs-keyword">case</span> MON:        <span class="hljs-keyword">case</span> TUE:        <span class="hljs-keyword">case</span> WED:        <span class="hljs-keyword">case</span> THU:        <span class="hljs-keyword">case</span> FRI:            System.out.println(<span class="hljs-string">"Today is "</span> + day + <span class="hljs-string">". Work at office!"</span>);            <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">case</span> SAT:        <span class="hljs-keyword">case</span> SUN:            System.out.println(<span class="hljs-string">"Today is "</span> + day + <span class="hljs-string">". Work at home!"</span>);            <span class="hljs-keyword">break</span>;        <span class="hljs-keyword">default</span>:            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> RuntimeException(<span class="hljs-string">"cannot process "</span> + day);        &#125;    &#125;&#125;<span class="hljs-keyword">enum</span> Weekday &#123;    MON, TUE, WED, THU, FRI, SAT, SUN;&#125;</code></pre><p>加上<code>default</code>语句，可以在漏写某个枚举常量时自动报错，从而及时发现错误。</p><h3 id="小结-9"><a href="#小结-9" class="headerlink" title="小结"></a>小结</h3><p>Java使用<code>enum</code>定义枚举类型，它被编译器编译为<code>final class Xxx extends Enum { … }</code>；</p><p>通过<code>name()</code>获取常量定义的字符串，注意不要使用<code>toString()</code>；</p><p>通过<code>ordinal()</code>返回常量定义的顺序（无实质意义）；</p><p>可以为<code>enum</code>编写构造方法、字段和方法</p><p><code>enum</code>的构造方法要声明为<code>private</code>，字段强烈建议声明为<code>final</code>；</p><p><code>enum</code>适合用在<code>switch</code>语句中。</p><h2 id="7-记录类"><a href="#7-记录类" class="headerlink" title="7.记录类"></a>7.记录类</h2><p>使用<code>String</code>、<code>Integer</code>等类型的时候，这些类型都是不变类，一个不变类具有以下特点：</p><ol><li>定义class时使用<code>final</code>，无法派生子类；</li><li>每个字段使用<code>final</code>，保证创建实例后无法修改任何字段。</li></ol><p>假设我们希望定义一个<code>Point</code>类，有<code>x</code>、<code>y</code>两个变量，同时它是一个不变类，可以这么写：</p><pre><code class="hljs cpp"><span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Point</span> &#123;</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> x;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> y;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Point</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span> </span>&#123;        <span class="hljs-keyword">this</span>.x = x;        <span class="hljs-keyword">this</span>.y = y;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">x</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.x;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">y</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.y;    &#125;&#125;</code></pre><p>为了保证不变类的比较，还需要正确覆写<code>equals()</code>和<code>hashCode()</code>方法，这样才能在集合类中正常使用。后续我们会详细讲解正确覆写<code>equals()</code>和<code>hashCode()</code>，这里演示<code>Point</code>不变类的写法目的是，这些代码写起来都非常简单，但是很繁琐。</p><h3 id="record"><a href="#record" class="headerlink" title="record"></a>record</h3><p>从Java 14开始，引入了新的<code>Record</code>类。我们定义<code>Record</code>类时，使用关键字<code>record</code>。把上述<code>Point</code>类改写为<code>Record</code>类，代码如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Point p = <span class="hljs-keyword">new</span> Point(<span class="hljs-number">123</span>, <span class="hljs-number">456</span>);        System.out.println(p.x());        System.out.println(p.y());        System.out.println(p);    &#125;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> record <span class="hljs-title">Point</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span> </span>&#123;&#125;</code></pre><pre><code class="hljs angelscript">Note: Main.java uses preview language features. Note: Recompile with -Xlint:preview <span class="hljs-keyword">for</span> details. <span class="hljs-number">123</span> <span class="hljs-number">456</span> Point[x=<span class="hljs-number">123</span>, y=<span class="hljs-number">456</span>]</code></pre><p>仔细观察<code>Point</code>的定义：</p><pre><code class="hljs pgsql"><span class="hljs-built_in">public</span> <span class="hljs-type">record</span> <span class="hljs-type">Point</span>(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y) &#123;&#125;</code></pre><p>把上述定义改写为class，相当于以下代码：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Point</span> <span class="hljs-title">extends</span> <span class="hljs-title">Record</span> &#123;</span>    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> x;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-keyword">int</span> y;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-title">Point</span><span class="hljs-params">(<span class="hljs-keyword">int</span> x, <span class="hljs-keyword">int</span> y)</span> </span>&#123;        <span class="hljs-keyword">this</span>.x = x;        <span class="hljs-keyword">this</span>.y = y;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">x</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.x;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">y</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">this</span>.y;    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">String</span> <span class="hljs-title">toString</span><span class="hljs-params">()</span> </span>&#123;        <span class="hljs-keyword">return</span> <span class="hljs-keyword">String</span>.format(<span class="hljs-string">"Point[x=%s, y=%s]"</span>, x, y);    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">boolean</span> <span class="hljs-title">equals</span><span class="hljs-params">(Object o)</span> </span>&#123;        ...    &#125;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">int</span> <span class="hljs-title">hashCode</span><span class="hljs-params">()</span> </span>&#123;        ...    &#125;&#125;</code></pre><p>除了用<code>final</code>修饰class以及每个字段外，编译器还自动为我们创建了构造方法，和字段名同名的方法，以及覆写<code>toString()</code>、<code>equals()</code>和<code>hashCode()</code>方法。</p><p>换句话说，使用<code>record</code>关键字，可以一行写出一个不变类。</p><p>和<code>enum</code>类似，我们自己不能直接从<code>Record</code>派生，只能通过<code>record</code>关键字由编译器实现继承。</p><h3 id="构造方法"><a href="#构造方法" class="headerlink" title="构造方法"></a>构造方法</h3><p>编译器默认按照<code>record</code>声明的变量顺序自动创建一个构造方法，并在方法内给字段赋值。那么问题来了，如果我们要检查参数，应该怎么办？</p><p>假设<code>Point</code>类的<code>x</code>、<code>y</code>不允许负数，我们就得给<code>Point</code>的构造方法加上检查逻辑：</p><pre><code class="hljs pgsql"><span class="hljs-built_in">public</span> <span class="hljs-type">record</span> <span class="hljs-type">Point</span>(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y) &#123;    <span class="hljs-built_in">public</span> <span class="hljs-type">Point</span> &#123;        <span class="hljs-keyword">if</span> (x &lt; <span class="hljs-number">0</span> || y &lt; <span class="hljs-number">0</span>) &#123;            throw <span class="hljs-built_in">new</span> IllegalArgumentException();        &#125;    &#125;&#125;</code></pre><p>注意到方法<code>public Point {...}</code>被称为Compact Constructor，它的目的是让我们编写检查逻辑，编译器最终生成的构造方法如下：</p><pre><code class="hljs scala">public <span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Point</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Record</span> </span>&#123;    public <span class="hljs-type">Point</span>(int x, int y) &#123;        <span class="hljs-comment">// 这是我们编写的Compact Constructor:</span>        <span class="hljs-keyword">if</span> (x &lt; <span class="hljs-number">0</span> || y &lt; <span class="hljs-number">0</span>) &#123;            <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-type">IllegalArgumentException</span>();        &#125;        <span class="hljs-comment">// 这是编译器继续生成的赋值代码:</span>        <span class="hljs-keyword">this</span>.x = x;        <span class="hljs-keyword">this</span>.y = y;    &#125;    ...&#125;</code></pre><p>作为<code>record</code>的<code>Point</code>仍然可以添加静态方法。一种常用的静态方法是<code>of()</code>方法，用来创建<code>Point</code>：</p><pre><code class="hljs pgsql"><span class="hljs-built_in">public</span> <span class="hljs-type">record</span> <span class="hljs-type">Point</span>(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y) &#123;    <span class="hljs-built_in">public</span> static <span class="hljs-type">Point</span> <span class="hljs-keyword">of</span>() &#123;        <span class="hljs-keyword">return</span> <span class="hljs-built_in">new</span> <span class="hljs-type">Point</span>(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>);    &#125;    <span class="hljs-built_in">public</span> static <span class="hljs-type">Point</span> <span class="hljs-keyword">of</span>(<span class="hljs-type">int</span> x, <span class="hljs-type">int</span> y) &#123;        <span class="hljs-keyword">return</span> <span class="hljs-built_in">new</span> <span class="hljs-type">Point</span>(x, y);    &#125;&#125;</code></pre><p>这样我们可以写出更简洁的代码：</p><pre><code class="hljs arcade"><span class="hljs-keyword">var</span> z = <span class="hljs-built_in">Point</span>.of();<span class="hljs-keyword">var</span> p = <span class="hljs-built_in">Point</span>.of(<span class="hljs-number">123</span>, <span class="hljs-number">456</span>);</code></pre><h3 id="小结-10"><a href="#小结-10" class="headerlink" title="小结"></a>小结</h3><p>从Java 14开始，提供新的<code>record</code>关键字，可以非常方便地定义Data Class：</p><ul><li>使用<code>record</code>定义的是不变类；</li><li>可以编写Compact Constructor对参数进行验证；</li><li>可以定义静态方法。</li></ul><h2 id="8-BigInteger"><a href="#8-BigInteger" class="headerlink" title="8.BigInteger"></a>8.BigInteger</h2><p>在Java中，由CPU原生提供的整型最大范围是64位<code>long</code>型整数。使用<code>long</code>型整数可以直接通过CPU指令进行计算，速度非常快。</p><p>如果我们使用的整数范围超过了<code>long</code>型怎么办？这个时候，就只能用软件来模拟一个大整数。<code>java.math.BigInteger</code>就是用来表示任意大小的整数。<code>BigInteger</code>内部用一个<code>int[]</code>数组来模拟一个非常大的整数：</p><pre><code class="hljs reasonml">BigInteger bi = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigInteger(<span class="hljs-string">"1234567890"</span>)</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(bi.pow(<span class="hljs-number">5</span>)); <span class="hljs-comment">// 2867971860299718107233761438093672048294900000</span></code></pre><p>对<code>BigInteger</code>做运算的时候，只能使用实例方法，例如，加法运算：</p><pre><code class="hljs haxe">BigInteger i1 = <span class="hljs-keyword">new</span> <span class="hljs-type">BigInteger</span>(<span class="hljs-string">"1234567890"</span>);BigInteger i2 = <span class="hljs-keyword">new</span> <span class="hljs-type">BigInteger</span>(<span class="hljs-string">"12345678901234567890"</span>);BigInteger sum = i1.add(i2); <span class="hljs-comment">// 12345678902469135780</span></code></pre><p>和<code>long</code>型整数运算比，<code>BigInteger</code>不会有范围限制，但缺点是速度比较慢。</p><p>也可以把<code>BigInteger</code>转换成<code>long</code>型：</p><pre><code class="hljs reasonml">BigInteger i = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigInteger(<span class="hljs-string">"123456789000"</span>)</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(i.long<span class="hljs-constructor">Value()</span>); <span class="hljs-comment">// 123456789000</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(i.multiply(i).long<span class="hljs-constructor">ValueExact()</span>); <span class="hljs-comment">// java.lang.ArithmeticException: BigInteger out of long range</span></code></pre><p>使用<code>longValueExact()</code>方法时，如果超出了<code>long</code>型的范围，会抛出<code>ArithmeticException</code>。</p><p><code>BigInteger</code>和<code>Integer</code>、<code>Long</code>一样，也是不可变类，并且也继承自<code>Number</code>类。因为<code>Number</code>定义了转换为基本类型的几个方法：</p><ul><li>转换为<code>byte</code>：<code>byteValue()</code></li><li>转换为<code>short</code>：<code>shortValue()</code></li><li>转换为<code>int</code>：<code>intValue()</code></li><li>转换为<code>long</code>：<code>longValue()</code></li><li>转换为<code>float</code>：<code>floatValue()</code></li><li>转换为<code>double</code>：<code>doubleValue()</code></li></ul><p>因此，通过上述方法，可以把<code>BigInteger</code>转换成基本类型。如果<code>BigInteger</code>表示的范围超过了基本类型的范围，转换时将丢失高位信息，即结果不一定是准确的。如果需要准确地转换成基本类型，可以使用<code>intValueExact()</code>、<code>longValueExact()</code>等方法，在转换时如果超出范围，将直接抛出<code>ArithmeticException</code>异常。</p><p>如果<code>BigInteger</code>的值甚至超过了<code>float</code>的最大范围（3.4x1038），那么返回的float是什么呢？</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        BigInteger n = <span class="hljs-keyword">new</span> BigInteger(<span class="hljs-string">"999999"</span>).<span class="hljs-built_in">pow</span>(<span class="hljs-number">99</span>);        <span class="hljs-keyword">float</span> f = n.floatValue();        System.out.<span class="hljs-built_in">println</span>(f);<span class="hljs-comment">//Infinity</span>    &#125;&#125;</code></pre><h3 id="小结-11"><a href="#小结-11" class="headerlink" title="小结"></a>小结</h3><p><code>BigInteger</code>用于表示任意大小的整数；</p><p><code>BigInteger</code>是不变类，并且继承自<code>Number</code>；</p><p>将<code>BigInteger</code>转换成基本类型时可使用<code>longValueExact()</code>等方法保证结果准确。</p><h2 id="9-BigDecimal"><a href="#9-BigDecimal" class="headerlink" title="9.BigDecimal"></a>9.BigDecimal</h2><p>和<code>BigInteger</code>类似，<code>BigDecimal</code>可以表示一个任意大小且精度完全准确的浮点数。</p><pre><code class="hljs reasonml">BigDecimal bd = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"123.4567"</span>)</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(bd.multiply(bd)); <span class="hljs-comment">// 15241.55677489</span></code></pre><p><code>BigDecimal</code>用<code>scale()</code>表示小数位数，例如：</p><pre><code class="hljs reasonml">BigDecimal d1 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"123.45"</span>)</span>;BigDecimal d2 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"123.4500"</span>)</span>;BigDecimal d3 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"1234500"</span>)</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d1.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// 2,两位小数</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d2.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// 4</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d3.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// 0</span></code></pre><p>通过<code>BigDecimal</code>的<code>stripTrailingZeros()</code>方法，可以将一个<code>BigDecimal</code>格式化为一个相等的，但去掉了末尾0的<code>BigDecimal</code>：</p><pre><code class="hljs reasonml">BigDecimal d1 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"123.4500"</span>)</span>;BigDecimal d2 = d1.strip<span class="hljs-constructor">TrailingZeros()</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d1.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// 4</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d2.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// 2,因为去掉了00</span>BigDecimal d3 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"1234500"</span>)</span>;BigDecimal d4 = d3.strip<span class="hljs-constructor">TrailingZeros()</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d3.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// 0</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d4.scale<span class="hljs-literal">()</span>); <span class="hljs-comment">// -2</span></code></pre><p>如果一个<code>BigDecimal</code>的<code>scale()</code>返回负数，例如，<code>-2</code>，表示这个数是个整数，并且末尾有2个0。</p><p>可以对一个<code>BigDecimal</code>设置它的<code>scale</code>，如果精度比原始值低，那么按照指定的方法进行四舍五入或者直接截断：</p><pre><code class="hljs arduino"><span class="hljs-keyword">import</span> java.math.BigDecimal;<span class="hljs-keyword">import</span> java.math.RoundingMode;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        BigDecimal d1 = <span class="hljs-keyword">new</span> BigDecimal(<span class="hljs-string">"123.456789"</span>);        BigDecimal d2 = d1.setScale(<span class="hljs-number">4</span>, RoundingMode.HALF_UP); <span class="hljs-comment">// 四舍五入，123.4568</span>        BigDecimal d3 = d1.setScale(<span class="hljs-number">4</span>, RoundingMode.DOWN); <span class="hljs-comment">// 直接截断，123.4567</span>        System.out.<span class="hljs-built_in">println</span>(d2);        System.out.<span class="hljs-built_in">println</span>(d3);    &#125;&#125;</code></pre><p>对<code>BigDecimal</code>做加、减、乘时，精度不会丢失，但是做除法时，存在无法除尽的情况，这时，就必须指定精度以及如何进行截断：</p><pre><code class="hljs armasm"><span class="hljs-keyword">BigDecimal </span><span class="hljs-built_in">d1</span> = new <span class="hljs-keyword">BigDecimal("123.456");</span><span class="hljs-keyword">BigDecimal </span><span class="hljs-built_in">d2</span> = new <span class="hljs-keyword">BigDecimal("23.456789");</span><span class="hljs-keyword">BigDecimal </span><span class="hljs-built_in">d3</span> = <span class="hljs-built_in">d1</span>.divide(<span class="hljs-built_in">d2</span>, <span class="hljs-number">10</span>, RoundingMode.HALF_UP)<span class="hljs-comment">; // 保留10位小数并四舍五入</span><span class="hljs-keyword">BigDecimal </span><span class="hljs-built_in">d4</span> = <span class="hljs-built_in">d1</span>.divide(<span class="hljs-built_in">d2</span>)<span class="hljs-comment">; // 报错：ArithmeticException，因为除不尽</span></code></pre><p>还可以对<code>BigDecimal</code>做除法的同时求余数：</p><pre><code class="hljs arduino"><span class="hljs-keyword">import</span> java.math.BigDecimal;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        BigDecimal n = <span class="hljs-keyword">new</span> BigDecimal(<span class="hljs-string">"12.345"</span>);        BigDecimal m = <span class="hljs-keyword">new</span> BigDecimal(<span class="hljs-string">"0.12"</span>);        BigDecimal[] dr = n.divideAndRemainder(m);        System.out.<span class="hljs-built_in">println</span>(dr[<span class="hljs-number">0</span>]); <span class="hljs-comment">// 102</span>        System.out.<span class="hljs-built_in">println</span>(dr[<span class="hljs-number">1</span>]); <span class="hljs-comment">// 0.105</span>    &#125;&#125;</code></pre><p>调用<code>divideAndRemainder()</code>方法时，返回的数组包含两个<code>BigDecimal</code>，分别是商和余数，其中商总是整数，余数不会大于除数。我们可以利用这个方法判断两个<code>BigDecimal</code>是否是整数倍数：</p><pre><code class="hljs reasonml">BigDecimal n = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"12.75"</span>)</span>;BigDecimal m = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"0.15"</span>)</span>;BigDecimal<span class="hljs-literal">[]</span> dr = n.divide<span class="hljs-constructor">AndRemainder(<span class="hljs-params">m</span>)</span>;<span class="hljs-keyword">if</span> (dr<span class="hljs-literal">[<span class="hljs-number">1</span>]</span>.signum<span class="hljs-literal">()</span><span class="hljs-operator"> == </span><span class="hljs-number">0</span>) &#123;  <span class="hljs-comment">//signum:符号函数</span>    <span class="hljs-comment">// n是m的整数倍</span>&#125;</code></pre><h3 id="比较BigDecimal"><a href="#比较BigDecimal" class="headerlink" title="比较BigDecimal"></a>比较BigDecimal</h3><p>在比较两个<code>BigDecimal</code>的值是否相等时，要特别注意，使用<code>equals()</code>方法不但要求两个<code>BigDecimal</code>的值相等，还要求它们的<code>scale()</code>相等：</p><pre><code class="hljs reasonml">BigDecimal d1 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"123.456"</span>)</span>;BigDecimal d2 = <span class="hljs-keyword">new</span> <span class="hljs-constructor">BigDecimal(<span class="hljs-string">"123.45600"</span>)</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d1.equals(d2)); <span class="hljs-comment">// false,因为scale不同</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d1.equals(d2.strip<span class="hljs-constructor">TrailingZeros()</span>)); <span class="hljs-comment">// true,因为d2去除尾部0后scale变为2</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(d1.compare<span class="hljs-constructor">To(<span class="hljs-params">d2</span>)</span>); <span class="hljs-comment">// 0</span></code></pre><p>必须使用<code>compareTo()</code>方法来比较，它根据两个值的大小分别返回负数、正数和<code>0</code>，分别表示小于、大于和等于。</p><p> <strong>总是使用compareTo()比较两个BigDecimal的值，不要使用equals()！</strong></p><p>如果查看<code>BigDecimal</code>的源码，可以发现，实际上一个<code>BigDecimal</code>是通过一个<code>BigInteger</code>和一个<code>scale</code>来表示的，即<code>BigInteger</code>表示一个完整的整数，而<code>scale</code>表示小数位数：</p><pre><code class="hljs scala">public <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BigDecimal</span> <span class="hljs-keyword">extends</span> <span class="hljs-title">Number</span> <span class="hljs-title">implements</span> <span class="hljs-title">Comparable&lt;BigDecimal&gt;</span> </span>&#123;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> <span class="hljs-type">BigInteger</span> intVal;    <span class="hljs-keyword">private</span> <span class="hljs-keyword">final</span> int scale;&#125;</code></pre><p><code>BigDecimal</code>也是从<code>Number</code>继承的，也是不可变对象。</p><h3 id="小结-12"><a href="#小结-12" class="headerlink" title="小结"></a>小结</h3><p><code>BigDecimal</code>用于表示精确的小数，常用于财务计算；</p><p>比较<code>BigDecimal</code>的值是否相等，必须使用<code>compareTo()</code>而不能使用<code>equals()</code>。</p><h2 id="10-常用工具类"><a href="#10-常用工具类" class="headerlink" title="10.常用工具类"></a>10.常用工具类</h2><p>Java的核心库提供了大量的现成的类供我们使用。本节我们介绍几个常用的工具类。</p><h3 id="Math"><a href="#Math" class="headerlink" title="Math"></a>Math</h3><p>顾名思义，<code>Math</code>类就是用来进行数学计算的，它提供了大量的静态方法来便于我们实现数学计算：</p><p>求绝对值：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>abs<span class="hljs-number">(-100)</span>; <span class="hljs-comment">// 100</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>abs<span class="hljs-number">(-7.8)</span>; <span class="hljs-comment">// 7.8</span></code></pre><p>取最大或最小值：</p><pre><code class="hljs angelscript">Math.max(<span class="hljs-number">100</span>, <span class="hljs-number">99</span>); <span class="hljs-comment">// 100</span>Math.min(<span class="hljs-number">1.2</span>, <span class="hljs-number">2.3</span>); <span class="hljs-comment">// 1.2</span></code></pre><p>计算xy次方：</p><pre><code class="hljs angelscript">Math.pow(<span class="hljs-number">2</span>, <span class="hljs-number">10</span>); <span class="hljs-comment">// 2的10次方=1024</span></code></pre><p>计算√x：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>sqrt(<span class="hljs-number">2</span>); <span class="hljs-comment">// 1.414...</span></code></pre><p>计算ex次方：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>exp(<span class="hljs-number">2</span>); <span class="hljs-comment">// 7.389...</span></code></pre><p>计算以e为底的对数：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>log(<span class="hljs-number">4</span>); <span class="hljs-comment">// 1.386...</span></code></pre><p>计算以10为底的对数：</p><pre><code class="hljs gcode">Math.lo<span class="hljs-name">g10</span><span class="hljs-comment">(100)</span>; <span class="hljs-comment">// 2</span></code></pre><p>三角函数：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>sin(<span class="hljs-number">3.14</span>); <span class="hljs-comment">// 0.00159...</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>cos(<span class="hljs-number">3.14</span>); <span class="hljs-comment">// -0.9999...</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>tan(<span class="hljs-number">3.14</span>); <span class="hljs-comment">// -0.0015...</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>asin(<span class="hljs-number">1.0</span>); <span class="hljs-comment">// 1.57079...</span><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>acos(<span class="hljs-number">1.0</span>); <span class="hljs-comment">// 0.0</span></code></pre><p>Math还提供了几个数学常量：</p><pre><code class="hljs qml"><span class="hljs-built_in">double</span> pi = <span class="hljs-built_in">Math</span>.PI; <span class="hljs-comment">// 3.14159...</span><span class="hljs-built_in">double</span> e = <span class="hljs-built_in">Math</span>.E; <span class="hljs-comment">// 2.7182818...</span><span class="hljs-built_in">Math</span>.sin(<span class="hljs-built_in">Math</span>.PI / <span class="hljs-number">6</span>); <span class="hljs-comment">// sin(π/6) = 0.5</span></code></pre><p>生成一个随机数x，x的范围是<code>0 &lt;= x &lt; 1</code>：</p><pre><code class="hljs reasonml"><span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">Math</span>.</span></span>random<span class="hljs-literal">()</span>; <span class="hljs-comment">// 0.53907... 每次都不一样</span></code></pre><p>如果我们要生成一个区间在<code>[MIN, MAX)</code>的随机数，可以借助<code>Math.random()</code>实现，计算如下：</p><pre><code class="hljs arduino"><span class="hljs-comment">// 区间在[MIN, MAX)的随机数</span><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        <span class="hljs-keyword">double</span> x = Math.<span class="hljs-built_in">random</span>(); <span class="hljs-comment">// x的范围是[0,1)</span>        <span class="hljs-keyword">double</span> <span class="hljs-built_in">min</span> = <span class="hljs-number">10</span>;        <span class="hljs-keyword">double</span> <span class="hljs-built_in">max</span> = <span class="hljs-number">50</span>;        <span class="hljs-keyword">double</span> y = x * (<span class="hljs-built_in">max</span> - <span class="hljs-built_in">min</span>) + <span class="hljs-built_in">min</span>; <span class="hljs-comment">// y的范围是[10,50)</span>        <span class="hljs-keyword">long</span> n = (<span class="hljs-keyword">long</span>) y; <span class="hljs-comment">// n的范围是[10,50)的整数（长整型）</span>        System.out.<span class="hljs-built_in">println</span>(y);        System.out.<span class="hljs-built_in">println</span>(n);    &#125;&#125;</code></pre><pre><code class="hljs angelscript"><span class="hljs-number">11.58634888840088</span> <span class="hljs-number">11</span></code></pre><p>有些童鞋可能注意到Java标准库还提供了一个<code>StrictMath</code>，它提供了和<code>Math</code>几乎一模一样的方法。这两个类的区别在于，由于浮点数计算存在误差，不同的平台（例如x86和ARM）计算的结果可能不一致（指误差不同），因此，<code>StrictMath</code>保证所有平台计算结果都是完全相同的，而<code>Math</code>会尽量针对平台优化计算速度，所以，绝大多数情况下，使用<code>Math</code>就足够了。</p><h3 id="Random"><a href="#Random" class="headerlink" title="Random"></a>Random</h3><p><code>Random</code>用来创建伪随机数。所谓伪随机数，是指只要给定一个初始的种子，产生的随机数序列是完全一样的。</p><p>要生成一个随机数，可以使用<code>nextInt()</code>、<code>nextLong()</code>、<code>nextFloat()</code>、<code>nextDouble()</code>：</p><pre><code class="hljs reasonml">Random r = <span class="hljs-keyword">new</span> <span class="hljs-constructor">Random()</span>;r.next<span class="hljs-constructor">Int()</span>; <span class="hljs-comment">// 2071575453,每次都不一样</span>r.next<span class="hljs-constructor">Int(10)</span>; <span class="hljs-comment">// 5,生成一个[0,10)之间的int</span>r.next<span class="hljs-constructor">Long()</span>; <span class="hljs-comment">// 8811649292570369305,每次都不一样</span>r.next<span class="hljs-constructor">Float()</span>; <span class="hljs-comment">// 0.54335...生成一个[0,1)之间的float</span>r.next<span class="hljs-constructor">Double()</span>; <span class="hljs-comment">// 0.3716...生成一个[0,1)之间的double</span></code></pre><p>有童鞋问，每次运行程序，生成的随机数都是不同的，没看出<em>伪随机数</em>的特性来。</p><p>这是因为我们创建<code>Random</code>实例时，如果不给定种子，就使用系统当前时间戳作为种子，因此每次运行时，种子不同，得到的伪随机数序列就不同。</p><p>如果我们在创建<code>Random</code>实例时指定一个种子，就会得到完全确定的随机数序列：</p><pre><code class="hljs angelscript"><span class="hljs-keyword">import</span> java.util.Random;<span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-symbol">Main</span> &#123;    <span class="hljs-keyword">public</span> static <span class="hljs-built_in">void</span> main(String[] args) &#123;        Random r = new Random(<span class="hljs-number">12345</span>);        <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> i = <span class="hljs-number">0</span>; i &lt; <span class="hljs-number">10</span>; i++) &#123;            System.<span class="hljs-keyword">out</span>.println(r.nextInt(<span class="hljs-number">100</span>));        &#125;        <span class="hljs-comment">// 51, 80, 41, 28, 55...</span>    &#125;&#125;</code></pre><p>前面我们使用的<code>Math.random()</code>实际上内部调用了<code>Random</code>类，所以它也是伪随机数，只是我们无法指定种子。</p><h3 id="SecureRandom"><a href="#SecureRandom" class="headerlink" title="SecureRandom"></a>SecureRandom</h3><p>有伪随机数，就有真随机数。实际上真正的真随机数只能通过量子力学原理来获取，而我们想要的是一个不可预测的安全的随机数，<code>SecureRandom</code>就是用来创建安全的随机数的：</p><pre><code class="hljs reasonml">SecureRandom sr = <span class="hljs-keyword">new</span> <span class="hljs-constructor">SecureRandom()</span>;<span class="hljs-module-access"><span class="hljs-module"><span class="hljs-identifier">System</span>.</span></span>out.println(sr.next<span class="hljs-constructor">Int(100)</span>);</code></pre><p><code>SecureRandom</code>无法指定种子，它使用RNG（random number generator）算法。JDK的<code>SecureRandom</code>实际上有多种不同的底层实现，有的使用安全随机种子加上伪随机数算法来产生安全的随机数，有的使用真正的随机数生成器。实际使用的时候，可以优先获取高强度的安全随机数生成器，如果没有提供，再使用普通等级的安全随机数生成器：</p><pre><code class="hljs arduino"><span class="hljs-keyword">import</span> java.util.Arrays;<span class="hljs-keyword">import</span> java.security.SecureRandom;<span class="hljs-keyword">import</span> java.security.NoSuchAlgorithmException;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        SecureRandom sr = null;        <span class="hljs-keyword">try</span> &#123;            sr = SecureRandom.getInstanceStrong(); <span class="hljs-comment">// 获取高强度安全随机数生成器</span>        &#125; <span class="hljs-keyword">catch</span> (NoSuchAlgorithmException e) &#123;            sr = <span class="hljs-keyword">new</span> SecureRandom(); <span class="hljs-comment">// 获取普通的安全随机数生成器</span>        &#125;        <span class="hljs-keyword">byte</span>[] <span class="hljs-built_in">buffer</span> = <span class="hljs-keyword">new</span> <span class="hljs-keyword">byte</span>[<span class="hljs-number">16</span>];        sr.nextBytes(<span class="hljs-built_in">buffer</span>); <span class="hljs-comment">// 用安全随机数填充buffer</span>        System.out.<span class="hljs-built_in">println</span>(Arrays.toString(<span class="hljs-built_in">buffer</span>));    &#125;&#125;</code></pre><p><code>SecureRandom</code>的安全性是通过操作系统提供的安全的随机种子来生成随机数。这个种子是通过CPU的热噪声、读写磁盘的字节、网络流量等各种随机事件产生的“熵”。</p><p>在密码学中，安全的随机数非常重要。如果使用不安全的伪随机数，所有加密体系都将被攻破。因此，时刻牢记必须使用<code>SecureRandom</code>来产生安全的随机数。</p><p> 需要使用安全随机数的时候，必须使用SecureRandom，绝不能使用Random！</p><h3 id="小结-13"><a href="#小结-13" class="headerlink" title="小结"></a>小结</h3><p>Java提供的常用工具类有：</p><ul><li>Math：数学计算</li><li>Random：生成伪随机数</li><li>SecureRandom：生成安全的随机数</li></ul>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（11.27)</title>
    <link href="/2020/11/27/11-27/"/>
    <url>/2020/11/27/11-27/</url>
    
    <content type="html"><![CDATA[<p>最近主要阅读了三篇视频目标检测方向的论文，分别是STSN(ECCV2018)、STMN(ECCV2018)、TCENet(AAAI2020)。</p><p><strong>阅读文献：</strong></p><p>[1] Bertasius G, Torresani L, Shi J. Object detection in video with spatiotemporal sampling networks[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 331-346.</p><p>[2] Xiao F, Jae Lee Y. Video object detection with an aligned spatial-temporal memory[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 485-501.</p><p>[3] He F, Gao N, Li Q, et al. Temporal Context Enhanced Feature Aggregation for Video Object Detection[C]//AAAI. 2020: 10941-10948.</p><h1 id="1-STSN"><a href="#1-STSN" class="headerlink" title="1. STSN"></a>1. STSN</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>时空采样网络（STSN）使用跨时间的可变形卷积来检测视频中的目标。通过从相邻帧中进行空间采样来在一个视频帧检测目标。这自然使该方法对于单个帧中的遮挡或运动模糊具有鲁棒性。该框架不需要额外的监督，因为它直接针对目标检测性能优化采样位置。该方法在STSN上取得SOTA效果，对比之前的视频目标检测方法，设计更简单，训练中不需要光流数据。</p><p>此前的后处理方法旨在使目标检测在时间上连贯，然而，因为时间一致性被强制在第二个步骤，使得这些方法不能被端到端的训练。本文提出了一个简单高效的时空采样网络STSN，在空间和时间上使用变形卷积，来利用时间信息进行视频目标检测。STSN学会从临近的视频帧中对有用的特征点进行空间采样，以使给定视频帧中的目标检测精度达到最大。</p><p>与标准的变形CNN（其在空间域上执行变形卷积）不同，STSN学会了在不用的视频帧之间采样特征，提高了视频目标检测的精度。预训练的光流CNN不总是在新的数据集上通用，这可能阻碍视频目标检测的性能。相比之下，本文的方法有一个可学习的时空采样模块，该模块可以从目标检测标签中进行有区别的训练，没有以上问题。</p><p>STSN可以在一步中进行端到端的训练。相比之下，依赖于光流的方法需要一个额外的步骤来训练一个光流CNN，这使得训练过程更加繁琐和冗长。例如，从头开始训练FGFA [24]的光流CNN大约需要四天，然后再训练FGFA [24]进行视频目标检测又需要四天，因此它需要八天的总训练时间。相比之下，STSN只需4天就可以训练一个阶段。最后， STSN在视频目标检测的准确性上也取得了一定的增长，尽管是中等的。</p><h2 id="背景：变形卷积"><a href="#背景：变形卷积" class="headerlink" title="背景：变形卷积"></a>背景：变形卷积</h2><p>标准的2维卷积由两步组成：</p><ol><li><p>在一个等距的空间网格R上采样位置。</p></li><li><p>使用权重w对采样到的值执行一个加权和。</p></li></ol><p>例如，一个核大小为3x3的标准2D卷积，膨胀因子为1，网格R被定义为：{(-1,-1),(-1,0),…,(0,1),(1,1)}。此时，在输出特征图y的p0位置的新值为：                               <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qpzylckj30lq046myr.jpg" srcset="/img/loading.gif" alt="image-20201127150050204" style="zoom:33%;" /></p><p>其中x为输入特征图。在变形2D卷积中，网格R用一个数据条件偏移增强，</p><p>   <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qqc37fbj30q806ajui.jpg" srcset="/img/loading.gif" alt="image-20201127150109591" style="zoom:33%;" /></p><p>∆pn通常是分数，以上操作是使用双线性插值实现的。注意，偏移是通过将单独的卷积层用于包含特征图x的激活张量而获得的。这将产生具有与输入要素图相同的空间分辨率的偏移图。另外，请注意，偏移量在给定激活张量的所有特征通道之间共享。在训练过程中，可变形卷积核和偏移核的权重是通过使用双线性插值算子传播梯度来共同学习的。</p><h2 id="时空采样网络"><a href="#时空采样网络" class="headerlink" title="时空采样网络"></a>时空采样网络</h2><p>作者的目标是设计一种网络架构，该架构结合了用于视频中对象检测的时间信息。</p><p>系统将视频中的每个帧依次视为参考帧，以便在视频的每个帧中进行目标检测。在实验中使用2K个支持帧来检测参考帧，（前面K帧，后面K帧）</p><p>为了高效的合并时间信息，需要两点：</p><ol><li><p>来自图像级别网络的强大的目标级特征。（通过backbone 网络实现）</p></li><li><p>从支持帧中为参考帧采样有用的目标级别特征的能力。（通过设计时空采样框架实现）</p></li></ol><h2 id="STSN可被总结为4步："><a href="#STSN可被总结为4步：" class="headerlink" title="STSN可被总结为4步："></a>STSN可被总结为4步：</h2><ol><li><p>使用backbone卷积网络为每个视频帧独立的计算目标级别的特征。</p></li><li><p>时空采样块被应用于目标级别的特征，来从相邻帧采样相关特征（在输入参考帧的条件下）。</p></li><li><p>使用每个像素的加权求和将每个视频帧的采样特征在时间上聚合为参考帧的单个特征张量。</p></li><li><p>特征向量输入到检测网络来产生参考帧最终的的检测结果。</p></li></ol><p>STSN的框架将这些在概念上截然不同的四个步骤集成到一个体系结构中，对其进行了端到端的训练。</p><h2 id="Backbone-Architecture"><a href="#Backbone-Architecture" class="headerlink" title="Backbone Architecture"></a>Backbone Architecture</h2><p>采用基于ResNet-101的变形CNN，采用6个可变形卷积层。</p><h2 id="时空特征采样"><a href="#时空特征采样" class="headerlink" title="时空特征采样"></a>时空特征采样</h2><p>时空采样机制是本文的主要贡献，它可以在给出的视频中无缝的合并时间信息。第一步，将参考帧It和支持帧It+k喂给图像级别的backbone网络，分别产生特征张量ft和ft+k。</p><p>  <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qr2d8glj30du02m3za.jpg" srcset="/img/loading.gif" alt="image-20201127150152330" style="zoom: 25%;" />其中c,h,w是激活张量的通道数、高度、宽度。然后将ft和ft+k串联为一个新的特征张量<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qrdye6cj30gq032jsk.jpg" srcset="/img/loading.gif" alt="image-20201127150210506" style="zoom:25%;" />注意，这个张量是初始张量的通道数的两倍。其包含了参考帧和支持帧的目标级别的信息。</p><p>接下来，使用ft,t+k来预测(x,y)位置偏移量，这将被用于采样支持张量ft+k。采样机制使用一个变形卷积层来实现，输入有两个：预测到的偏移量和支持张量ft+k，输出一个新的采样过的特征张量gt,t+k，其可被用于在参考帧进行目标检测。使用下标t，t+k来表示重采样的张量，尽管g是通过重采样支持帧来获得，但是偏移量的计算同时使用参考帧和支持帧。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qrs5mwmj310c0k0qlp.jpg" srcset="/img/loading.gif" alt="image-20201127150233081" style="zoom:50%;" /></p><blockquote><p>图：STSN框架。给出参考帧，目标是从相邻的支持帧合并信息。首先从支持帧和参考帧使用backbone提取特征。之后将参考帧和支持帧的特征串联，喂给多个可变形卷积层。其将产生偏移量，用于从支持帧采样有信息的特征。STSN体系允许在参考帧出现模糊或遮挡的情况下，执行精确的检测。</p></blockquote><h1 id="2-STMN"><a href="#2-STMN" class="headerlink" title="2.STMN"></a>2.STMN</h1><p>本文介绍了一个用于视频目标检测的时空记忆网络(STMN)，其核心是时空记忆模块（STMM）作为循环计算单元，以对长时间表观和运动变化建模。STMM的设计允许完全集成从静态图像预训练的CNNbackbone权重，这对于精确的检测十分重要。此外，提出了一个新颖的MatchTrans模块来将帧与帧之间的时空memory对齐。</p><p>本文的方法能在一个较长并且数量可变的帧上捕捉信息，而现有方法仅能捕捉短而固定长度帧的信息。本文的方法仅仅需要计算单个frame-level的空间memory，计算独立于proposal的数量。</p><p>Detect and Track方法旨在统一检测和跟踪，两个连续帧间的关系被用于预测检测框的位移。与之不同，本文的时空记忆模块，聚合t&gt;2帧的信息。并且使用所提出的MatchTrans模块来对整个特征图进行对齐，而不是预测框的位移。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qsdalvpj314u0puqt5.jpg" srcset="/img/loading.gif" alt="image-20201127150306794" style="zoom:50%;" /></p><h2 id="时空memory对齐的必要性："><a href="#时空memory对齐的必要性：" class="headerlink" title="时空memory对齐的必要性："></a>时空memory对齐的必要性：</h2><p>如果不对齐，时空memory将不会遗忘一个目标，即使它的位置已经改变。例如第四行的显著性图，由于多个没有对齐的特征图重叠在一起，产生的幻觉特征将导致误警检测、不精确的定位（第三行）。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qsnt7f3j311e0s0kjl.jpg" srcset="/img/loading.gif" alt="image-20201127150322942" style="zoom:50%;" /></p><p>为了减轻这一问题，作者提出了MatchTrans模块来对时空memory进行跨帧对齐，对于一个在位置（x,y)的特征cell Ft(x,y)∈1x1xD，MatchTrans计算Ft（x,y)与Ft-1中小的相邻区域之间的亲和力，以将Mt-1的时空memory转换对齐到第t帧：</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qsyni71j316k05a41r.jpg" srcset="/img/loading.gif" alt="image-20201127150340883" style="zoom: 33%;" /></p><p>用T将非对齐的memory Mt-1对齐到M’t-1:</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qt5k4spj312s054djg.jpg" srcset="/img/loading.gif" alt="image-20201127150352341" style="zoom:33%;" /></p><blockquote><p>自己的理解：先计算出一个当前帧t的(x,y)位置与t-1帧的一个小区域的变换系数。使用这个系数，将t-1帧的Memory的一个小区域，对齐到M’的(x,y)位置</p></blockquote><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qts6x6rj319k0gyasz.jpg" srcset="/img/loading.gif" alt="image-20201127150428748"></p><p>直觉是：使用给出的变换T，重构空间memory M’t-1(x,y)为在Mt-1与(x,y)相邻的(2k+1 X 2k+1)小邻域内的加权均值。可以用得到的M’t-1简单的替代等式1-4中的Mt-1。</p><p>通过合适的对齐之后，生成的memory更干净(如图4第二行所示)，可以更精确的检测。因为计算代价是k的平方，实验中将k设为2。</p><p>MatchTrans模块相比使用光流，更高效，节省了用于存储光流的空间。</p><h1 id="3-TCENet"><a href="#3-TCENet" class="headerlink" title="3.TCENet"></a>3.TCENet</h1><p>  为解决视频表观退化问题，现有的一些方法聚合相邻帧的特征来增强每一帧的表观特征。然而这些方法忽略了聚合帧之间的时间关系，这对于提升识别精度十分重要。作者提出了一个“时间上下文增强网络”——TCENet来通过时间聚合利用时间上下文信息。为了处理目标视频中目标位移问题，作者提出了一个DeformAlign模块来对齐帧与帧之间的空间特征。此外，作者没有采用固定的融合窗口长度，提出了一个temporal stride predictor来自适应的选择用于聚合的视频帧，有利于利用可变的时间信息，用更少的视频帧进行聚合来实现更好的结果。</p><p>  一些方法使用固定的相邻帧长度来增强参考帧的表观特征，作者将其称为增强空间特征的聚合(SFEA)，这些方法忽略了用于聚合的帧的时间关系。特别是当打乱用于聚合的帧的顺序时，这些方法的性能几乎不受影响，也就是说这些模型没有从时间上下文中受益。仅执行外观特征表示，可能难以识别一些外观严重恶化的目标，如罕见的姿势、部分遮挡。</p><p>  本文的哲学是相邻帧间的时间上下文信息在视频目标检测中扮演重要的角色。本文提出的TCEA方法，聚合相邻帧的特征来对时间上下文建模，以增强参考帧的特征。具体来说，对于每个参考帧，根据<strong>attention weights</strong>和<strong>temporal order</strong>聚合相邻帧的特征和参考帧的特征。</p><h2 id="TCENet框架"><a href="#TCENet框架" class="headerlink" title="TCENet框架"></a>TCENet框架</h2><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qu3ep0cj310k0n6tp1.jpg" srcset="/img/loading.gif" alt="image-20201127150446536" style="zoom:50%;" /></p><p>TCENet基于R-FCN构建，在每个时间t，TCENet将参考帧t与t-s(t)和t+s(t)聚合，其中s(t)通过 temporal stride predictor计算。特征提取器Nfeat产生这三帧图像的中间特征ft-s(t)、ft、ft+s(t)。在进行特征聚合之前，首先使用DeformAlign模块，将前后两张特征图的空间特征对齐到ft，生成ft-s(t)-&gt;t和ft+s(t)-&gt;t。之后使用聚合模块TCEA将三张特征图聚合生成gt。最终将合并过的特征gt喂给检测网络，获得参考帧的检测结果。</p><h2 id="Temporal-Context-Enhanced-Aggregation-TCEA"><a href="#Temporal-Context-Enhanced-Aggregation-TCEA" class="headerlink" title="Temporal Context Enhanced Aggregation(TCEA)"></a>Temporal Context Enhanced Aggregation(TCEA)</h2><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qub6oitj311k0fyarh.jpg" srcset="/img/loading.gif" alt="image-20201127150459481"></p><p>  如上图中间所示，<strong>时间融合</strong>被提出，用于聚合相邻帧的特征来对时间上下文建模。来自N个相邻帧的，尺寸为CxHxW的特征首先被串联在一起，合并事件信息，形成一个NC通道的特征图。与常见的表观特征不同，这里包含了一个额外的时间维度。之后，使用一个1x1的卷积核对串联的特征图进行卷积，捕捉帧间的时间关系。最终生成一个C通道的特征图，同时保留了时间和空间信息。</p><p>  此外，TCEA也采用了增加了时空注意力模块，来为每一帧分配pixel-level的聚合权重。<strong>时间注意力</strong>的目标是：在一个嵌入空间中计算帧的相似性，来关注<strong>“何时”</strong>的相邻帧重要。直觉上，在位置p，如果对齐过的特征fi-&gt;t(p)与ft(p)越接近，就越该被关注。这里作者采用点积相似性度量来度量相似性。与时间注意力木桶，<strong>空间注意力</strong>关注<strong>“哪里”</strong>的信息最丰富。空间注意力图从时间融合后的特征图计算。为了突出信息区域并增加注意力感受野，首先将平均池化和最大池化应用于融合后的特征，获得两张半分辨率的特征图。然后将它们连接起来以生成特征描述符，并在描述符上应用卷积层以生成中间特征图。中间特征图使用双线性插值上采样生成一张空间注意力图fs。之后使用公式(2)将其与ft融合，空间注意力为特征提供了一个细粒度的控制，即哪些地方应该被强调，哪些地方应该被抑制。</p><h2 id="DeformAlign-feature-alignment"><a href="#DeformAlign-feature-alignment" class="headerlink" title="DeformAlign feature alignment"></a>DeformAlign feature alignment</h2><p>  相同目标实例的特征，由于视频的运动，通常没有在帧与帧之间空间对齐。如果在进行特征聚合之前，没有进行恰当的特征对齐，目标检测器可能会发生检测识别、定位不准的问题。因此，作者提出了DeformAlign模块，采用变形卷积，实现精确的像素级别的空间对齐。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3quo1s24j30ta0fwdnz.jpg" srcset="/img/loading.gif" alt="image-20201127150519561" style="zoom:50%;" /></p><p>  为了将第i帧的特征图与参考帧t对齐，DeformAlign模块首先使用fi和ft作为输入，预测采样参数θ，使用采样参数θ和fi，采用变形卷积计算出fi-&gt;t。</p><h2 id="Temporal-Stride-Predictor"><a href="#Temporal-Stride-Predictor" class="headerlink" title="Temporal Stride Predictor"></a>Temporal Stride Predictor</h2><p>  为了获得更丰富的信息，一些模型基于固定的滑动窗口长度，聚合较长的输入视频帧特征。使用大卷积核，可以在特征图上获得更大的空间感受野。然而，参考帧的时间邻域主要包括冗余信息，并且对于提升当前目标的信念几乎是无用的。此外，巨大的滑动窗口时间长度计算开销更大。</p><p>  作者发现增加聚合帧之间的时间步长，能增加时间感受野，并且能聚合更多有用的信息，不增加计算量。这里的时间步长定义为s=|t2-t1|。低级的时间步长选取策略在每一个参考帧t，使用固定的时间步长，使得建模变化的时间信息变的困难。更好的时间步长选取策略应该是能适应时间阈的变化。</p><p>  一个自然地判断时间步长的标准是根据视频内容的变化速度。如果内容变化速度很快，就选择一个更小的时间步长，合并更近的帧。相反，如果变化速度慢，就选择更长的时间步长，合并更远的帧。视频内容的速度可以用ground truth目标的变化速度来度量。目标的运动速度通过其与相邻帧（±10帧）对应实例的IoU分数度量。这个指标被定义为“motion IoU”，motion IoU越小，目标运动越快。</p><p>  基于此，作者设计了Temporal Stride Predictor,来为参考帧t选择哪些帧用于聚合。该预测器将特征t和特征k之间的差异作为输入（如ft-fk），预测第t帧和第k帧之间的偏差分数（即motion IoU）。若motion IoU<0.7（说明目标运动快）就将当前参考帧设为快时间步长（默认为9）。若0.7≤motion IoU≤0.9（说明目标运动适中）就将当前参考帧设为中时间步长（默认为24）。若motion IoU>0.9（说明目标运动慢）就将当前参考帧设慢快时间步长（默认为38）。</p><p>  该预测网络由：2个卷积层（核大小为3x3,256通道），全局池化层，全连接层，sigmoid函数组成。在运行时，ft和ft-10被喂给该网络，来预测参考帧t的运动速度。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>创新点：</p><ul><li>提出了TCENet来通过时间聚合利用时间上下文信息。</li><li>提出了TCEA，聚合相邻帧的特征来对时间上下文建模，以增强参考帧的特征，更有效的处理表观退化问题。</li><li>提出了temporal tride predictor，来自适应的选择用于聚合的视频帧，有利于利用可变的时间信息，用更少的视频帧进行聚合来实现更好的结果。</li><li>提出了DeformAlign模块，对由帧间运动引发的位移建模，实现精确的像素级空间特征对齐。</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>STSN</tag>
      
      <tag>STMN</tag>
      
      <tag>TCENet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（11.20)</title>
    <link href="/2020/11/20/11-20/"/>
    <url>/2020/11/20/11-20/</url>
    
    <content type="html"><![CDATA[<p>最近一周主要集中精力在CVPR2020上发表的MEGA视频目标检测方法上，</p><ul><li>认真研读了MEGA论文，做了详细的阅读笔记。</li><li>运行了MEGA的demo代码和test代码，整理了相关笔记。</li></ul><p><strong>参考文献</strong>：Chen Y, Cao Y, Hu H, et al. Memory Enhanced Global-Local Aggregation for Video Object Detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10337-10346.</p><h1 id="1-MEGA论文阅读笔记"><a href="#1-MEGA论文阅读笔记" class="headerlink" title="1.MEGA论文阅读笔记"></a>1.MEGA论文阅读笔记</h1><ul><li>论文地址：</li></ul><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Memory_Enhanced_Global-Local_Aggregation_for_Video_Object_Detection_CVPR_2020_paper.pdf" target="_blank" rel="noopener">https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Memory_Enhanced_Global-Local_Aggregation_for_Video_Object_Detection_CVPR_2020_paper.pdf</a></p><ul><li>代码地址：<a href="https://github.com/Scalsol/mega.pytorch" target="_blank" rel="noopener">https://github.com/Scalsol/mega.pytorch</a></li></ul><p>作者提出的Memory Enhanced Global-Local Aggregation（MEGA）首次同时考虑<strong>全局语义信息</strong>和局部<strong>定位信息</strong>。此外通过使用Long Range Memory(LRM)模块，MEGA使得关键帧能访问更多的内容。</p><h2 id="全局语义信息："><a href="#全局语义信息：" class="headerlink" title="全局语义信息："></a>全局语义信息：</h2><p>  当单帧遇到诸如运动模糊、遮挡、失焦等问题时，人们自然会想到从整个视频中查找线索。当人们不确定目标的身份时，他们将寻求从其他帧中找到与当前目标具有高度语义相似性的独特目标，并将它们分配在一起。作者将这个线索作为视频中每一帧可以参考的<strong>全局语义信息</strong>。但是当不确定目标是否存在时，语义信息将会失效。例如一只黑猫在黑夜里走，因为无法确定关键帧中实例的存在性，所以不能依赖语义信息来判断目标在哪。</p><h2 id="局部定位信息："><a href="#局部定位信息：" class="headerlink" title="局部定位信息："></a>局部定位信息：</h2><p>  如果给出临近帧，该问题将被减轻。借助由临近帧之间的差异计算出的运动信息，就可以在关键帧中定位目标。作者将这种信息来源称为局部定位信息。通常，人们主要通过这两种信息源来识别对象。</p><h2 id="ineffective-problem"><a href="#ineffective-problem" class="headerlink" title="ineffective problem"></a>ineffective problem</h2><p>先前的视频目标检测方法可以被看成是不同的近似方法，可被分为两类：局部聚合方法和全局聚合方法。但这些方法都没有同时使用局部和全局信息，作者将该问题称为ineffective problem。</p><h2 id="insufficient-problem"><a href="#insufficient-problem" class="headerlink" title="insufficient problem"></a>insufficient problem</h2><p>  先前方法的另一个问题是用于聚合的帧的数量问题。指的是关键帧可以收集的信息量。此前方法仅选择20-30帧，仅持续1-2秒用于特征聚合。这种小的聚合规模,不足以近似地反映出局部影响力或全局影响力,更不用说图1(a)了。作者将用于关键帧聚合的帧的数量太少的问题称为insufficient problem。</p><h2 id="MEGA"><a href="#MEGA" class="headerlink" title="MEGA"></a>MEGA</h2><p>MEGA通过有效地聚合全局和局部信息来增强关键帧的候选框特征。MEGA是一个多阶段的结构，第一阶段MEGA解决ineffective问题，通过聚合全局和局部信息。如图1(b)上所示，可用的内容仍然十分有限。因此在第二阶段，作者引入了一个新颖的Long Range Memory(LRM)模块，使关键帧能够访问比以前任何方法更多的内容。特别是，不使用从头开始计算当前关键帧的特征，而是重用在先前帧的检测过程中获得的预先计算的特征。这些预先计算的特征存储在LRM中，并在当前帧和先前帧之间建立循环连接。注意，这些存储的特征首次用全局信息增强，这意味着当前关键帧可以获得更多局部和全局的信息。</p><p>MEGA首先将选定的全局特征聚合到局部特征，然后这些经过global增强的局部特征与LRM模块将较长的全局内容和局部信息合并到关键帧中，以更好的进行检测。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qgg1dwsj30ry170hbv.jpg" srcset="/img/loading.gif" alt="image-20201127145138884" style="zoom: 33%;" /></p><h2 id="符号定义"><a href="#符号定义" class="headerlink" title="符号定义"></a>符号定义</h2><p>视频目标检测的目的是对视频中的每一帧{It}Tt=1给出检测结果。假设当前帧为Ik，  Bt={bit}表示RPN在每一帧It上生成的候选框。临近帧{It}k+τt=k-τ中所有的候选框构成了   local pool:<strong>L</strong>={Bt}k+τt=k-τ。对于全局特征，随机打乱顺序排列的序列{1,…,T}，得到一个打乱的索引序列S，然后依次选取Tg帧，将其所有的候选框构成global pool：<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q6qcnz0j30h203cq4f.jpg" srcset="/img/loading.gif" alt="image-20201127144219511" style="zoom:25%;" />。最后，引入了一种新颖的long range存储模块<strong>M</strong>，用于存储在前一帧的检测过程中产生的中间特征，以使关键帧能够利用缓存的信息，从而具有对long-term全局和局部依赖性进行建模的能力。作者的终极目标是在<strong>L,G</strong>和<strong>M</strong>的帮助下，为关键帧中所有的候选框给出分类和回归的结果。</p><p>用fi表示每个框bi的语义特征，用gi表示定位特征。gi同时表示空间信息（宽度、高度、中心位置）和时间信息（帧的编号）。</p><h2 id="关系模块："><a href="#关系模块：" class="headerlink" title="关系模块："></a>关系模块：</h2><p>用于挖掘框之间的关系。给出一个框的集合B={bi}，目标关系模块为每个框bi,计算其与其他M个框的关系特征，作为语义特征的加权和。框bi的第m个关系特征计算公式为：</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q72trqdj30nk02emy8.jpg" srcset="/img/loading.gif" alt="image-20201127144238991" style="zoom: 50%;" /></p><p>其中WmV是一个线性变化矩阵。wij表示bi和bj之间的影响力，通过语义特征f和可能的定位特征g度量。*∈{L,N}表示定位特征是否被合并到w中，L表示合并，N表示不合并。因为定位特征在两个遥远的框之间在时间维度上是冗余的，可能会伤害整体的性能，所以作者设计了一个location-free的版本，去除关系模块，仅关注语义模块。作者的定位特征包含了时间信息，来区别不同帧对box的影响。</p><p>最终将M个关系特征与其原始特征连接，获得增强后的输出特征：</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q7qawzwj30ua03gabw.jpg" srcset="/img/loading.gif" alt="image-20201127144316936" style="zoom:50%;" /></p><p>生成增强特征后，还附加了一个非线性变换函数h（·），该函数被实现为一个全连接层，并在其后附加了ReLU。此外，可以将关系模块扩展到两个box集合之间的关系。为了方便，使用 <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q88b5rvj307m026t94.jpg" srcset="/img/loading.gif" alt="image-20201127144345945" style="zoom:50%;" />来表示聚集所有增强的proposal特征，例如<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q8lj054j30g203o402.jpg" srcset="/img/loading.gif" alt="image-20201127144407231" style="zoom:33%;" />表示B中所有通过P中的边界框特征增强过的边界框。</p><h2 id="Global-local聚合来解决ineffective问题"><a href="#Global-local聚合来解决ineffective问题" class="headerlink" title="Global-local聚合来解决ineffective问题"></a>Global-local聚合来解决ineffective问题</h2><p>首先详细说明了我们如何设计网络来聚合全局和局部特征，来解决ineffective问题，这意味着分开考虑全局和局部信息。使用图2(a)中的base model来表示该结构。</p><p>首先将<strong>G</strong>中的全局特征合并至<strong>L</strong>，更新函数表示为：</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q8tvvt2j30gk01i74p.jpg" srcset="/img/loading.gif" alt="image-20201127144420262" style="zoom:50%;" /></p><p>其中，<em>N</em>g(·)是一个堆叠的<strong>location-free</strong>关系模块的函数，Lg表示L通过该函数进行全局增强后的版本。我们的目的是完全利用潜在的特征来增强局部特征,我们使用Ng关系模块，用堆叠的方式迭代进行关系推理，以更好的表征<strong>G</strong>和<strong>L</strong>之间的关系。特别的，第k个关系模块的计算过程表示为。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q922z6lj30ie01ct9f.jpg" srcset="/img/loading.gif" alt="image-20201127144434021" style="zoom:33%;" /></p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q9dw8yaj304g01ujrl.jpg" srcset="/img/loading.gif" alt="image-20201127144452536" style="zoom:33%;" />  表示等式(2)定义的location-free关系模块，<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3q9qiblfj30400143yh.jpg" srcset="/img/loading.gif" alt="image-20201127144513142" style="zoom:33%;" />表示第一个关系模块的输入。后面的关系模块用前面关系模块的输出作为输入。第Ng个关系模块输出为<strong>L</strong>g。</p><p>在将全局特征合并到局部特征之后，我们寻求使用在<strong>局部特征之下的语义信息和位置信息来进一步增强局部特征</strong>。为实现这一目的，采用一堆Nl <strong>location-based</strong>的关系模块。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qad44vrj30ek012q36.jpg" srcset="/img/loading.gif" alt="image-20201127144548968" style="zoom:50%;" /></p><p>其中<strong>L</strong>l表示最终增强版本的local pool。将Nl的整个过程进行如下分解：第k个关系模块的计算过程与Ng类似。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qcf77qhj30rg02eab7.jpg" srcset="/img/loading.gif" alt="image-20201127144747425" style="zoom:50%;" /></p><p>  <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qcqsebrj303801idfw.jpg" srcset="/img/loading.gif" alt="image-20201127144805931" style="zoom:50%;" />表示基于位置的关系模块。采用<strong>L</strong>g（经过全局增强的<strong>L</strong>）作为第一个基于位置的关系模块的输入。  是输出增强过的池<strong>L</strong>l。在最终更新完成后，<strong>L</strong>l中属于关键帧的所有box特征将被提取出来，并通过传统的RCNN头部进行传播，得出分类和回归结果。提取后的特征表示为<strong>C</strong>。</p><h2 id="Long-Range-Memory用于解决insufficient-问题"><a href="#Long-Range-Memory用于解决insufficient-问题" class="headerlink" title="Long Range Memory用于解决insufficient 问题"></a>Long Range Memory用于解决insufficient 问题</h2><p>  如图2(a)所示，在base model中，单帧可以聚合整个Tg帧的全局特征和Tl帧的局部特，这朝着解决ineffective problem迈出了一大步，但仍未解决。该问题可以通过增加Tg和Tl至整个视频的长度来被幼稚的解决，前提是内存和算力无限大。</p><p>  那么，如何能在有限的算力下解决insufficient问题呢？受[5]中循环机制的启发，作者设计了一个Long Range Memory(LRM)模块来实现这一目标。总的来说，LRM通过充分使用预计算的特征，使得base model能捕捉更长的全局和局部内容。作者将这个记忆增强过的版本称为MEGA。</p><h1 id="2-MEGA代码运行结果及相关笔记"><a href="#2-MEGA代码运行结果及相关笔记" class="headerlink" title="2.MEGA代码运行结果及相关笔记"></a>2.MEGA代码运行结果及相关笔记</h1><h2 id="demo运行结果："><a href="#demo运行结果：" class="headerlink" title="demo运行结果："></a>demo运行结果：</h2><p>   <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qdi1vtdj30oq0iy7nn.jpg" srcset="/img/loading.gif" alt="image-20201127144849745" style="zoom:50%;" /></p><p>   <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qdtd3raj30yo0jc4qp.jpg" srcset="/img/loading.gif" alt="image-20201127144906084" style="zoom:50%;" /></p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qe8yrr4j311m0g4b29.jpg" srcset="/img/loading.gif" alt="image-20201127144932144"></p><h2 id="相关函数记录："><a href="#相关函数记录：" class="headerlink" title="相关函数记录："></a>相关函数记录：</h2><ul><li>颜色更改：predictor.py line:562 compute_colors_for_labels()</li><li>画边界框的函数：predictor.py line:570 overlay_boxes()</li><li>给边界框打标签(类名、分数)：predictor.py line:592 overlay_class_names()</li><li>predictor.py line:610 cv2.putText()可以更改字体、文字颜色</li></ul><h2 id="base-与-MEGA对比"><a href="#base-与-MEGA对比" class="headerlink" title="base 与 MEGA对比"></a>base 与 MEGA对比</h2><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qerfo3pj30to0eih3u.jpg" srcset="/img/loading.gif" alt="image-20201127145002247" style="zoom:50%;" /></p><p>左边为base，在某些帧会出现丢失目标现象，MEGA的效果好很多。</p><p><img src="/Users/htx/Library/Application Support/typora-user-images/image-20201127145443365.png" srcset="/img/loading.gif" alt="image-20201127145443365" style="zoom:50%;" /></p><p>但某些情况下MEGA会将目标误检为cat，并且持续好几帧（可能是由于LRM存储了前几帧检测结果的原因）。base对每一帧都重新检测，没有先前帧的干扰。</p><h2 id="test-net-py的程序逻辑"><a href="#test-net-py的程序逻辑" class="headerlink" title="test_net.py的程序逻辑"></a>test_net.py的程序逻辑</h2><p>首先加载了checkpoints,初始化了用于验证的dataloader，之后在第116行调用Inference进行推理。inference.py中先初始化了计时器，然后调用compute_on_dataset()进行计算。</p><p>计算完成后，调用_accumulate_predictions_from_multiple_gpus()对预测结果进行累积。</p><h2 id="test-net-py运行结果："><a href="#test-net-py运行结果：" class="headerlink" title="test_net.py运行结果："></a>test_net.py运行结果：</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gl3qfqh580j30q80zwqqq.jpg" srcset="/img/loading.gif" alt="image-20201127145058408" style="zoom:50%;" /></p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>MEGA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Java学习笔记1：快速入门</title>
    <link href="/2020/11/12/Java-1/"/>
    <url>/2020/11/12/Java-1/</url>
    
    <content type="html"><![CDATA[<p>最近因为一些原因，开始学习java，现将学习笔记整理发布，便于后续查阅。我之前有C、C++和Python的基础，希望学习java能容易一些吧。</p><p>这次的笔记主要是java的入门语法，除了输入输出与C++有些不同之外，其余的语法格式与C++比较相似，也比较容易理解。</p><p>本次学习主要是参考<a href="https://www.liaoxuefeng.com/wiki/1252599548343744" target="_blank" rel="noopener">廖雪峰的Java教程</a></p><h1 id="一、java简介"><a href="#一、java简介" class="headerlink" title="一、java简介"></a>一、java简介</h1><p>Java最早是由SUN公司（已被Oracle收购）的<a href="https://en.wikipedia.org/wiki/James_Gosling" target="_blank" rel="noopener">詹姆斯·高斯林</a>（高司令，人称Java之父）在上个世纪90年代初开发的一种编程语言，最初被命名为Oak，目标是针对小型家电设备的嵌入式应用，结果市场没啥反响。谁料到互联网的崛起，让Oak重新焕发了生机，于是SUN公司改造了Oak，在1995年以Java的名称正式发布，原因是Oak已经被人注册了，因此SUN注册了Java这个商标。随着互联网的高速发展，Java逐渐成为最重要的网络编程语言。</p><p>Java介于编译型语言和解释型语言之间。编译型语言如C、C++，代码是直接编译成机器码执行，但是不同的平台（x86、ARM等）CPU的指令集不同，因此，需要编译出每一种平台的对应机器码。解释型语言如Python、Ruby没有这个问题，可以由解释器直接加载源码然后运行，代价是运行效率太低。而Java是将代码编译成一种“字节码”，它类似于抽象的CPU指令，然后，针对不同平台编写虚拟机，不同平台的虚拟机负责加载字节码并执行，这样就实现了“一次编写，到处运行”的效果。当然，这是针对Java开发者而言。对于虚拟机，需要为每个平台分别开发。为了保证不同平台、不同公司开发的虚拟机都能正确执行Java字节码，SUN公司制定了一系列的Java虚拟机规范。从实践的角度看，JVM的兼容性做得非常好，低版本的Java字节码完全可以正常运行在高版本的JVM上。</p><p>随着Java的发展，SUN给Java又分出了三个不同版本：</p><ul><li>Java SE：Standard Edition</li><li>Java EE：Enterprise Edition</li><li>Java ME：Micro Edition</li></ul><p>这三者之间有啥关系呢？</p><p>简单来说，Java SE就是标准版，包含标准的JVM和标准库，而Java EE是企业版，它只是在Java SE的基础上加上了大量的API和库，以便方便开发Web应用、数据库、消息服务等，Java EE的应用使用的虚拟机和Java SE完全相同。</p><pre><code class="hljs ascii">┌───────────────────────────┐│Java EE                    ││    ┌────────────────────┐ ││    │Java SE             │ ││    │    ┌─────────────┐ │ ││    │    │   Java ME   │ │ ││    │    └─────────────┘ │ ││    └────────────────────┘ │└───────────────────────────┘</code></pre><h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><ul><li>JDK：Java Development Kit</li><li>JRE：Java Runtime Environment</li></ul><p>简单地说，JRE就是运行Java字节码的虚拟机。但是，如果只有Java源码，要编译成Java字节码，就需要JDK，因为JDK除了包含JRE，还提供了编译器、调试器等开发工具。</p><p>二者关系如下：</p><pre><code class="hljs ascii"> ┌─    ┌──────────────────────────────────┐ │     │     Compiler, debugger, etc.     │ │     └──────────────────────────────────┘JDK ┌─ ┌──────────────────────────────────┐ │  │  │                                  │ │ JRE │      JVM + Runtime Library       │ │  │  │                                  │ └─ └─ └──────────────────────────────────┘       ┌───────┐┌───────┐┌───────┐┌───────┐       │Windows││ Linux ││ macOS ││others │       └───────┘└───────┘└───────┘└───────┘</code></pre><ul><li>JSR规范：Java Specification Request</li><li>JCP组织：Java Community Process</li></ul><p>为了保证Java语言的规范性，SUN公司搞了一个JSR规范，凡是想给Java平台加一个功能，比如说访问数据库的功能，大家要先创建一个JSR规范，定义好接口，这样，各个数据库厂商都按照规范写出Java驱动程序，开发者就不用担心自己写的数据库代码在MySQL上能跑，却不能跑在PostgreSQL上。</p><p>所以JSR是一系列的规范，从JVM的内存模型到Web程序接口，全部都标准化了。而负责审核JSR的组织就是JCP。</p><p>一个JSR规范发布时，为了让大家有个参考，还要同时发布一个“参考实现”，以及一个“兼容性测试套件”：</p><ul><li>RI：Reference Implementation</li><li>TCK：Technology Compatibility Kit</li></ul><h2 id="第一个java程序"><a href="#第一个java程序" class="headerlink" title="第一个java程序"></a>第一个java程序</h2><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> </span>&#123;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;System.out.println(<span class="hljs-string">"Hello,world!"</span>);&#125;&#125;</code></pre><p>Java规定，某个类定义的<code>public static void main(String[] args)</code>是Java程序的固定入口方法，因此，Java程序总是从<code>main</code>方法开始执行。</p><p>把代码保存为文件时，文件名必须是<code>Hello.java</code>，而且文件名也要注意大小写，因为要和我们定义的类名<code>Hello</code>完全保持一致。</p><p>Java源码本质上是一个文本文件，我们需要先用<code>javac</code>把<code>Hello.java</code>编译成字节码文件<code>Hello.class</code>，然后，用<code>java</code>命令执行这个字节码文件</p><pre><code class="hljs ascii">┌──────────────────┐│    Hello.java    │&lt;─── source code└──────────────────┘          │ compile          ▼┌──────────────────┐│   Hello.class    │&lt;─── byte code└──────────────────┘          │ execute          ▼┌──────────────────┐│    Run on JVM    │└──────────────────┘</code></pre><p>小结：</p><ul><li>一个Java源码只能定义一个<code>public</code>类型的class，并且class名称和文件名要完全一致；</li><li>使用<code>javac</code>可以将<code>.java</code>源码编译成<code>.class</code>字节码；</li><li>使用<code>java</code>可以运行一个已编译的Java程序，参数是类名。</li></ul><h1 id="二、Java程序基础"><a href="#二、Java程序基础" class="headerlink" title="二、Java程序基础"></a>二、Java程序基础</h1><h2 id="1-基本结构："><a href="#1-基本结构：" class="headerlink" title="1.基本结构："></a>1.基本结构：</h2><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Hello</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        System.out.<span class="hljs-built_in">println</span>(<span class="hljs-string">"Hello, world!"</span>); <span class="hljs-comment">// 语句</span>    &#125;&#125;</code></pre><p>Eclipse IDE提供了快捷键<code>Ctrl+Shift+F</code>（macOS是<code>⌘+⇧+F</code>）帮助我们快速格式化代码</p><h3 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h3><p>在Java中，变量分为两种：基本类型的变量和引用类型的变量。</p><p>在Java中，变量必须先定义后使用</p><p><code>System.out.println(&quot;x = &quot; + x); // 打印x的值</code></p><h2 id="2-基本数据类型"><a href="#2-基本数据类型" class="headerlink" title="2.基本数据类型"></a>2.基本数据类型</h2><p>基本数据类型是CPU可以直接进行运算的类型。Java定义了以下几种基本数据类型：</p><ul><li>整数类型：byte，short，int，long</li><li>浮点数类型：float，double</li><li>字符类型：char</li><li>布尔类型：boolean</li></ul><p>Java基本数据类型占用的字节数：</p><pre><code class="hljs ascii">       ┌───┐  byte │   │       └───┘       ┌───┬───┐ short │   │   │       └───┴───┘       ┌───┬───┬───┬───┐   int │   │   │   │   │       └───┴───┴───┴───┘       ┌───┬───┬───┬───┬───┬───┬───┬───┐  long │   │   │   │   │   │   │   │   │       └───┴───┴───┴───┴───┴───┴───┴───┘       ┌───┬───┬───┬───┐ float │   │   │   │   │       └───┴───┴───┴───┘       ┌───┬───┬───┬───┬───┬───┬───┬───┐double │   │   │   │   │   │   │   │   │       └───┴───┴───┴───┴───┴───┴───┴───┘       ┌───┬───┐  char │   │   │       └───┴───┘</code></pre><h3 id="整型"><a href="#整型" class="headerlink" title="整型"></a>整型</h3><p>对于整型类型，Java只定义了带符号的整型，因此，最高位的bit表示符号位（0表示正数，1表示负数）。各种整型能表示的最大范围如下：</p><ul><li>byte：-128 ~ 127</li><li>short: -32768 ~ 32767</li><li>int: -2147483648 ~ 2147483647</li><li>long: -9223372036854775808 ~ 9223372036854775807   long型的结尾需要加L</li></ul><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p>浮点类型的数就是小数，因为小数用科学计数法表示的时候，小数点是可以“浮动”的，如1234.5可以表示成12.345x102，也可以表示成1.2345x103，所以称为浮点数。</p><pre><code class="hljs angelscript"><span class="hljs-built_in">float</span> f1 = <span class="hljs-number">3.14f</span>;<span class="hljs-built_in">float</span> f2 = <span class="hljs-number">3.14e38f</span>; <span class="hljs-comment">// 科学计数法表示的3.14x10^38</span><span class="hljs-built_in">double</span> d = <span class="hljs-number">1.79e308</span>;<span class="hljs-built_in">double</span> d2 = <span class="hljs-number">-1.79e308</span>;<span class="hljs-built_in">double</span> d3 = <span class="hljs-number">4.9e-324</span>; <span class="hljs-comment">// 科学计数法表示的4.9x10^-324</span></code></pre><p>对于<code>float</code>类型，需要加上<code>f</code>后缀。</p><h3 id="布尔类型"><a href="#布尔类型" class="headerlink" title="布尔类型"></a>布尔类型</h3><p>布尔类型<code>boolean</code>只有<code>true</code>和<code>false</code>两个值，布尔类型总是关系运算的计算结果：</p><pre><code class="hljs angelscript"><span class="hljs-built_in">bool</span>ean b1 = <span class="hljs-literal">true</span>;<span class="hljs-built_in">bool</span>ean b2 = <span class="hljs-literal">false</span>;<span class="hljs-built_in">bool</span>ean isGreater = <span class="hljs-number">5</span> &gt; <span class="hljs-number">3</span>; <span class="hljs-comment">// 计算结果为true</span><span class="hljs-built_in">int</span> age = <span class="hljs-number">12</span>;<span class="hljs-built_in">bool</span>ean isAdult = age &gt;= <span class="hljs-number">18</span>; <span class="hljs-comment">// 计算结果为false</span></code></pre><p>Java语言对布尔类型的存储并没有做规定，因为理论上存储布尔类型只需要1 bit，但是通常JVM内部会把<code>boolean</code>表示为4字节整数。</p><h3 id="字符类型"><a href="#字符类型" class="headerlink" title="字符类型"></a>字符类型</h3><p>字符类型<code>char</code>表示一个字符。Java的<code>char</code>类型除了可表示标准的ASCII外，还可以表示一个Unicode字符：</p><pre><code class="hljs arduino"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> &#123;</span>    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(<span class="hljs-keyword">String</span>[] args)</span> </span>&#123;        <span class="hljs-keyword">char</span> a = <span class="hljs-string">'A'</span>;        <span class="hljs-keyword">char</span> zh = <span class="hljs-string">'中'</span>;        System.out.<span class="hljs-built_in">println</span>(a);        System.out.<span class="hljs-built_in">println</span>(zh);    &#125;&#125;</code></pre><p>注意<code>char</code>类型使用单引号<code>&#39;</code>，且仅有一个字符，要和双引号<code>&quot;</code>的字符串类型区分开。</p><h3 id="引用类型"><a href="#引用类型" class="headerlink" title="引用类型"></a>引用类型</h3><p>除了上述基本类型的变量，剩下的都是引用类型。例如，引用类型最常用的就是<code>String</code>字符串：</p><pre><code class="hljs ebnf"><span class="hljs-attribute">String s</span> = <span class="hljs-string">"hello"</span>;</code></pre><p>引用类型的变量类似于C语言的指针，它内部存储一个“地址”，指向某个对象在内存的位置。</p><h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><p>定义变量的时候，如果加上<code>final</code>修饰符，这个变量就变成了常量：</p><pre><code class="hljs angelscript"><span class="hljs-keyword">final</span> <span class="hljs-built_in">double</span> PI = <span class="hljs-number">3.14</span>; <span class="hljs-comment">// PI是一个常量</span><span class="hljs-built_in">double</span> r = <span class="hljs-number">5.0</span>;<span class="hljs-built_in">double</span> area = PI * r * r;PI = <span class="hljs-number">300</span>; <span class="hljs-comment">// compile error!</span></code></pre><p>常量在定义时进行初始化后就不可再次赋值，再次赋值会导致编译错误。</p><h3 id="var关键字"><a href="#var关键字" class="headerlink" title="var关键字"></a>var关键字</h3><p>有些时候，类型的名字太长，写起来比较麻烦。例如：</p><pre><code class="hljs armasm"><span class="hljs-keyword">StringBuilder </span><span class="hljs-built_in">sb</span> = new <span class="hljs-keyword">StringBuilder();</span></code></pre><p>这个时候，如果想省略变量类型，可以使用<code>var</code>关键字：</p><pre><code class="hljs haxe"><span class="hljs-keyword">var</span> sb = <span class="hljs-keyword">new</span> <span class="hljs-type">StringBuilder</span>();</code></pre><p>编译器会根据赋值语句自动推断出变量<code>sb</code>的类型是<code>StringBuilder</code>。对编译器来说，语句：</p><pre><code class="hljs haxe"><span class="hljs-keyword">var</span> sb = <span class="hljs-keyword">new</span> <span class="hljs-type">StringBuilder</span>();</code></pre><p>实际上会自动变成：</p><pre><code class="hljs armasm"><span class="hljs-keyword">StringBuilder </span><span class="hljs-built_in">sb</span> = new <span class="hljs-keyword">StringBuilder();</span></code></pre><p>因此，使用<code>var</code>定义变量，仅仅是少写了变量类型而已。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><p>Java提供了两种变量类型：基本类型和引用类型</p><p>基本类型包括整型，浮点型，布尔型，字符型。</p><p>变量可重新赋值，等号是赋值语句，不是数学意义的等号。</p><p>常量在初始化后不可重新赋值，使用常量便于理解程序意图。</p><h2 id="3-整数运算"><a href="#3-整数运算" class="headerlink" title="3.整数运算"></a>3.整数运算</h2><h3 id="移位运算："><a href="#移位运算：" class="headerlink" title="移位运算："></a>移位运算：</h3><ul><li>左移：&lt;&lt;</li><li>右移：&gt;&gt;    (如果对一个负数进行右移，最高位的<code>1</code>不动，结果仍然是一个负数)</li><li>无符号右移：使用<code>&gt;&gt;&gt;</code>，它的特点是不管符号位，右移后高位总是补<code>0</code>，因此，对一个负数进行<code>&gt;&gt;&gt;</code>右移，它会变成正数，原因是最高位的<code>1</code>变成了<code>0</code>：</li><li>对<code>byte</code>和<code>short</code>类型进行移位时，会首先转换为<code>int</code>再进行位移。</li></ul><p>仔细观察可发现，左移实际上就是不断地×2，右移实际上就是不断地÷2。</p><h3 id="位运算"><a href="#位运算" class="headerlink" title="位运算"></a>位运算</h3><p>位运算是按位进行与、或、非和异或的运算。</p><p>与运算的规则是，必须两个数同时为<code>1</code>，结果才为<code>1</code>：</p><pre><code class="hljs angelscript">n = <span class="hljs-number">0</span> &amp; <span class="hljs-number">0</span>; <span class="hljs-comment">// 0</span>n = <span class="hljs-number">0</span> &amp; <span class="hljs-number">1</span>; <span class="hljs-comment">// 0</span>n = <span class="hljs-number">1</span> &amp; <span class="hljs-number">0</span>; <span class="hljs-comment">// 0</span>n = <span class="hljs-number">1</span> &amp; <span class="hljs-number">1</span>; <span class="hljs-comment">// 1</span></code></pre><p>或运算的规则是，只要任意一个为<code>1</code>，结果就为<code>1</code>：</p><pre><code class="hljs angelscript">n = <span class="hljs-number">0</span> | <span class="hljs-number">0</span>; <span class="hljs-comment">// 0</span>n = <span class="hljs-number">0</span> | <span class="hljs-number">1</span>; <span class="hljs-comment">// 1</span>n = <span class="hljs-number">1</span> | <span class="hljs-number">0</span>; <span class="hljs-comment">// 1</span>n = <span class="hljs-number">1</span> | <span class="hljs-number">1</span>; <span class="hljs-comment">// 1</span></code></pre><p>非运算的规则是，<code>0</code>和<code>1</code>互换：</p><pre><code class="hljs ini"><span class="hljs-attr">n</span> = ~<span class="hljs-number">0</span><span class="hljs-comment">; // 1</span><span class="hljs-attr">n</span> = ~<span class="hljs-number">1</span><span class="hljs-comment">; // 0</span></code></pre><p>异或运算的规则是，如果两个数不同，结果为<code>1</code>，否则为<code>0</code>：</p><pre><code class="hljs angelscript">n = <span class="hljs-number">0</span> ^ <span class="hljs-number">0</span>; <span class="hljs-comment">// 0</span>n = <span class="hljs-number">0</span> ^ <span class="hljs-number">1</span>; <span class="hljs-comment">// 1</span>n = <span class="hljs-number">1</span> ^ <span class="hljs-number">0</span>; <span class="hljs-comment">// 1</span>n = <span class="hljs-number">1</span> ^ <span class="hljs-number">1</span>; <span class="hljs-comment">// 0</span></code></pre><h3 id="运算优先级"><a href="#运算优先级" class="headerlink" title="运算优先级"></a>运算优先级</h3><p>在Java的计算表达式中，运算优先级从高到低依次是：</p><ul><li><code>()</code></li><li><code>!</code> <code>~</code> <code>++</code> <code>--</code></li><li><code>*</code> <code>/</code> <code>%</code></li><li><code>+</code> <code>-</code></li><li><code>&lt;&lt;</code> <code>&gt;&gt;</code> <code>&gt;&gt;&gt;</code></li><li><code>&amp;</code></li><li><code>|</code></li><li><code>+=</code> <code>-=</code> <code>*=</code> <code>/=</code></li></ul><h2 id="4-浮点数运算"><a href="#4-浮点数运算" class="headerlink" title="4.浮点数运算"></a>4.浮点数运算</h2><p>求平方根可用 Math.sqrt()</p><p>求平方：Math.sqrt(Math.pow(b, 2)</p><pre><code class="hljs pgsql">//<span class="hljs-type">int</span> n = <span class="hljs-number">100</span>;//<span class="hljs-type">int</span> sum = ((<span class="hljs-number">1</span> + n) * n) / <span class="hljs-number">2</span>;//<span class="hljs-keyword">System</span>.<span class="hljs-keyword">out</span>.println("结果：" + sum);//<span class="hljs-type">double</span> a = <span class="hljs-number">1.0</span>;//<span class="hljs-type">double</span> b = <span class="hljs-number">3.0</span>;//<span class="hljs-type">double</span> c = <span class="hljs-number">-4.0</span>;//<span class="hljs-type">double</span> r1 = (-b + Math.sqrt(Math.pow(b, <span class="hljs-number">2</span>) - <span class="hljs-number">4</span> * a * c)) / (<span class="hljs-number">2</span> * a);//<span class="hljs-type">double</span> r2 = (-b - Math.sqrt(Math.pow(b, <span class="hljs-number">2</span>) - <span class="hljs-number">4</span> * a * c)) / (<span class="hljs-number">2</span> * a);//<span class="hljs-keyword">System</span>.<span class="hljs-keyword">out</span>.println("" + r1 + "," + r2);</code></pre><h2 id="5-布尔运算"><a href="#5-布尔运算" class="headerlink" title="5.布尔运算"></a>5.布尔运算</h2><p>对于布尔类型<code>boolean</code>，永远只有<code>true</code>和<code>false</code>两个值。</p><p>布尔运算是一种关系运算，包括以下几类：</p><ul><li>比较运算符：<code>&gt;</code>，<code>&gt;=</code>，<code>&lt;</code>，<code>&lt;=</code>，<code>==</code>，<code>!=</code></li><li>与运算 <code>&amp;&amp;</code></li><li>或运算 <code>||</code></li><li>非运算 <code>!</code></li></ul><p>关系运算符的优先级从高到低依次是：</p><ul><li><code>!</code></li><li><code>&gt;</code>，<code>&gt;=</code>，<code>&lt;</code>，<code>&lt;=</code></li><li><code>==</code>，<code>!=</code></li><li><code>&amp;&amp;</code></li><li><code>||</code></li></ul><h3 id="短路运算"><a href="#短路运算" class="headerlink" title="短路运算"></a>短路运算</h3><p>布尔运算的一个重要特点是短路运算。如果一个布尔运算的表达式能提前确定结果，则后续的计算不再执行，直接返回结果。</p><p>因为<code>false &amp;&amp; x</code>的结果总是<code>false</code>，无论<code>x</code>是<code>true</code>还是<code>false</code>，因此，与运算在确定第一个值为<code>false</code>后，不再继续计算，而是直接返回<code>false</code>。</p><p>三元运算<code>b ? x : y</code>会首先计算<code>b</code>，如果<code>b</code>为<code>true</code>，则只计算<code>x</code>，否则，只计算<code>y</code>。此外，<code>x</code>和<code>y</code>的类型必须相同，因为返回值不是<code>boolean</code>，而是<code>x</code>和<code>y</code>之一。</p><pre><code class="hljs pgsql">//<span class="hljs-type">int</span> age = <span class="hljs-number">3</span>;//<span class="hljs-type">boolean</span> isPrimaryStudent = age &gt;= <span class="hljs-number">6</span> &amp;&amp; age &lt;= <span class="hljs-number">12</span> ? <span class="hljs-keyword">true</span> : <span class="hljs-keyword">false</span>;//<span class="hljs-keyword">System</span>.<span class="hljs-keyword">out</span>.println(isPrimaryStudent ? "Yes" : "No");</code></pre><h2 id="6-字符和字符串"><a href="#6-字符和字符串" class="headerlink" title="6.字符和字符串"></a>6.字符和字符串</h2><h3 id="字符类型-1"><a href="#字符类型-1" class="headerlink" title="字符类型"></a>字符类型</h3><p>字符类型<code>char</code>是基本数据类型，它是<code>character</code>的缩写。一个<code>char</code>保存一个Unicode字符：</p><h3 id="字符串类型"><a href="#字符串类型" class="headerlink" title="字符串类型"></a>字符串类型</h3><p>和<code>char</code>类型不同，字符串类型<code>String</code>是引用类型，我们用双引号<code>&quot;...&quot;</code>表示字符串。一个字符串可以存储0个到任意个字符：</p><p>常见的转义字符包括：</p><ul><li><code>\&quot;</code> 表示字符<code>&quot;</code></li><li><code>\&#39;</code> 表示字符<code>&#39;</code></li><li><code>\\</code> 表示字符<code>\</code></li><li><code>\n</code> 表示换行符</li><li><code>\r</code> 表示回车符</li><li><code>\t</code> 表示Tab</li><li><code>\u####</code> 表示一个Unicode编码的字符</li></ul><h3 id="字符串连接"><a href="#字符串连接" class="headerlink" title="字符串连接"></a>字符串连接</h3><p>Java的编译器对字符串做了特殊照顾，可以使用<code>+</code>连接任意字符串和其他数据类型，这样极大地方便了字符串的处理。</p><p>如果用<code>+</code>连接字符串和其他数据类型，会将其他数据类型先自动转型为字符串，再连接。</p><pre><code class="hljs java"><span class="hljs-keyword">int</span> a = <span class="hljs-number">72</span>;<span class="hljs-keyword">int</span> b = <span class="hljs-number">105</span>;<span class="hljs-keyword">int</span> c = <span class="hljs-number">65281</span>;<span class="hljs-keyword">char</span> aa = (<span class="hljs-keyword">char</span>) a;<span class="hljs-keyword">char</span> bb = (<span class="hljs-keyword">char</span>) b;<span class="hljs-keyword">char</span> cc = (<span class="hljs-keyword">char</span>) c;String s = <span class="hljs-string">""</span> + aa + bb + cc;System.out.println(s);</code></pre><h2 id="7-数组类型"><a href="#7-数组类型" class="headerlink" title="7.数组类型"></a>7.数组类型</h2><pre><code class="hljs java"><span class="hljs-comment">// 5位同学的成绩:</span><span class="hljs-keyword">int</span>[] ns = <span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[<span class="hljs-number">5</span>];ns[<span class="hljs-number">0</span>] = <span class="hljs-number">68</span>;ns[<span class="hljs-number">1</span>] = <span class="hljs-number">79</span>;ns[<span class="hljs-number">2</span>] = <span class="hljs-number">91</span>;ns[<span class="hljs-number">3</span>] = <span class="hljs-number">85</span>;ns[<span class="hljs-number">4</span>] = <span class="hljs-number">62</span>;</code></pre><p>定义一个数组类型的变量，使用数组类型“类型[]”，例如，<code>int[]</code>。和单个基本类型变量不同，数组变量初始化必须使用<code>new int[5]</code>表示创建一个可容纳5个<code>int</code>元素的数组。</p><p>Java的数组有几个特点：</p><ul><li>数组所有元素初始化为默认值，整型都是<code>0</code>，浮点型是<code>0.0</code>，布尔型是<code>false</code>；</li><li>数组一旦创建后，大小就不可改变。</li><li>可以用<code>数组变量.length</code>获取数组大小：</li></ul><pre><code class="hljs java"><span class="hljs-keyword">int</span>[] grade = <span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[<span class="hljs-number">5</span>];<span class="hljs-keyword">int</span>[] ns = <span class="hljs-keyword">new</span> <span class="hljs-keyword">int</span>[] &#123; <span class="hljs-number">68</span>, <span class="hljs-number">79</span>, <span class="hljs-number">91</span>, <span class="hljs-number">85</span>, <span class="hljs-number">62</span> &#125;;<span class="hljs-comment">//int[] ns = &#123; 68, 79, 91, 85, 62 &#125;;</span>System.out.println(ns.length);grade[<span class="hljs-number">0</span>]=<span class="hljs-number">99</span>;grade[<span class="hljs-number">1</span>]=<span class="hljs-number">100</span>;System.out.println(grade[<span class="hljs-number">4</span>]);</code></pre><h3 id="字符串数组"><a href="#字符串数组" class="headerlink" title="字符串数组"></a>字符串数组</h3><p>如果数组元素不是基本类型，而是一个引用类型，那么，修改数组元素会有哪些不同？</p><p>字符串是引用类型，因此我们先定义一个字符串数组：</p><pre><code class="hljs java">String[] names = &#123;    <span class="hljs-string">"ABC"</span>, <span class="hljs-string">"XYZ"</span>, <span class="hljs-string">"zoo"</span>&#125;;</code></pre><h3 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h3><p>数组是同一数据类型的集合，数组一旦创建后，大小就不可变；</p><p>可以通过索引访问数组元素，但索引超出范围将报错；</p><p>数组元素可以是值类型（如int）或引用类型（如String），但数组本身是引用类型；</p><h1 id="三、流程控制"><a href="#三、流程控制" class="headerlink" title="三、流程控制"></a>三、流程控制</h1><h2 id="1-输入和输出"><a href="#1-输入和输出" class="headerlink" title="1.输入和输出"></a>1.输入和输出</h2><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><p>使用<code>System.out.println()</code>来向屏幕输出一些内容。<code>println</code>是print line的缩写，表示输出并换行。因此，如果输出后不想换行，可以用<code>print()</code></p><h3 id="格式化输出"><a href="#格式化输出" class="headerlink" title="格式化输出"></a>格式化输出</h3><pre><code class="hljs java"><span class="hljs-keyword">double</span> d = <span class="hljs-number">3.1415926</span>;System.out.printf(<span class="hljs-string">"%.2f\n"</span>, d); <span class="hljs-comment">// 显示两位小数3.14</span>System.out.printf(<span class="hljs-string">"%.4f\n"</span>, d); <span class="hljs-comment">// 显示4位小数3.1416</span></code></pre><p>Java的格式化功能提供了多种占位符，可以把各种数据类型“格式化”成指定的字符串：</p><div class="table-container"><table><thead><tr><th style="text-align:left">占位符</th><th style="text-align:left">说明</th></tr></thead><tbody><tr><td style="text-align:left">%d</td><td style="text-align:left">格式化输出整数</td></tr><tr><td style="text-align:left">%x</td><td style="text-align:left">格式化输出十六进制整数</td></tr><tr><td style="text-align:left">%f</td><td style="text-align:left">格式化输出浮点数</td></tr><tr><td style="text-align:left">%e</td><td style="text-align:left">格式化输出科学计数法表示的浮点数</td></tr><tr><td style="text-align:left">%s</td><td style="text-align:left">格式化字符串</td></tr></tbody></table></div><h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.Scanner;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        Scanner scanner = <span class="hljs-keyword">new</span> Scanner(System.in); <span class="hljs-comment">// 创建Scanner对象</span>        System.out.print(<span class="hljs-string">"Input your name: "</span>); <span class="hljs-comment">// 打印提示</span>        String name = scanner.nextLine(); <span class="hljs-comment">// 读取一行输入并获取字符串</span>        System.out.print(<span class="hljs-string">"Input your age: "</span>); <span class="hljs-comment">// 打印提示</span>        <span class="hljs-keyword">int</span> age = scanner.nextInt(); <span class="hljs-comment">// 读取一行输入并获取整数</span>        System.out.printf(<span class="hljs-string">"Hi, %s, you are %d\n"</span>, name, age); <span class="hljs-comment">// 格式化输出</span>    &#125;&#125;</code></pre><p>首先，我们通过<code>import</code>语句导入<code>java.util.Scanner</code>，<code>import</code>是导入某个类的语句，必须放到Java源代码的开头，后面我们在Java的<code>package</code>中会详细讲解如何使用<code>import</code>。</p><p>然后，创建<code>Scanner</code>对象并传入<code>System.in</code>。<code>System.out</code>代表标准输出流，而<code>System.in</code>代表标准输入流。直接使用<code>System.in</code>读取用户输入虽然是可以的，但需要更复杂的代码，而通过<code>Scanner</code>就可以简化后续的代码。</p><p>有了<code>Scanner</code>对象后，要读取用户输入的字符串，使用<code>scanner.nextLine()</code>，要读取用户输入的整数，使用<code>scanner.nextInt()</code>。<code>Scanner</code>会自动转换数据类型，因此不必手动转换。</p><h2 id="2-if判断"><a href="#2-if判断" class="headerlink" title="2.if判断"></a>2.if判断</h2><pre><code class="hljs java"><span class="hljs-keyword">int</span> n = <span class="hljs-number">90</span>;<span class="hljs-keyword">if</span> (n &gt; <span class="hljs-number">90</span>) &#123;  System.out.println(<span class="hljs-string">"优秀"</span>);&#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (n &gt;= <span class="hljs-number">60</span>) &#123;  System.out.println(<span class="hljs-string">"及格了"</span>);&#125; <span class="hljs-keyword">else</span> &#123;  System.out.println(<span class="hljs-string">"挂科了"</span>);&#125;</code></pre><p>要判断引用类型的变量内容是否相等，必须使用<code>equals()</code>方法：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        String s1 = <span class="hljs-string">"hello"</span>;        String s2 = <span class="hljs-string">"HELLO"</span>.toLowerCase();        System.out.println(s1);        System.out.println(s2);        <span class="hljs-keyword">if</span> (s1.equals(s2)) &#123;            System.out.println(<span class="hljs-string">"s1 equals s2"</span>);        &#125; <span class="hljs-keyword">else</span> &#123;            System.out.println(<span class="hljs-string">"s1 not equals s2"</span>);        &#125;    &#125;&#125;</code></pre><h3 id="小结-2"><a href="#小结-2" class="headerlink" title="小结"></a>小结</h3><p><code>if ... else</code>可以做条件判断，<code>else</code>是可选的；</p><p>不推荐省略花括号<code>{}</code>；</p><p>多个<code>if ... else</code>串联要特别注意判断顺序；</p><p>要注意<code>if</code>的边界条件；</p><p>要注意浮点数判断相等不能直接用<code>==</code>运算符；</p><p>引用类型判断内容相等要使用<code>equals()</code>，注意避免<code>NullPointerException</code>。</p><h2 id="3-switch"><a href="#3-switch" class="headerlink" title="3.switch"></a>3.switch</h2><p>如果<code>option</code>的值没有匹配到任何<code>case</code>，例如<code>option = 99</code>，那么，<code>switch</code>语句不会执行任何语句。这时，可以给<code>switch</code>语句加一个<code>default</code>，当没有匹配到任何<code>case</code>时，执行<code>default</code>：</p><pre><code class="hljs java"><span class="hljs-keyword">int</span> option = <span class="hljs-number">99</span>;<span class="hljs-keyword">switch</span> (option) &#123;  <span class="hljs-keyword">case</span> <span class="hljs-number">1</span>:  System.out.println(<span class="hljs-string">"Selected 1"</span>);  <span class="hljs-keyword">break</span>;  <span class="hljs-keyword">case</span> <span class="hljs-number">2</span>:  System.out.println(<span class="hljs-string">"Selected 2"</span>);  <span class="hljs-keyword">break</span>;  <span class="hljs-keyword">case</span> <span class="hljs-number">3</span>:  System.out.println(<span class="hljs-string">"Selected 3"</span>);  <span class="hljs-keyword">break</span>;  <span class="hljs-keyword">default</span>:  System.out.println(<span class="hljs-string">"Not selected"</span>);  <span class="hljs-keyword">break</span>;&#125;</code></pre><p>使用<code>switch</code>时，注意<code>case</code>语句并没有花括号<code>{}</code>，而且，<code>case</code>语句具有“<em>穿透性</em>”，漏写<code>break</code>将导致意想不到的结果：</p><h3 id="switch表达式"><a href="#switch表达式" class="headerlink" title="switch表达式"></a>switch表达式</h3><p>使用<code>switch</code>时，如果遗漏了<code>break</code>，就会造成严重的逻辑错误，而且不易在源代码中发现错误。从Java 12开始，<code>switch</code>语句升级为更简洁的表达式语法，使用类似模式匹配（Pattern Matching）的方法，保证只有一种路径会被执行，并且不需要<code>break</code>语句：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        String fruit = <span class="hljs-string">"apple"</span>;        <span class="hljs-keyword">switch</span> (fruit) &#123;        <span class="hljs-keyword">case</span> <span class="hljs-string">"apple"</span> -&gt; System.out.println(<span class="hljs-string">"Selected apple"</span>);        <span class="hljs-keyword">case</span> <span class="hljs-string">"pear"</span> -&gt; System.out.println(<span class="hljs-string">"Selected pear"</span>);        <span class="hljs-keyword">case</span> <span class="hljs-string">"mango"</span> -&gt; &#123;            System.out.println(<span class="hljs-string">"Selected mango"</span>);            System.out.println(<span class="hljs-string">"Good choice!"</span>);        &#125;        <span class="hljs-keyword">default</span> -&gt; System.out.println(<span class="hljs-string">"No fruit selected"</span>);        &#125;    &#125;&#125;</code></pre><p>注意新语法使用<code>-&gt;</code>，如果有多条语句，需要用<code>{}</code>括起来。不要写<code>break</code>语句，因为新语法只会执行匹配的语句，没有穿透效应。</p><p>还可能用<code>switch</code>语句给某个变量赋值。例如：</p><pre><code class="hljs java"><span class="hljs-keyword">int</span> opt;<span class="hljs-keyword">switch</span> (fruit) &#123;<span class="hljs-keyword">case</span> <span class="hljs-string">"apple"</span>:    opt = <span class="hljs-number">1</span>;    <span class="hljs-keyword">break</span>;<span class="hljs-keyword">case</span> <span class="hljs-string">"pear"</span>:<span class="hljs-keyword">case</span> <span class="hljs-string">"mango"</span>:    opt = <span class="hljs-number">2</span>;    <span class="hljs-keyword">break</span>;<span class="hljs-keyword">default</span>:    opt = <span class="hljs-number">0</span>;    <span class="hljs-keyword">break</span>;&#125;</code></pre><p>使用新的<code>switch</code>语法，不但不需要<code>break</code>，还可以直接返回值。把上面的代码改写如下：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        String fruit = <span class="hljs-string">"apple"</span>;        <span class="hljs-keyword">int</span> opt = <span class="hljs-keyword">switch</span> (fruit) &#123;            <span class="hljs-keyword">case</span> <span class="hljs-string">"apple"</span> -&gt; <span class="hljs-number">1</span>;            <span class="hljs-keyword">case</span> <span class="hljs-string">"pear"</span>, <span class="hljs-string">"mango"</span> -&gt; <span class="hljs-number">2</span>;            <span class="hljs-keyword">default</span> -&gt; <span class="hljs-number">0</span>;        &#125;; <span class="hljs-comment">// 注意赋值语句要以;结束</span>        System.out.println(<span class="hljs-string">"opt = "</span> + opt);    &#125;&#125;</code></pre><h2 id="4-while"><a href="#4-while" class="headerlink" title="4.while"></a>4.while</h2><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>; <span class="hljs-comment">// 累加的和，初始化为0</span>        <span class="hljs-keyword">int</span> n = <span class="hljs-number">1</span>;        <span class="hljs-keyword">while</span> (n &lt;= <span class="hljs-number">100</span>) &#123; <span class="hljs-comment">// 循环条件是n &lt;= 100</span>            sum = sum + n; <span class="hljs-comment">// 把n累加到sum中</span>            n ++; <span class="hljs-comment">// n自身加1</span>        &#125;        System.out.println(sum); <span class="hljs-comment">// 5050</span>    &#125;&#125;</code></pre><h2 id="5-do-while"><a href="#5-do-while" class="headerlink" title="5.do while"></a>5.do while</h2><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;        <span class="hljs-keyword">int</span> n = <span class="hljs-number">1</span>;        <span class="hljs-keyword">do</span> &#123;            sum = sum + n;            n ++;        &#125; <span class="hljs-keyword">while</span> (n &lt;= <span class="hljs-number">100</span>);        System.out.println(sum);    &#125;&#125;</code></pre><h2 id="6-for"><a href="#6-for" class="headerlink" title="6.for"></a>6.for</h2><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">1</span>; i&lt;=<span class="hljs-number">100</span>; i++) &#123;            sum = sum + i;        &#125;        System.out.println(sum);    &#125;&#125;</code></pre><h3 id="for-each循环"><a href="#for-each循环" class="headerlink" title="for each循环"></a>for each循环</h3><p><code>for</code>循环经常用来遍历数组，因为通过计数器可以根据索引来访问数组的每个元素：</p><pre><code class="hljs java"><span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>, <span class="hljs-number">25</span> &#125;;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;ns.length; i++) &#123;    System.out.println(ns[i]);&#125;</code></pre><p>但是，很多时候，我们实际上真正想要访问的是数组每个元素的值。Java还提供了另一种<code>for each</code>循环，它可以更简单地遍历数组：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>, <span class="hljs-number">25</span> &#125;;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n : ns) &#123;            System.out.println(n);        &#125;    &#125;&#125;</code></pre><p>和<code>for</code>循环相比，<code>for each</code>循环的变量n不再是计数器，而是直接对应到数组的每个元素。<code>for each</code>循环的写法也更简洁。但是，<code>for each</code>循环无法指定遍历顺序，也无法获取数组的索引。</p><p>除了数组外，<code>for each</code>循环能够遍历所有“可迭代”的数据类型，包括后面会介绍的<code>List</code>、<code>Map</code>等。</p><p>利用<code>for each</code>循环对数组每个元素求和：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>, <span class="hljs-number">25</span> &#125;;        <span class="hljs-keyword">int</span> sum = <span class="hljs-number">0</span>;        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n:ns) &#123;            sum += n;        &#125;        System.out.println(sum); <span class="hljs-comment">// 55</span>    &#125;&#125;</code></pre><h2 id="7-break-、continue"><a href="#7-break-、continue" class="headerlink" title="7.break 、continue"></a>7.break 、continue</h2><h3 id="break"><a href="#break" class="headerlink" title="break"></a>break</h3><p>在循环过程中，可以使用<code>break</code>语句跳出当前循环。</p><p><code>break</code>语句通常配合<code>if</code>，在满足条件时提前结束整个循环；</p><p><code>break</code>语句总是跳出最近的一层循环；</p><h3 id="continue"><a href="#continue" class="headerlink" title="continue"></a>continue</h3><p><code>continue</code>则是提前结束本次循环，直接继续执行下次循环。</p><p><code>continue</code>语句通常配合<code>if</code>，在满足条件时提前结束本次循环。</p><h1 id="四、数组操作"><a href="#四、数组操作" class="headerlink" title="四、数组操作"></a>四、数组操作</h1><h2 id="1-遍历数组"><a href="#1-遍历数组" class="headerlink" title="1.遍历数组"></a>1.遍历数组</h2><pre><code class="hljs java"><span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;<span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>, <span class="hljs-number">25</span> &#125;;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i=<span class="hljs-number">0</span>; i&lt;ns.length; i++) &#123;<span class="hljs-keyword">int</span> n = ns[i];System.out.println(n);&#125;&#125;<span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;  <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">9</span>, <span class="hljs-number">16</span>, <span class="hljs-number">25</span> &#125;;  <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n : ns) &#123;    System.out.println(n);  &#125;&#125;</code></pre><p>注意：在<code>for (int n : ns)</code>循环中，变量<code>n</code>直接拿到<code>ns</code>数组的元素，而不是索引。</p><p>显然<code>for each</code>循环更加简洁。但是，<code>for each</code>循环无法拿到数组的索引，因此，到底用哪一种<code>for</code>循环，取决于我们的需要。</p><h3 id="打印数组内容"><a href="#打印数组内容" class="headerlink" title="打印数组内容"></a>打印数组内容</h3><pre><code class="hljs java"><span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span> &#125;;<span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n : ns) &#123;    System.out.print(n + <span class="hljs-string">", "</span>);&#125;</code></pre><p>使用<code>for each</code>循环打印也很麻烦。幸好Java标准库提供了<code>Arrays.toString()</code>，可以快速打印数组内容：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">8</span> &#125;;        System.out.println(Arrays.toString(ns));    &#125;</code></pre><h2 id="2-数组排序"><a href="#2-数组排序" class="headerlink" title="2.数组排序"></a>2.数组排序</h2><p>冒泡排序：冒泡排序的特点是，每一轮循环后，最大的一个数被交换到末尾，因此，下一轮循环就可以“刨除”最后的数，每一轮循环都比上一轮循环的结束位置靠前一位。</p><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.Arrays;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">28</span>, <span class="hljs-number">12</span>, <span class="hljs-number">89</span>, <span class="hljs-number">73</span>, <span class="hljs-number">65</span>, <span class="hljs-number">18</span>, <span class="hljs-number">96</span>, <span class="hljs-number">50</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span> &#125;;        <span class="hljs-comment">// 排序前:</span>        System.out.println(Arrays.toString(ns));        <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> i = <span class="hljs-number">0</span>; i &lt; ns.length - <span class="hljs-number">1</span>; i++) &#123;            <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> j = <span class="hljs-number">0</span>; j &lt; ns.length - i - <span class="hljs-number">1</span>; j++) &#123;                <span class="hljs-keyword">if</span> (ns[j] &gt; ns[j+<span class="hljs-number">1</span>]) &#123;                    <span class="hljs-comment">// 交换ns[j]和ns[j+1]:</span>                    <span class="hljs-keyword">int</span> tmp = ns[j];                    ns[j] = ns[j+<span class="hljs-number">1</span>];                    ns[j+<span class="hljs-number">1</span>] = tmp;                &#125;            &#125;        &#125;        <span class="hljs-comment">// 排序后:</span>        System.out.println(Arrays.toString(ns));    &#125;&#125;</code></pre><p>实际上，Java的标准库已经内置了排序功能，我们只需要调用JDK提供的<code>Arrays.sort()</code>就可以排序：</p><pre><code class="hljs java"><span class="hljs-keyword">import</span> java.util.Arrays;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">28</span>, <span class="hljs-number">12</span>, <span class="hljs-number">89</span>, <span class="hljs-number">73</span>, <span class="hljs-number">65</span>, <span class="hljs-number">18</span>, <span class="hljs-number">96</span>, <span class="hljs-number">50</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span> &#125;;        Arrays.sort(ns);        System.out.println(Arrays.toString(ns));    &#125;&#125;</code></pre><pre><code class="hljs java"><span class="hljs-comment">// 降序排序</span><span class="hljs-keyword">import</span> java.util.Arrays;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[] ns = &#123; <span class="hljs-number">28</span>, <span class="hljs-number">12</span>, <span class="hljs-number">89</span>, <span class="hljs-number">73</span>, <span class="hljs-number">65</span>, <span class="hljs-number">18</span>, <span class="hljs-number">96</span>, <span class="hljs-number">50</span>, <span class="hljs-number">8</span>, <span class="hljs-number">36</span> &#125;;        <span class="hljs-comment">// 排序前:</span>        System.out.println(Arrays.toString(ns));        <span class="hljs-comment">// 排序后:</span>        System.out.println(Arrays.toString(ns));        <span class="hljs-keyword">if</span> (Arrays.toString(ns).equals(<span class="hljs-string">"[96, 89, 73, 65, 50, 36, 28, 18, 12, 8]"</span>)) &#123;            System.out.println(<span class="hljs-string">"测试成功"</span>);        &#125; <span class="hljs-keyword">else</span> &#123;            System.out.println(<span class="hljs-string">"测试失败"</span>);        &#125;    &#125;&#125;</code></pre><h2 id="3-多维数组"><a href="#3-多维数组" class="headerlink" title="3.多维数组"></a>3.多维数组</h2><h4 id="二维数组"><a href="#二维数组" class="headerlink" title="二维数组"></a>二维数组</h4><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[][] ns = &#123;            &#123; <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span> &#125;,            &#123; <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span> &#125;,            &#123; <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span> &#125;        &#125;;        System.out.println(ns.length); <span class="hljs-comment">// 3</span>    &#125;&#125;</code></pre><p>要打印一个二维数组，可以使用两层嵌套的for循环：</p><pre><code class="hljs java"><span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span>[] arr : ns) &#123;    <span class="hljs-keyword">for</span> (<span class="hljs-keyword">int</span> n : arr) &#123;        System.out.print(n);        System.out.print(<span class="hljs-string">', '</span>);    &#125;    System.out.println();&#125;</code></pre><pre><code class="hljs java"><span class="hljs-comment">//或者使用Java标准库的Arrays.deepToString()：</span><span class="hljs-keyword">import</span> java.util.Arrays;<span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">int</span>[][] ns = &#123;            &#123; <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span> &#125;,            &#123; <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span> &#125;,            &#123; <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>, <span class="hljs-number">12</span> &#125;        &#125;;        System.out.println(Arrays.deepToString(ns));    &#125;&#125;</code></pre><h2 id="4-命令行参数"><a href="#4-命令行参数" class="headerlink" title="4.命令行参数"></a>4.命令行参数</h2><p>Java程序的入口是<code>main</code>方法，而<code>main</code>方法可以接受一个命令行参数，它是一个<code>String[]</code>数组。</p><p>这个命令行参数由JVM接收用户输入并传给<code>main</code>方法：</p><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">for</span> (String arg : args) &#123;            System.out.println(arg);        &#125;    &#125;&#125;</code></pre><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Main</span> </span>&#123;    <span class="hljs-function"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title">main</span><span class="hljs-params">(String[] args)</span> </span>&#123;        <span class="hljs-keyword">for</span> (String arg : args) &#123;            <span class="hljs-keyword">if</span> (<span class="hljs-string">"-version"</span>.equals(arg)) &#123;                System.out.println(<span class="hljs-string">"v 1.0"</span>);                <span class="hljs-keyword">break</span>;            &#125;        &#125;    &#125;&#125;</code></pre><p>上面这个程序必须在命令行执行，执行的时候，给它传递一个<code>-version</code>参数：</p><pre><code class="hljs mipsasm">$ <span class="hljs-keyword">javac </span>Main.<span class="hljs-keyword">java</span><span class="hljs-keyword">$ </span><span class="hljs-keyword">java </span>Main -versionv <span class="hljs-number">1</span>.<span class="hljs-number">0</span></code></pre>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Java</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（11.6)</title>
    <link href="/2020/11/06/11-6/"/>
    <url>/2020/11/06/11-6/</url>
    
    <content type="html"><![CDATA[<p>最近主要阅读了三篇视频目标检测方向的文章，这三篇都是Attention-Related的文章，主要目的在于提高检测的精度。</p><h2 id="阅读文献："><a href="#阅读文献：" class="headerlink" title="阅读文献："></a>阅读文献：</h2><p>[1] Han M, Wang Y, Chang X, et al. Mining Inter-Video Proposal Relations for Video Object Detection[J]// Proceedings of the European Conference on Computer Vision (ECCV). 2020</p><p>[2] Wu H, Chen Y, Wang N, et al. Sequence level semantics aggregation for video object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 9217-9225.</p><p>[3] Deng J, Pan Y, Yao T, et al. Relation distillation networks for video object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 7023-7032.</p><h1 id="1-HVR-Net"><a href="#1-HVR-Net" class="headerlink" title="1.HVR-Net"></a>1.HVR-Net</h1><ul><li><strong>论文地址：</strong><a href="http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf" target="_blank" rel="noopener"><strong>http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123660426.pdf</strong></a></li><li><strong>代码地址：</strong><a href="https://github.com/youthHan/HVRNet" target="_blank" rel="noopener"><strong>https://github.com/youthHan/HVRNet</strong></a></li></ul><p>此前的方法证明了合并不同帧间的proposal信息可以增强视频目标检测的性能，然而这些方法主要探索了单个视频内部的proposal关系，忽略了不同视频间的proposal关系。不同视频之间的proposal关系有助于识别易混淆的目标。作者为了解决这一问题，提出了Inter-Video Proposal Relation module和Hierarchical Video Relation Network（HVR-Net）。前者基于一个简明的多级三元选择体系，通过对不同视频间的困难proposal建模，学习更有效的目标表示。后者通过分层的方式，整合Intra-video和Inter-video的proposal关系，促进视频目标检测。</p><p>​                               <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrq3q5trj30ck0cok0z.jpg" srcset="/img/loading.gif" alt="image-20201110082413497"></p><h2 id="Video-level-Triplet-Selection"><a href="#Video-level-Triplet-Selection" class="headerlink" title="Video-level Triplet Selection"></a>Video-level Triplet Selection</h2><p>  作者设计了一个video-level的triplet-selection模块，可以为一个Target Video从一组Support Videos中选出两个Confused Videos。Confused videos可以来自相同类别中最不相似的视频，或不同类别中最相似的两个视频。至此，在每个训练batch中得到了一个Confused videos的三元组，可用于指导HVR-Net对视频间的易混淆目标建模。</p><p>  详细来说，作者从训练集中随机选择K个目标类别，每个类随机选N个视频。这样，一个batch就有KxN个视频，随机选择其中一个视频作为target video，其余KxN-1个为support video。对每个视频，随机选一帧作为目标帧t，其余T-1帧为辅助帧。</p><p>  将每个视频的T帧单独喂给Faster RCNN的CNN backbone，提取特征。最终得到的该视频的特征张量的尺寸为HxWxCxT，H和W是尺寸，C为通道数。然后在空间和时间维度执行global average pooling，产生C维的视频表示。根据视频间cosine相似度得到video triplet:   <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrvwq753j30hm01yaaw.jpg" srcset="/img/loading.gif" alt="image-20201110082949155">其中V+是同类中最不相似的辅助视频，V-是不同类间最相似的辅助视频。</p><h2 id="Intra-Video-Proposal-Relation"><a href="#Intra-Video-Proposal-Relation" class="headerlink" title="Intra-Video Proposal Relation"></a>Intra-Video Proposal Relation</h2><p>  HVR-Net将上述三元组中的每个视频，抽取其样本帧(如t-s,t,t+e)喂给Faster RCNN的RPN和ROI层，为每个样本帧产生目标proposal的特征向量。随后加权合并t-s和t+e的proposals以增强目标帧t的proposal。这样可以解决Intra-Video的问题（如运动模糊、遮挡等）。使用下式加权合并所有辅助帧的所有proposal特征：(concise non-local-style relation module)</p><p>  是目标帧t的第m个proposal特征，  是辅助帧i的第j个proposal特征，i属于{t-s,t+e}。上式使用一个核函数g(···,·)（例如Embedded Gaussian）来对比  和  的相似性。</p><h2 id="Proposal-level-Triplet-Selection"><a href="#Proposal-level-Triplet-Selection" class="headerlink" title="Proposal-level Triplet Selection"></a>Proposal-level Triplet Selection</h2><p>  上述Intra-video增强proposal只考虑了每个独立视频内的目标语义，忽略了视频间的目标差异。为了对这个差异建模，作者从视频三元组Vtriplet中根据Intra-video增强过的proposal特征，选择hard proposal triplets。</p><p>  详细来说，根据等式(2)进行了视频内增强过的特征，对比这些proposal的cosine相似性。对于目标视频中的一个proposal  ，获得其对应的proposal triplet：</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrs5cuiij30c600qaa6.jpg" srcset="/img/loading.gif" alt="image-20201110082612431"></p><p>其中<strong>P+是同类中最不相似的proposal</strong>，<strong>P-是不同类中最相似的proposal</strong>。</p><h2 id="Inter-Video-Proposal-Relation"><a href="#Inter-Video-Proposal-Relation" class="headerlink" title="Inter-Video Proposal Relation"></a>Inter-Video Proposal Relation</h2><p>  为每个proposal triplets从support video的proposal合并增强到target video的proposal。每个proposal特征都进一步利用视频之间的依存关系来解决视频之间的目标混淆。</p><p>  为proposal triplets之间的关系建模，以描述视频间的目标差异。为每个proposal triplet采用concise non-local-style relation module：</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrsa1rz7j30ds00qdg5.jpg" srcset="/img/loading.gif" alt="image-20201110082619766"></p><p>其中f(·,·)是用于相似性对比的核函数（例如Embedded Gaussian），α+是经过视频内增强的hard positive proposal P+的特征，α-是经过视频内增强的 hard negative proposal P-。</p><p>作者使用以下损失函数来有效的减少检测时的目标混淆：</p><p>​    <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrstso0ij306y00qt8x.jpg" srcset="/img/loading.gif" alt="image-20201110082651032"> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrsvxzlhj308200mt8w.jpg" srcset="/img/loading.gif" alt="image-20201110082654644"></p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrt1ow4ej30ea00wdgr.jpg" srcset="/img/loading.gif" alt="image-20201110082704279"></p><p>这里我发现作者的表述有一定问题，与梁俊学长讨论，他也发现了这个问题：文中所述损失函数中的d为欧氏距离。作者的目的是增强当前Proposal Ptargett,m和P+的关系，减少当前Proposal Ptargett,m和P-的关系。那么就应该缩短与P+的距离，增加与P-的距离。而按照作者的Lrelation的定义则恰恰相反。经过与作者沟通，作者表示通过实验发现这样的Loss效果比较好，但还是感觉解释的有点牵强。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><p>创新点：</p><ul><li>基于灵活的多级triplet selection体系，提出了一个分层视频关系网络HVR-Net。</li><li>学习了不同视频间的关系，提升了视频目标检测效果。</li></ul><p>缺点：损失函数定义可能有误，与文章描述的意图描述不符。</p><h1 id="2-SELSA"><a href="#2-SELSA" class="headerlink" title="2.SELSA"></a>2.SELSA</h1><ul><li><strong>论文链接：</strong></li><li><strong>代码链接：</strong><a href="https://github.com/happywu/Sequence-Level-Semantics-Aggregation" target="_blank" rel="noopener"><strong>https://github.com/happywu/</strong></a></li></ul><p>此前的视频目标检测方法依赖光流或循环神经网络进行特征融合，更强调时间上相邻帧。本文在full-sequence level(整个序列级别)上进行特征融合，具有更多的判别性和鲁棒性。作者不将视频看为连续的，而是将其看为一组无序帧，并尝试在整个序列级别上学习每个类的不变表示。将视频目标检测从顺序检测任务重新解释为multi-shot检测任务。本文的方法设法在proposal级别利用视频级别的信息合并特征，并且是端到端的优化，没有后处理步骤。</p><p>  FGFA和MANet从一个短的时间窗口中提取特征。然而，目标表观退化会在一个较长的时间窗口持续发生，使得基于时间的方法效率不高。而且，这些帧在短时间窗口内可能是高度冗余的，因此削弱了特征聚合的优势。为了解决该问题，作者提出从语义邻域中聚合特征，该特征不易随时间持续出现外观退化。</p><p>  理想的特征聚合方式是在ground truth小片段内合并，但在测试阶段无法为proposal获得跨帧的黄金关联。对于每一帧f，Xf={xf1,xf2,…}表示Faster-RCNN中RPN生成的proposals。对于特定的一对proposal(xki,xlj)使用cosine相似性度量其语义相似性。相似度越高，说明proposal属于同一类的概率越大。</p><p>在定义了proposal间的相似性后，语义相似性就可以指导参考proposal从其他proposal合并特征。通过合并多个proposal，新的proposal将包含更丰富的信息，并且对目标外观变化（如姿势、运动模糊、变形）具有鲁棒性。由于相似性建立在proposal级别上，因此与需要在特征图的每个位置计算光流的方法相比，更具有鲁棒性。</p><p>为了保持合并后的特征大小不变，在所有proposal上使用softmax函数将相似性进行标准化。假设从视频中随机选择F帧进行合并，每帧有N个proposal，合并后的参考proposal定义为：  ，其中Ω是帧的索引。SELSA可以用标准SGD进行端到端的优化，在合并之后，增强过的proposal特征被喂给检测header网络，进行分类和边界框回归。</p><p> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrtchr5vj30ig05yjv4.jpg" srcset="/img/loading.gif" alt="image-20201110082721299"></p><p>SELSA的结构如上图所示，首先在视频的不同帧提取proposal，然后计算其语义相似性，最后根据语义相似性合并其他proposal的特征，从而获得更具有辨识性和鲁棒的目标检测特征。</p><p>此外，作者进一步揭示了该方法与经典光谱聚类算法的紧密联系，从类内方差减少的角度阐明了SELSA的工作方式。</p><h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h2><p>本文使用全序列级别的特征聚合，提出了一个简单高效的SELSA模块，聚合帧间的语义特征。对于关键帧上的某个proposal，聚合来自其他帧中的同类proposal。本文的特征聚合是在proposal level的，而非在feature map级或像素级，所以该方法对运动模糊或较大的姿势变化比较鲁棒。</p><p><strong>创新点：</strong></p><ul><li><p>将聚合的目标放在具有相同语义的proposal上，对于关键帧上的某个proposal，聚合来自其他帧中的同类proposal。</p></li><li><p>将抽样的角度放在了整个序列，将检测与时序解耦，通过打乱视频帧顺序，又一次的提高了准确度。</p></li><li>由于进行了全视频序列采样，该方法很好融合了序列信息，所以不需要进行 SeqNMS 之类的后处理，极大简化了整个方法流程。</li></ul><h1 id="3-Relation-Distillation-Networks"><a href="#3-Relation-Distillation-Networks" class="headerlink" title="3.Relation Distillation Networks"></a>3.Relation Distillation Networks</h1><ul><li><p><strong>论文链接：</strong><a href="https://arxiv.org/pdf/1908.09511v1.pdf" target="_blank" rel="noopener"><strong>https://arxiv.org/pdf/1908.09511v1.pdf</strong></a></p></li><li><p><strong>代码链接：暂未公开</strong></p></li></ul><p>RDN直接在局部范围学习不同帧的候选框之间的关系来增强box-level的特征。</p><p>众所周知，对目标间的关系建模有助于目标检测。但探索目标间的交互对于增强视频目标检测器同样重要。本文设计了RDN来捕捉时空上下文间的目标交互，RDN聚集和传播目标关系来增强用于检测的目标特征。首先通过RPN生成目标proposals，一方面通过多步推理对目标关系建模，另一方面用级联方式的高度对象性的评分，通过精练辅助目标proposals，逐步提炼关系。所学习到的关系验证了该算法在提高帧内目标检测和帧间框连接方面的有效性。</p><p>视频目标检测通过两个方向来探索时空一致性：1）box-level 聚合2)特征聚合。前者在连续帧的bbox内探索关联，生成tubelets。后者通过聚合临近的特征来提升每帧的特征。以上这些增强视频目标检测的方法都没有充分研究如何利用目标之间的关系。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gkjrtgkad1j30eo08uwj8.jpg" srcset="/img/loading.gif" alt="image-20201110082727995"> </p><p>目标关系描述了目标间的交互或几何位置。文章《Relation networks for object detection》的基本思想是，测量一个目标的关系特征，作为图像中其他目标的外观特征的加权总和，权重反映外观和几何信息方面的目标依赖性。图(a)展示了一个堆叠的关系模块，以多步方式聚合关系特征并增强目标特征。该方法验证了对目标关系建模来增强图像目标检测的有效性。然而，将挖掘图像的目标关系扩展至视频非常有挑战，因为视频包含复杂的时空上下文信息。从相邻帧提取的辅助目标以及参考帧的目标都需要被考虑到。这个区别就导致了计算开销和内存需求的急剧增加。如果直接使用《Relation networks for object detection》中的目标关系度量，不考虑增加辅助目标proposals会导致更多无效的proposals，可能会影响整个关系学习的稳定性。</p><p>为了解决这一问题，作者提出了如图(b)所示的多步模块，其独到的设计是逐步安排关系蒸馏。我们从所有支持框架中选择具有较高客观性得分的目标proposals，并且仅通过目标关系来增强这些提议的特征，以进一步提炼相对于参考帧中proposal的关系。这种级联的方法一方面可以减少计算量，过滤掉无效的proposal，另一方面可以更好地改善目标关系。</p><h2 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h2><p>作者通过巩固时空上下文中对象关系建模的思想，提出了关系蒸馏网络（RDN），用于增强视频对象检测。具体来说，利用Region Proposal Network（RPN）从参考帧和所有支持帧中生成目标proposal。从支持帧中提取的proposal被打包放入supportive pool。RDN的目标是：通过聚合supportive pool中proposals的关系特征，增强参考帧中每个目标proposal的特征。</p><p>RDN采用多阶段推理结构，包括基础阶段和高级阶段。在基础阶段，RDN利用supportive pool中的所有proposal来度量在外观和几何信息上度量的关系特征。无论proposal的有效性如何，都将在此阶段对所有支持proposal的交互进行整体探讨。相反，在高级阶段，RDN会很好地选择具有高客观性得分的支持proposal，并且首先为这些proposal的特征赋予其与所有支持proposal的关系。然后，这些合并的特征反过来加强了参考帧中proposal的关系蒸馏。用目标关系增强过的每个proposal特征最终被用于proposal分类和回归。并且学习到的关系也有利于box-linking的后处理，RDN适用于任何基于region的视觉任务。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>HVR-Net</tag>
      
      <tag>SELSA</tag>
      
      <tag>RDN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（10.23)</title>
    <link href="/2020/10/23/10-23/"/>
    <url>/2020/10/23/10-23/</url>
    
    <content type="html"><![CDATA[<p>最近主要阅读了Flow-Related的三篇视频目标检测的相关论文：FGFA、DFF、MANet。其中，FGFA使用FlowNet计算帧间光流信息，根据光流信息将临近帧的特征图加权融合到参考帧上，然后利用整合后的信息做检测和分类。DFF对FGFA进行了改进，仅在稀疏的关键帧上运行卷积子网络，并且通过光流场将其深度特征图传播给其他帧。</p><p><strong>阅读文献：</strong></p><p>[1] Zhu X, Wang Y, Dai J, et al. Flow-guided feature aggregation for video object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 408-417.</p><p>[2] Zhu X, Xiong Y, Dai J, et al. Deep feature flow for video recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 2349-2358.</p><p>[3] Wang S, Zhou Y, Yan J, et al. Fully motion-aware network for video object detection[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 542-557.</p><h1 id="1-FGFA"><a href="#1-FGFA" class="headerlink" title="1.FGFA"></a>1.FGFA</h1><p>将用于图像的目标检测器应用于视频非常有挑战，视频中会出现运动模糊、失焦、和罕见的姿势等目标表观退化的情况。现有的一些方法在box level利用时间信息做出尝试，但其不是端到端的训练。本文采用光流指导的特征聚合，提出了一种featue-level的端到端学习的视频目标检测框架，提升了逐帧特征，可生成高质量的边界框。</p><h2 id="Flow-Guided-Feature-Aggregation-FGFA"><a href="#Flow-Guided-Feature-Aggregation-FGFA" class="headerlink" title="Flow-Guided Feature Aggregation(FGFA)"></a>Flow-Guided Feature Aggregation(FGFA)</h2><p>为每一帧图像应用特征提取网络，产生逐帧的特征图。使用FlowNet估计参考帧及其临近帧之间的运动，根据运动流将临近帧的特征图扭曲到参考帧。最后将扭曲的特征图、参考帧自己的特征图用自适应权重网络进行聚合，以增强参考帧的特征。之后将产生的聚合特征图喂给检测网络，来在参考帧上产生预测结果。</p><p>在特征传播、增强参考帧过程中有两个必要模块：1）运动指导的空间扭曲，根据帧与帧之间的运动扭曲特征图。2）特征聚合模块，解决怎样恰当的融合多帧特征。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5pe0cwj30ef0hc0xl.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="模型设计："><a href="#模型设计：" class="headerlink" title="模型设计："></a>模型设计：</h2><ul><li><strong>光流指导的扭曲</strong>：<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5rygshj30ch00yt8k.jpg" srcset="/img/loading.gif" alt="img">F是由FlowNet产生的光流场，Ii是参考帧，Ij是临近帧，W是双线性扭曲函数，作用于特征图每个通道的所有位置。</li><li><strong>特征聚合</strong>：在特征扭曲后，参考帧累积了多张特征图（包括其自己的），这些特征图提供了目标的不同信息（如变化的光照、视角、姿势、非刚体形变）。作者在不同的空间位置采用不同的权重，并使所有的特征通道共享相同的空间权重。聚合的特征图表示为<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5qiq1uj306i01mweb.jpg" srcset="/img/loading.gif" alt="img">其中，从j到i的二维权重图用wj-&gt;i表示。聚合后的特征图喂给检测子网络以获得检测结果：<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5ilbv9j303r00tmwy.jpg" srcset="/img/loading.gif" alt="img"></li><li><strong>自适应权重</strong>：自适应权重表示在每个空间位置上临近帧到参考帧的重要性。在位置p上，如果扭曲的特征图fj-&gt;i(p)与特征图fi(p)相似，就被分配一个大权重。否则就分配一个小权重。作者使用cos相似度来度量扭曲特征图和参考帧特征图之间的相似性。使用一个小的卷积神经网络：嵌入子网络来将fi和fi-&gt;j投影到一个新的embedding用于相似性度量。<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5rk5fpj308t01tq2t.jpg" srcset="/img/loading.gif" alt="img">使用以上公式估计权重，其中<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5myo6yj303500vmwx.jpg" srcset="/img/loading.gif" alt="img">是用于相似性度量的嵌入特征，wj-&gt;i在每个空间位置p在相邻帧进行正则化：<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5lxr8bj302100ejr5.jpg" srcset="/img/loading.gif" alt="img">。权重的估计也就是通过SoftMax操作计算嵌入特征之间cos相似度的过程。</li></ul><h2 id="网络结构："><a href="#网络结构：" class="headerlink" title="网络结构："></a>网络结构：</h2><p>  使用FlowNet来预测光流场，采用在ImageNet上预训练的ResNet(-50and-101)和Aligned-Inception-Resnet作为特征网络。一个随机初始化的嵌入网络，检测网络采用R-FCN和RPN。</p><h2 id="ImageNet-VID数据集："><a href="#ImageNet-VID数据集：" class="headerlink" title="ImageNet VID数据集："></a>ImageNet VID数据集：</h2><p>  是一个普遍用于视频目标检测的大规模基准数据集，训练集有3862个视频片段，验证集有555个视频片段。片段被完全标注，帧率为25或30fps，有30个目标类别。类别是ImageNet DET数据集的子集。</p><h2 id="评价标准："><a href="#评价标准：" class="headerlink" title="评价标准："></a>评价标准：</h2><p>  为了便于分析，将ground truth目标根据其运动速度进行分类。目标的速度定义为在其相邻的前后10帧上对应实例的平均IoU，称为“motion IoU”，motion IoU越小，说明目标移动速度越快。IoU&gt;0.9的为slow，0.7&lt;IoU&lt;0.9的为medium，IoU&lt;0.7的为fast。相对应的：沿用目标检测中的mAP，但是会根据目标的速度分为mAP(slow), mAP(medium), mAP(fast)。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5k48n1j30ed088glq.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h2><h4 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a>创新点：</h4><ul><li>使用FlowNet计算帧间光流信息，根据光流信息将临近帧的特征图加权融合到参考帧上。</li><li>利用前后帧光流信息增强当前参考帧的特征，从而提升识别精度。</li></ul><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>识别速度很慢。</li><li>物体运动速度越快，光流的效果就会越差。</li><li>仅融合与当前帧相邻的前后帧，有一定局限性，是否有更加全局的特征融合</li></ul><h1 id="2-Deep-Feature-Flow-for-video-Recognition"><a href="#2-Deep-Feature-Flow-for-video-Recognition" class="headerlink" title="2.Deep Feature Flow for video Recognition"></a>2.Deep Feature Flow for video Recognition</h1><p>  直接将图像识别方法应用在视频的每一帧上需要更大的计算量，并且更慢。作者提出了一种用于视频目标检测的快速且精确的框架：deep feature flow，该框架仅在稀疏的关键帧上运行卷积子网络，并且通过光流场将其深度特征图传播给其他帧。因为光流估计和特征传播比计算卷积特征更快，所以可以避免计算瓶颈，提升检测速度。</p><h2 id="Deep-Feature-Flow"><a href="#Deep-Feature-Flow" class="headerlink" title="Deep Feature Flow"></a>Deep Feature Flow</h2><p>  deep feature flow首次在一个深度学习框架中联合训练光流和识别任务。作者将卷积神经网络N分为两个连续的子网络Nfeat和Ntask:第一个称为特征子网络，输出若干中间特征图;第二个称为任务网络，对于任务有特定的结构，在特征图上执行识别任务（在执行目标检测任务时，采用R-FCN、RPN，使用了anchors）。</p><p>  连续的视频帧具有高度相似性，编码高级别语义概念的深度特征图的相似性甚至更高。作者正是利用这些相似性来减少计算开销。特征网络Nfeat仅仅在某些稀疏关键帧Ik上运行，非关键帧Ii的特征图来自于之前的关键帧Ik的传播。</p><h2 id="数学推导："><a href="#数学推导：" class="headerlink" title="数学推导："></a>数学推导：</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5jhk6oj307901gjr8.jpg" srcset="/img/loading.gif" alt="img"><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5omoe0j304e00n0si.jpg" srcset="/img/loading.gif" alt="img">是一个由光流估计算法获得的二维光流场。</p><p>  特征扭曲通过双线性插值实现：          (1) 其中c为特征图f的通道数，q枚举特征图上所有的空间位置，<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5j2v07j303k00r742.jpg" srcset="/img/loading.gif" alt="img">，G（·,·）是双线性插值核，G是二维的，可被分为两个一维的核：<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5ubrh2j30ay00u0sm.jpg" srcset="/img/loading.gif" alt="img">，其中<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5tveo0j306l00pq2r.jpg" srcset="/img/loading.gif" alt="img"></p><p>  由于光流估计的误差，空间扭曲有时不精确，故使用“scale field”Si-&gt;k来调整其振幅。最终，特征传播函数定义为：<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5q2riej305p00u0sj.jpg" srcset="/img/loading.gif" alt="img">(3) 其中，W在特征图的所有通道和所有位置应用了等式(1)，并按元素乘以Si-&gt;k。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5r3v7nj30cu0bf75g.jpg" srcset="/img/loading.gif" alt="img"> <img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5t4tztj30dt0artaj.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="训练过程："><a href="#训练过程：" class="headerlink" title="训练过程："></a>训练过程：</h2><p>使用CNN估计光流场和尺度场，所有的内容以任务为导向进行联合的端到端训练。网络结构如图b所示，使用随机梯度下降（SGD）训练，在每个mini-batch中随机采样Ik和Ii，i与k相隔不超过9帧。在前向传播过程中，首先在Ik上应用特征网络Nfeat，获得特征图fk。然后在Ik和Ii上运行光流网络F，预测光流场和尺度场。当i&gt;k时，按照等式(3)将特征图fk传播给fi。最终Ntask作用于fi来产生结果yi。通过在最后一个卷积层增加一些通道，作者在网络的输出加入了一个尺度函数S。尺度函数被初始化为1（输出层的权重初始化为0，偏差初始化为1）</p><h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a>总结：</h2><h4 id="创新点：-1"><a href="#创新点：-1" class="headerlink" title="创新点："></a>创新点：</h4><ul><li>利用轻量的光流网络计算帧与帧之间的关系，根据计算出的光流场将关键帧的特征传播至其他帧。无需对每一帧计算特征图，提升了检测速度。</li><li>传统的逐帧训练仅可使用标注过的帧进行训练，而DFF可以使用所有帧进行训练（只要参考帧被标注）。</li></ul><h4 id="缺点：-1"><a href="#缺点：-1" class="headerlink" title="缺点："></a>缺点：</h4><ul><li>传播后的特征会变弱，造成非关键帧的精度有一定损失。</li><li>使用固定的关键帧持续长度l，当图像内容急剧变化时，没有分配新的关键帧。</li></ul><h1 id="3-MANet"><a href="#3-MANet" class="headerlink" title="3.MANet"></a>3.MANet</h1><p>  为了增强视频每一帧的特征，此前的做法是聚合相邻帧的特征。但由于目标和摄像机的移动，目标的特征通常没有进行逐帧的空间校准。本文提出了一个端到端的模型：MANet，在<strong>像素级别</strong>和<strong>实例级别</strong>以一个统一的框架，联合校准目标特征。像素级校准可以灵活地对详细的运动进行建模，而实例级校准则可以捕获更多的全局运动线索，以便对遮挡具有鲁棒性。</p><p>  FGFA使用光流估计来预测逐像素的运动（以下称为像素级别的特征校准），当目标的表观剧烈变化时，尤其是目标被遮挡时，这种方法将会不精确。一旦光流估计不精确，光流指导的扭曲就会误导特征校准。</p><p>  本文在现有像素级别的方法上，提出了<strong>实例级别</strong>的校准方法。估计每个目标随时间的运动，以更加精确的聚合特征。具体地，为参考帧的每个proposal，提取其相应的运动特征来预测附近帧与当前帧之间的相对运动。根据预测的相对运动，将相邻帧中同一对象的特征进行RoI池化和合并，以更好地表示。对比像素级别的校准，实例级别的校准对较大的表观变化（如遮挡）更鲁棒。</p><p>  作者在观察的基础上，提出了一个<strong>运动模式推理模块</strong>：若某个运动模式更有可能是非刚体的，并且没有发生遮挡，最终结果将会更依赖像素级别的校准，否则就更依赖实例级别的校准。</p><h2 id="MANet结构："><a href="#MANet结构：" class="headerlink" title="MANet结构："></a>MANet结构：</h2><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5o0qszj30pg09awhn.jpg" srcset="/img/loading.gif" alt="img"></p><ul><li>首先，Nfeat特征提取器对三帧图像进行计算，产生三张中间特征图ft-τ，ft，ft+τ。</li><li>第二步，对ft-τ和ft+τ应用pixel-level的校准，生成ft-τ-&gt;t和ft+τ-&gt;t，被合并为fpixel，随后喂给Nrpn和Nrfcn产生proposal。fpixel也被喂给Nrfcn，对第i个proposal产生<strong>Sipixel</strong>。</li><li>第三步，在Nrfcn 位置敏感得分图上执行instance-level校准。在ft-τ，ft，ft+τ上应用专门的卷积层，产生一组k2个位置敏感的得分图St-τ，St，St+τ。对St的第i个proposal(xit,yit,wit,hit)。引入一个过程，回归相对应的St-τ的proposal位置(xit-τ,yit-τ,wit-τ,hit-τ)和St+τ的proposal位置(xit+τ,yit+τ,wit+τ,hit+τ)。通过这些预测出的proposal，附近帧的特征被RoI池化，合并为<strong>Siinsta</strong>。</li><li>最终，执行<strong>运动模式推理模块</strong>,将pixel-level的校准和instance-level的校准合并。运动模式推理模块根据动态的运动模式合并<strong>Siinsta</strong>和<strong>Sipixel</strong>得到综合的score map。</li></ul><p>以上所有模块（包括Nfeat,Nrpn,Nrfcn,pixel-level校准、instance-level校准）都被端到端的训练。</p><h2 id="Pixel-level-Calibration："><a href="#Pixel-level-Calibration：" class="headerlink" title="Pixel-level Calibration："></a>Pixel-level Calibration：</h2><p>  与FGFA、DFF相似，在此不再赘述。对非刚体建模更灵活(如tiny animals)</p><h2 id="Instance-level-Calibration："><a href="#Instance-level-Calibration：" class="headerlink" title="Instance-level Calibration："></a>Instance-level Calibration：</h2><p>  instance-level对常规运动轨迹更好(如汽车)，对遮挡的容忍度更高。实例级校准是在R-FCN的分数图上进行的。R-FCN使用专门的卷积层来生成位置敏感的得分图St。为了对第i个proposal合并出分数Sit,需要知道St-τ、St+τ和proposal位移。前两者可以通过将ft-τ和ft+τ喂给R-FCN获得，下面详细介绍如何学习到第i个proposal的相对位移。</p><p>  作者采用光流估计和参考帧的proposal作为输入，期望生成每一个proposal在临近帧和参考帧之间的位移。计算相对位移需要运动信息，尽管FlowNet预测的逐像素的运动由于遮挡可能不精确，但其有能力描述运动趋势。作者将这个运动趋势作为输入，输出整个目标的位移。首先使用RoI池化操作来生成池化过的特征mit-τ<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5ur2eaj30cc0150sm.jpg" srcset="/img/loading.gif" alt="img">（在（x,y,h,w）的第i个proposal），φ指RoI池化，F指光流估计。然后根据mit-τ，利用回归网络R(·)来估计第i个proposal在t-τ和t之间的相对位移。<img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5l57eaj30dd016jr9.jpg" srcset="/img/loading.gif" alt="img">其中R(·)是一个全连接层。根据估计到的相邻帧的proposal位置，合并后的第i个proposal的特征为Siinsta，其中Sj是相邻的分数图，ψ是位置敏感的池化层。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gjyx5hufxgj30cj01y0so.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a>总结：</h2><p>MANet利用R-FCN对FGFA进行了改进，在<strong>像素级别</strong>和<strong>实例级别</strong>联合校准目标特征。先根据提取出的feature和用FlowNet提取出帧间的光流信息，完成pixel-level的calibration。接着通过预测出来的instance的位移（其实就是R-FCN得到的proposal的位移），进行instance-level的calibration。最后使用Motion pattern reasoning model融合pixel-level和instance-level得到的feature用于训练和测试。</p><h4 id="创新点：-2"><a href="#创新点：-2" class="headerlink" title="创新点："></a><strong>创新点：</strong></h4><ul><li>在现有像素级别的方法上，提出了实例级别的校准方法，对遮挡更鲁棒。</li><li>提出了运动模式推理模块，根据运动，动态的联合像素级别和实例级别的校准。</li></ul><h4 id="缺点：-2"><a href="#缺点：-2" class="headerlink" title="缺点："></a><strong>缺点：</strong></h4><ul><li>实验部分仅对比了精度，速度没有提，可能会更慢一些。</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FGFA</tag>
      
      <tag>DFF</tag>
      
      <tag>MANet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>深度学习作业：MINIST多层感知机分类</title>
    <link href="/2020/10/19/minist/"/>
    <url>/2020/10/19/minist/</url>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1-Multi-Layer-Perceptron-with-MNIST-Dataset"><a href="#Assignment-1-Multi-Layer-Perceptron-with-MNIST-Dataset" class="headerlink" title="Assignment 1: Multi-Layer Perceptron with MNIST Dataset"></a>Assignment 1: Multi-Layer Perceptron with MNIST Dataset</h1><p>In this assignment, you are required to train two MLPs to classify images from the <a href="http://yann.lecun.com/exdb/mnist/" target="_blank" rel="noopener">MNIST database</a> hand-written digit database by using PyTorch.</p><p>The process will be broken down into the following steps:</p><ol><li>Load and visualize the data.</li><li>Define a neural network. (30 marks)</li><li>Train the models. (30 marks)</li><li>Evaluate the performance of our trained models on the test dataset. (20 marks)</li><li>Analysis your results. (20 marks)</li></ol><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">import</span> osos.environ[<span class="hljs-string">'KMP_DUPLICATE_LIB_OK'</span>]=<span class="hljs-string">'True'</span>  <span class="hljs-comment"># 解决macos下重复初始化libiomp5.dylib问题</span></code></pre><hr><h2 id="Load-and-Visualize-the-Data"><a href="#Load-and-Visualize-the-Data" class="headerlink" title="Load and Visualize the Data"></a>Load and Visualize the Data</h2><p>Downloading may take a few moments, and you should see your progress as the data is loading. You may also choose to change the <code>batch_size</code> if you want to load more data at a time.</p><p>This cell will create DataLoaders for each of our datasets.</p><pre><code class="hljs python"><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms<span class="hljs-comment"># number of subprocesses to use for data loading</span>num_workers = <span class="hljs-number">0</span><span class="hljs-comment"># how many samples per batch to load</span>batch_size = <span class="hljs-number">20</span><span class="hljs-comment"># convert data to torch.FloatTensor</span>transform = transforms.ToTensor()<span class="hljs-comment"># choose the training and test datasets</span>train_data = datasets.MNIST(root=<span class="hljs-string">'data'</span>, train=<span class="hljs-literal">True</span>,                                   download=<span class="hljs-literal">True</span>, transform=transform)test_data = datasets.MNIST(root=<span class="hljs-string">'data'</span>, train=<span class="hljs-literal">False</span>,                                  download=<span class="hljs-literal">True</span>, transform=transform)<span class="hljs-comment"># prepare data loaders</span>train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)</code></pre><h3 id="Visualize-a-Batch-of-Training-Data"><a href="#Visualize-a-Batch-of-Training-Data" class="headerlink" title="Visualize a Batch of Training Data"></a>Visualize a Batch of Training Data</h3><p>The first step in a classification task is to take a look at the data, make sure it is loaded in correctly, then make any initial observations about patterns in that data.</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt%matplotlib inline    <span class="hljs-comment"># obtain one batch of training images</span>dataiter = iter(train_loader)images, labels = dataiter.next()images = images.numpy()<span class="hljs-comment"># plot the images in the batch, along with the corresponding labels</span>fig = plt.figure(figsize=(<span class="hljs-number">25</span>, <span class="hljs-number">4</span>))<span class="hljs-keyword">for</span> idx <span class="hljs-keyword">in</span> np.arange(<span class="hljs-number">20</span>):    ax = fig.add_subplot(<span class="hljs-number">2</span>, <span class="hljs-number">20</span>/<span class="hljs-number">2</span>, idx+<span class="hljs-number">1</span>, xticks=[], yticks=[])    ax.imshow(np.squeeze(images[idx]), cmap=<span class="hljs-string">'gray'</span>)  <span class="hljs-comment"># np.squeeze():从数组的形状中删除单维度条目，把shape中为1的维度去掉</span>    <span class="hljs-comment"># print out the correct label for each image</span>    <span class="hljs-comment"># .item() gets the value contained in a Tensor</span>    ax.set_title(str(labels[idx].item()))</code></pre><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjv0pc9iptj312p06z3yp.jpg" srcset="/img/loading.gif" alt="png"></p><h3 id="View-an-Image-in-More-Detail"><a href="#View-an-Image-in-More-Detail" class="headerlink" title="View an Image in More Detail"></a>View an Image in More Detail</h3><pre><code class="hljs python">img = np.squeeze(images[<span class="hljs-number">1</span>])fig = plt.figure(figsize = (<span class="hljs-number">12</span>,<span class="hljs-number">12</span>)) ax = fig.add_subplot(<span class="hljs-number">111</span>)ax.imshow(img, cmap=<span class="hljs-string">'gray'</span>)width, height = img.shapethresh = img.max()/<span class="hljs-number">2.5</span><span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> range(width):    <span class="hljs-keyword">for</span> y <span class="hljs-keyword">in</span> range(height):        val = round(img[x][y],<span class="hljs-number">2</span>) <span class="hljs-keyword">if</span> img[x][y] !=<span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>  <span class="hljs-comment"># round：四舍五入2位</span>        ax.annotate(str(val), xy=(y,x),                    horizontalalignment=<span class="hljs-string">'center'</span>,                    verticalalignment=<span class="hljs-string">'center'</span>,                    color=<span class="hljs-string">'white'</span> <span class="hljs-keyword">if</span> img[x][y]&lt;thresh <span class="hljs-keyword">else</span> <span class="hljs-string">'black'</span>)</code></pre><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjv0pd3887j30j20iztag.jpg" srcset="/img/loading.gif" alt="png"></p><hr><h2 id="Define-the-Network-Architecture-30-marks"><a href="#Define-the-Network-Architecture-30-marks" class="headerlink" title="Define the Network Architecture (30 marks)"></a>Define the Network Architecture (30 marks)</h2><ul><li>Input: a 784-dim Tensor of pixel values for each image.</li><li>Output: a 10-dim Tensor of number of classes that indicates the class scores for an input image. </li></ul><p>You need to implement three models:</p><ol><li>a vanilla multi-layer perceptron. (10 marks)</li><li>a multi-layer perceptron with regularization (dropout or L2 or both). (10 marks)</li><li>the corresponding loss functions and optimizers. (10 marks)</li></ol><h3 id="Build-model-1"><a href="#Build-model-1" class="headerlink" title="Build model_1"></a>Build model_1</h3><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> optim<span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> inittorch.manual_seed(<span class="hljs-number">1</span>) <span class="hljs-comment"># 设置随机数种子</span><span class="hljs-comment">## Define the MLP architecture</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">VanillaMLP</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(VanillaMLP, self).__init__()        <span class="hljs-comment"># 手动实现全连接层</span><span class="hljs-comment">#         self.w1 = nn.Parameter(torch.randn(784,hidden_features))</span><span class="hljs-comment">#         self.b1 = nn.Parameter(torch.randn(hidden_features))</span><span class="hljs-comment">#         self.w2 = nn.Parameter(torch.randn(hidden_features,10))</span><span class="hljs-comment">#         self.b2 = nn.Parameter(torch.randn(10))</span><span class="hljs-comment">#         self.relu = nn.ReLU() ,nn.BatchNorm1d(1024)</span>                self.layer1 = nn.Sequential(nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">1024</span>),nn.ReLU())        self.layer2 = nn.Sequential(nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>),nn.ReLU())        self.layer3 = nn.Sequential(nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>),nn.ReLU())        self.layer4 = nn.Sequential(nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">100</span>),nn.ReLU())        self.layer5 = nn.Sequential(nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>))<span class="hljs-comment">#         init.xavier_normal_(self.layer1[0].weight)  # 初始化权重为正态分布（nn.Linear默认初始化为均匀分布）</span><span class="hljs-comment">#         init.xavier_normal_(self.layer2[0].weight)</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        <span class="hljs-comment"># flatten image input</span>        x = x.view(<span class="hljs-number">-1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>)<span class="hljs-comment"># 调用手动实现的全连接层</span><span class="hljs-comment">#         x = x.mm(self.w1)</span><span class="hljs-comment">#         h = x + self.b1.expand_as(x)</span><span class="hljs-comment">#         h = self.relu(h)  # 应用激活函数</span><span class="hljs-comment">#         h = h.mm(self.w2)</span><span class="hljs-comment">#         x = h + self.b2.expand_as(h)</span>        x = self.layer1(x)        x = self.layer2(x)        x = self.layer3(x)        x = self.layer4(x)        x = self.layer5(x)                <span class="hljs-keyword">return</span> x<span class="hljs-comment"># initialize the MLP</span>model_1 = VanillaMLP() <span class="hljs-comment"># specify loss function</span><span class="hljs-comment"># implement your codes here</span>loss1 = nn.CrossEntropyLoss()<span class="hljs-comment"># specify your optimizer</span><span class="hljs-comment"># implement your codes here</span>optimizer = optim.SGD(params=model_1.parameters(), lr=<span class="hljs-number">0.005</span>)optimizer.zero_grad()  <span class="hljs-comment"># 梯度清零</span>optimizer</code></pre><pre><code>SGD (Parameter Group 0    dampening: 0    lr: 0.005    momentum: 0    nesterov: False    weight_decay: 0)</code></pre><h3 id="Build-model-2"><a href="#Build-model-2" class="headerlink" title="Build model_2"></a>Build model_2</h3><pre><code class="hljs python"><span class="hljs-comment">## Define the MLP architecture</span><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">RegularizedMLP</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(RegularizedMLP, self).__init__()                <span class="hljs-comment"># implement your codes here</span>        <span class="hljs-comment">#self.layer1 = nn.Sequential(nn.Linear(784, hidden_features),nn.ReLU(),nn.Dropout(0.5))</span>        <span class="hljs-comment">#self.layer2 = nn.Sequential(nn.Linear(hidden_features, 10))</span>        self.layer1 = nn.Sequential(nn.Linear(<span class="hljs-number">784</span>, <span class="hljs-number">1024</span>),nn.ReLU(),nn.Dropout(<span class="hljs-number">0.5</span>))        self.layer2 = nn.Sequential(nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">512</span>),nn.ReLU(),nn.Dropout(<span class="hljs-number">0.5</span>))        self.layer3 = nn.Sequential(nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">256</span>),nn.ReLU(),nn.Dropout(<span class="hljs-number">0.5</span>))        self.layer4 = nn.Sequential(nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">100</span>),nn.ReLU(),nn.Dropout(<span class="hljs-number">0.5</span>))        self.layer5 = nn.Sequential(nn.Linear(<span class="hljs-number">100</span>, <span class="hljs-number">10</span>))            <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        <span class="hljs-comment"># flatten image input</span>        x = x.view(<span class="hljs-number">-1</span>, <span class="hljs-number">28</span> * <span class="hljs-number">28</span>)        <span class="hljs-comment"># implement your codes here</span>        x = self.layer1(x)        x = self.layer2(x)        x = self.layer3(x)        x = self.layer4(x)        x = self.layer5(x)                <span class="hljs-keyword">return</span> x<span class="hljs-comment"># initialize the MLP</span>model_2 = RegularizedMLP()<span class="hljs-comment"># specify loss function</span><span class="hljs-comment"># implement your codes here</span>loss2 = nn.CrossEntropyLoss()<span class="hljs-comment"># specify your optimizer</span><span class="hljs-comment"># implement your codes here</span>optimizer = optim.SGD(params=model_2.parameters(), lr=<span class="hljs-number">0.005</span>)optimizer.zero_grad()  <span class="hljs-comment"># 梯度清零</span>optimizer</code></pre><pre><code>SGD (Parameter Group 0    dampening: 0    lr: 0.005    momentum: 0    nesterov: False    weight_decay: 0)</code></pre><hr><h2 id="Train-the-Network-30-marks"><a href="#Train-the-Network-30-marks" class="headerlink" title="Train the Network (30 marks)"></a>Train the Network (30 marks)</h2><p>Train your models in the following two cells.</p><p>The following loop trains for 30 epochs; feel free to change this number. For now, we suggest somewhere between 20-50 epochs. As you train, take a look at how the values for the training loss decrease over time. We want it to decrease while also avoiding overfitting the training data. </p><p><strong>The key parts in the training process are left for you to implement.</strong></p><h3 id="Train-model-1"><a href="#Train-model-1" class="headerlink" title="Train model_1"></a>Train model_1</h3><pre><code class="hljs python"><span class="hljs-comment"># number of epochs to train the model</span>n_epochs = <span class="hljs-number">20</span>  <span class="hljs-comment"># suggest training between 20-50 epochs</span>model_1.train() <span class="hljs-comment"># prep model for training 将当前module及其子module中的所有training属性都设为True</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(n_epochs):    <span class="hljs-comment"># monitor training loss</span>    train_loss = <span class="hljs-number">0.0</span>    total_correct = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> train_loader:        y_pre = model_1(data)        l = loss1(y_pre, target).sum()        optimizer.zero_grad()        l.backward()        optimizer.step()  <span class="hljs-comment"># 执行优化</span>                <span class="hljs-comment"># implement your code here</span>        train_loss += l.item()<span class="hljs-comment"># the total loss of this batch</span>        total_correct += (y_pre.argmax(dim=<span class="hljs-number">1</span>) == target).sum().item() <span class="hljs-comment"># the accumulated number of correctly classified samples of this batch</span>            <span class="hljs-comment"># print training statistics </span>    <span class="hljs-comment"># calculate average loss and accuracy over an epoch</span>    train_loss = train_loss / len(train_loader.dataset)    train_acc = <span class="hljs-number">100.</span> * total_correct / len(train_loader.dataset)        print(<span class="hljs-string">'Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125; \tTraining Acc: &#123;:.2f&#125;%%'</span>.format(        epoch+<span class="hljs-number">1</span>,         train_loss,        train_acc        ))</code></pre><pre><code>Epoch: 1     Training Loss: 0.113190     Training Acc: 22.09%%Epoch: 2     Training Loss: 0.046626     Training Acc: 74.66%%Epoch: 3     Training Loss: 0.019871     Training Acc: 88.53%%Epoch: 4     Training Loss: 0.014905     Training Acc: 91.53%%Epoch: 5     Training Loss: 0.011858     Training Acc: 93.19%%Epoch: 6     Training Loss: 0.009501     Training Acc: 94.55%%Epoch: 7     Training Loss: 0.007791     Training Acc: 95.55%%Epoch: 8     Training Loss: 0.006547     Training Acc: 96.28%%Epoch: 9     Training Loss: 0.005593     Training Acc: 96.83%%Epoch: 10     Training Loss: 0.004822     Training Acc: 97.28%%Epoch: 11     Training Loss: 0.004186     Training Acc: 97.69%%Epoch: 12     Training Loss: 0.003650     Training Acc: 98.00%%Epoch: 13     Training Loss: 0.003196     Training Acc: 98.28%%Epoch: 14     Training Loss: 0.002803     Training Acc: 98.48%%Epoch: 15     Training Loss: 0.002459     Training Acc: 98.71%%Epoch: 16     Training Loss: 0.002155     Training Acc: 98.91%%Epoch: 17     Training Loss: 0.001881     Training Acc: 99.08%%Epoch: 18     Training Loss: 0.001634     Training Acc: 99.24%%Epoch: 19     Training Loss: 0.001413     Training Acc: 99.38%%Epoch: 20     Training Loss: 0.001215     Training Acc: 99.51%%</code></pre><h3 id="Train-model-2"><a href="#Train-model-2" class="headerlink" title="Train model_2"></a>Train model_2</h3><pre><code class="hljs python"><span class="hljs-comment"># number of epochs to train the model</span>n_epochs = <span class="hljs-number">30</span>  <span class="hljs-comment"># suggest training between 20-50 epochs</span>model_2.train() <span class="hljs-comment"># prep model for training</span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(n_epochs):    <span class="hljs-comment"># monitor training loss</span>    train_loss = <span class="hljs-number">0.0</span>    total_correct = <span class="hljs-number">0</span>        <span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> train_loader:        y_pre = model_2(data)        l = loss2(y_pre, target).sum()        optimizer.zero_grad()        l.backward()        optimizer.step()  <span class="hljs-comment"># 执行优化</span>                <span class="hljs-comment"># implement your code here</span>        train_loss += l.item()<span class="hljs-comment"># the total loss of this batch</span>        total_correct += (y_pre.argmax(dim=<span class="hljs-number">1</span>) == target).sum().item() <span class="hljs-comment"># the accumulated number of correctly classified samples of this batch</span>            <span class="hljs-comment"># print training statistics </span>    <span class="hljs-comment"># calculate average loss and accuracy over an epoch</span>    train_loss = train_loss / len(train_loader.dataset)    train_acc = <span class="hljs-number">100.</span> * total_correct / len(train_loader.dataset)        print(<span class="hljs-string">'Epoch: &#123;&#125; \tTraining Loss: &#123;:.6f&#125; \tTraining Acc: &#123;:.2f&#125;%%'</span>.format(        epoch+<span class="hljs-number">1</span>,         train_loss,        train_acc        ))</code></pre><pre><code>Epoch: 1     Training Loss: 0.114623     Training Acc: 14.26%%Epoch: 2     Training Loss: 0.089865     Training Acc: 37.94%%Epoch: 3     Training Loss: 0.047813     Training Acc: 66.14%%Epoch: 4     Training Loss: 0.033164     Training Acc: 79.35%%Epoch: 5     Training Loss: 0.024138     Training Acc: 86.14%%Epoch: 6     Training Loss: 0.018936     Training Acc: 89.40%%Epoch: 7     Training Loss: 0.015476     Training Acc: 91.53%%Epoch: 8     Training Loss: 0.013406     Training Acc: 92.89%%Epoch: 9     Training Loss: 0.011899     Training Acc: 93.57%%Epoch: 10     Training Loss: 0.010539     Training Acc: 94.33%%Epoch: 11     Training Loss: 0.009646     Training Acc: 94.91%%Epoch: 12     Training Loss: 0.008601     Training Acc: 95.45%%Epoch: 13     Training Loss: 0.008043     Training Acc: 95.69%%Epoch: 14     Training Loss: 0.007412     Training Acc: 96.11%%Epoch: 15     Training Loss: 0.006955     Training Acc: 96.30%%Epoch: 16     Training Loss: 0.006423     Training Acc: 96.61%%Epoch: 17     Training Loss: 0.006151     Training Acc: 96.65%%Epoch: 18     Training Loss: 0.005697     Training Acc: 96.97%%Epoch: 19     Training Loss: 0.005397     Training Acc: 97.13%%Epoch: 20     Training Loss: 0.005209     Training Acc: 97.30%%Epoch: 21     Training Loss: 0.004915     Training Acc: 97.39%%Epoch: 22     Training Loss: 0.004662     Training Acc: 97.48%%Epoch: 23     Training Loss: 0.004268     Training Acc: 97.67%%Epoch: 24     Training Loss: 0.004211     Training Acc: 97.69%%Epoch: 25     Training Loss: 0.004022     Training Acc: 97.78%%Epoch: 26     Training Loss: 0.003985     Training Acc: 97.87%%Epoch: 27     Training Loss: 0.003757     Training Acc: 97.95%%Epoch: 28     Training Loss: 0.003597     Training Acc: 98.03%%Epoch: 29     Training Loss: 0.003284     Training Acc: 98.17%%Epoch: 30     Training Loss: 0.003382     Training Acc: 98.12%%</code></pre><hr><h2 id="Test-the-Trained-Network-20-marks"><a href="#Test-the-Trained-Network-20-marks" class="headerlink" title="Test the Trained Network (20 marks)"></a>Test the Trained Network (20 marks)</h2><p>Test the performance of trained models on test data. Except the total test accuracy, you should calculate the accuracy for each class.</p><h3 id="Test-model-1"><a href="#Test-model-1" class="headerlink" title="Test model_1"></a>Test model_1</h3><pre><code class="hljs python"><span class="hljs-comment"># initialize lists to monitor test loss and accuracy</span>test_loss = <span class="hljs-number">0.0</span>class_correct = list(<span class="hljs-number">0.</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>))class_total = list(<span class="hljs-number">0.</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>))model_1.eval() <span class="hljs-comment"># prep model for *evaluation*  #把training属性都设为False,固定BN和dropout层</span><span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:    y_pre = model_1(data)    l = loss1(y_pre, target).sum()    <span class="hljs-comment"># implement your code here</span>        test_loss += l.item()<span class="hljs-comment"># the total loss of this batch</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(batch_size):        <span class="hljs-keyword">if</span> y_pre.argmax(dim=<span class="hljs-number">1</span>)[i] == target[i]: <span class="hljs-comment"># 预测正确</span>            class_correct[target[i]] += <span class="hljs-number">1</span>  <span class="hljs-comment"># the list of number of correctly classified samples of each class of this batch. label is the index.</span>        class_total[target[i]] +=<span class="hljs-number">1</span>  <span class="hljs-comment"># the list of total number of samples of each class of this batch. label is the index.</span>        <span class="hljs-comment"># calculate and print avg test loss</span>test_loss = test_loss / len(test_loader.dataset)print(<span class="hljs-string">'Test Loss: &#123;:.6f&#125;\n'</span>.format(test_loss))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):    <span class="hljs-keyword">if</span> class_total[i] &gt; <span class="hljs-number">0</span>:        print(<span class="hljs-string">'Test Accuracy of class %d: %.2f%%'</span> % (i, <span class="hljs-number">100</span> * class_correct[i] / class_total[i]))    <span class="hljs-keyword">else</span>:        print(<span class="hljs-string">'Test Accuracy of class %d: N/A (no training examples)'</span> % (i))print(<span class="hljs-string">'\nTest Accuracy (Overall): %.2f%%'</span> % (<span class="hljs-number">100.</span> * np.sum(class_correct) / np.sum(class_total)))</code></pre><pre><code>Test Loss: 0.004451Test Accuracy of class 0: 98.67%Test Accuracy of class 1: 98.77%Test Accuracy of class 2: 97.19%Test Accuracy of class 3: 98.61%Test Accuracy of class 4: 98.47%Test Accuracy of class 5: 96.64%Test Accuracy of class 6: 96.45%Test Accuracy of class 7: 96.11%Test Accuracy of class 8: 96.20%Test Accuracy of class 9: 97.52%Test Accuracy (Overall): 97.49%</code></pre><h3 id="Test-model-2"><a href="#Test-model-2" class="headerlink" title="Test model_2"></a>Test model_2</h3><pre><code class="hljs python"><span class="hljs-comment"># initialize lists to monitor test loss and accuracy</span>test_loss = <span class="hljs-number">0.0</span>class_correct = list(<span class="hljs-number">0.</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>))class_total = list(<span class="hljs-number">0.</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>))model_2.eval() <span class="hljs-comment"># prep model for *evaluation*</span><span class="hljs-keyword">for</span> data, target <span class="hljs-keyword">in</span> test_loader:    y_pre = model_2(data)    l = loss2(y_pre, target).sum()    <span class="hljs-comment"># implement your code here</span>        test_loss += l.item()<span class="hljs-comment"># the total loss of this batch</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(batch_size):        <span class="hljs-keyword">if</span> y_pre.argmax(dim=<span class="hljs-number">1</span>)[i] == target[i]: <span class="hljs-comment"># 预测正确</span>            class_correct[target[i]] += <span class="hljs-number">1</span>  <span class="hljs-comment"># the list of number of correctly classified samples of each class of this batch. label is the index.</span>        class_total[target[i]] +=<span class="hljs-number">1</span>  <span class="hljs-comment"># the list of total number of samples of each class of this batch. label is the index.</span><span class="hljs-comment"># calculate and print avg test loss</span>test_loss = test_loss / len(test_loader.dataset)print(<span class="hljs-string">'Test Loss: &#123;:.6f&#125;\n'</span>.format(test_loss))<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>):    <span class="hljs-keyword">if</span> class_total[i] &gt; <span class="hljs-number">0</span>:        print(<span class="hljs-string">'Test Accuracy of class %d: %.2f%%'</span> % (i, <span class="hljs-number">100</span> * class_correct[i] / class_total[i]))    <span class="hljs-keyword">else</span>:        print(<span class="hljs-string">'Test Accuracy of class %d: N/A (no training examples)'</span> % (i))print(<span class="hljs-string">'\nTest Accuracy (Overall): %.2f%%'</span> % (<span class="hljs-number">100.</span> * np.sum(class_correct) / np.sum(class_total)))</code></pre><pre><code>Test Loss: 0.003905Test Accuracy of class 0: 98.98%Test Accuracy of class 1: 99.47%Test Accuracy of class 2: 98.16%Test Accuracy of class 3: 98.42%Test Accuracy of class 4: 98.17%Test Accuracy of class 5: 97.98%Test Accuracy of class 6: 97.60%Test Accuracy of class 7: 97.86%Test Accuracy of class 8: 97.02%Test Accuracy of class 9: 96.93%Test Accuracy (Overall): 98.08%</code></pre><hr><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><div class="table-container"><table><thead><tr><th style="text-align:center">num</th><th style="text-align:center">epoch</th><th style="text-align:center">hidden_neural</th><th style="text-align:center">lr</th><th style="text-align:center">TrainLoss</th><th style="text-align:center">TrainAcc</th><th style="text-align:center">TestAcc</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.016939</td><td style="text-align:center">90.97</td><td style="text-align:center">86.76</td><td style="text-align:center">手动实现的全连接层</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.008375</td><td style="text-align:center">95.4</td><td style="text-align:center">93.9</td><td style="text-align:center">手动实现+Relu</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.004658</td><td style="text-align:center">97.2</td><td style="text-align:center">94.59</td><td style="text-align:center">使用nn.Linear</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.005426</td><td style="text-align:center">96.72</td><td style="text-align:center">95.54</td><td style="text-align:center">加入BatchNorm层</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.005731</td><td style="text-align:center">96.68</td><td style="text-align:center">95.19</td><td style="text-align:center">加入均匀初始化</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.033589</td><td style="text-align:center">77.92</td><td style="text-align:center">92.08</td><td style="text-align:center">Dropout+BN</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.2</td><td style="text-align:center">0.034423</td><td style="text-align:center">77.33</td><td style="text-align:center">91.96</td><td style="text-align:center">仅Dropout</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.1</td><td style="text-align:center">0.003467</td><td style="text-align:center">97.92</td><td style="text-align:center">95.36</td><td style="text-align:center">无Dropout</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">30</td><td style="text-align:center">20</td><td style="text-align:center">0.1</td><td style="text-align:center">0.030022</td><td style="text-align:center">79.61</td><td style="text-align:center">92.57</td><td style="text-align:center">仅Dropout</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">40</td><td style="text-align:center">20</td><td style="text-align:center">0.01</td><td style="text-align:center">0.006123</td><td style="text-align:center">96.46</td><td style="text-align:center">95.53</td><td style="text-align:center">无Dropout</td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">40</td><td style="text-align:center">20</td><td style="text-align:center">0.01</td><td style="text-align:center">0.028218</td><td style="text-align:center">81.17</td><td style="text-align:center">93.53</td><td style="text-align:center">有Dropout</td></tr><tr><td style="text-align:center">12</td><td style="text-align:center">40</td><td style="text-align:center">50</td><td style="text-align:center">0.01</td><td style="text-align:center">0.002915</td><td style="text-align:center">98.41</td><td style="text-align:center">97.31</td><td style="text-align:center">无Dropout</td></tr><tr><td style="text-align:center">13</td><td style="text-align:center">40</td><td style="text-align:center">50</td><td style="text-align:center">0.01</td><td style="text-align:center">0.012396</td><td style="text-align:center">92.38</td><td style="text-align:center">96.16</td><td style="text-align:center">有Dropout</td></tr><tr><td style="text-align:center">14</td><td style="text-align:center">40</td><td style="text-align:center">50</td><td style="text-align:center">0.005</td><td style="text-align:center">0.005171</td><td style="text-align:center">97.09</td><td style="text-align:center">96.53</td><td style="text-align:center">无Dropout</td></tr><tr><td style="text-align:center">15</td><td style="text-align:center">40</td><td style="text-align:center">50</td><td style="text-align:center">0.005</td><td style="text-align:center">0.014154</td><td style="text-align:center">91.52</td><td style="text-align:center">95.48</td><td style="text-align:center">有Dropout</td></tr><tr><td style="text-align:center">16</td><td style="text-align:center">50</td><td style="text-align:center">50</td><td style="text-align:center">0.005</td><td style="text-align:center">0.004808</td><td style="text-align:center">97.62</td><td style="text-align:center">96.97</td><td style="text-align:center">无Dropout</td></tr><tr><td style="text-align:center">17</td><td style="text-align:center">50</td><td style="text-align:center">50</td><td style="text-align:center">0.005</td><td style="text-align:center">0.01167</td><td style="text-align:center">92.77</td><td style="text-align:center">96.21</td><td style="text-align:center">有Dropout</td></tr><tr><td style="text-align:center">18</td><td style="text-align:center">20</td><td style="text-align:center">1024/512/256/100</td><td style="text-align:center">0.005</td><td style="text-align:center">0.000003</td><td style="text-align:center">100</td><td style="text-align:center">98.55</td><td style="text-align:center">有BN，无Dropout</td></tr><tr><td style="text-align:center">19</td><td style="text-align:center">20</td><td style="text-align:center">1024/512/256/100</td><td style="text-align:center">0.005</td><td style="text-align:center">0.005303</td><td style="text-align:center">97.16</td><td style="text-align:center">97.71</td><td style="text-align:center">无BN，有Dropout</td></tr><tr><td style="text-align:center"><strong>20</strong></td><td style="text-align:center"><strong>20</strong></td><td style="text-align:center"><strong>1024/512/256/100</strong></td><td style="text-align:center"><strong>0.005</strong></td><td style="text-align:center"><strong>0.001215</strong></td><td style="text-align:center"><strong>99.51</strong></td><td style="text-align:center"><strong>97.49</strong></td><td style="text-align:center"><strong>仅Relu</strong></td></tr><tr><td style="text-align:center"><strong>21</strong></td><td style="text-align:center"><strong>30</strong></td><td style="text-align:center"><strong>1024/512/256/100</strong></td><td style="text-align:center"><strong>0.005</strong></td><td style="text-align:center"><strong>0.003382</strong></td><td style="text-align:center"><strong>98.12</strong></td><td style="text-align:center"><strong>98.08</strong></td><td style="text-align:center"><strong>有Dropout</strong></td></tr></tbody></table></div><hr><h2 id="Analyze-Your-Result-20-marks"><a href="#Analyze-Your-Result-20-marks" class="headerlink" title="Analyze Your Result (20 marks)"></a>Analyze Your Result (20 marks)</h2><p>Compare the performance of your models with the following analysis. Both English and Chinese answers are acceptable.</p><p>1.Does your vanilla MLP overfit to the training data? (5 marks)</p><p>Answer: Yes.</p><p>2.If yes, how do you observe it? If no, why? (5 marks)</p><p>Answer: The accuracy during training is close to 100%.<br>      But the accuracy on the test set is relatively low.</p><p>3.Is regularized model help prevent overfitting? (5 marks)</p><p>Answer: When I used Dropout, the training accuracy dropped, and the accuracy of the test improved by 5% compared to the inapplicable Dropout.The regularized model effectively avoids overfitting.</p><p>4.Generally compare the performance of two models. (5 marks)</p><p>Answer:<br>    The vanilla model converges quickly and the training accuracy is close to 100%.The model using regularization converges relatively slowly, but overfitting is avoided.<br>    In addition, I found that using both Dropout and BatchNormal did not work as well as expected.</p><h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p><a href="https://blog.csdn.net/lxslx/article/details/81746556" target="_blank" rel="noopener">https://blog.csdn.net/lxslx/article/details/81746556</a></p><p><a href="https://blog.csdn.net/zjh12312311/article/details/107217024/" target="_blank" rel="noopener">https://blog.csdn.net/zjh12312311/article/details/107217024/</a></p><p><a href="https://www.zhihu.com/question/68433311" target="_blank" rel="noopener">https://www.zhihu.com/question/68433311</a></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
      <tag>MINIST</tag>
      
      <tag>DeepLearning</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算智能作业:用遗传算法解决旅行商问题</title>
    <link href="/2020/10/19/GA_TSP/"/>
    <url>/2020/10/19/GA_TSP/</url>
    
    <content type="html"><![CDATA[<h1 id="1-问题描述："><a href="#1-问题描述：" class="headerlink" title="1.问题描述："></a><strong>1.问题描述：</strong></h1><p>编写一个程序，使用遗传算法求解旅行商问题：</p><p>旅行商问题(Travelling Salesman Problem, 简记TSP，亦称货郎担问题):设有n个城市和距离矩阵D=[dij]，其中dij表示 城市i到城市j的距离，i，j=1，2 … n，问题是要找出遍访每个城市恰好一次的一条回路并使其路径长度为最短。</p><h1 id="2-问题定义："><a href="#2-问题定义：" class="headerlink" title="2.问题定义："></a><strong>2.问题定义：</strong></h1><ol><li><strong>定义个体、种群：</strong>每一条可能的路径为一个个体、所有个体构成种群。</li><li><strong>定义适应度：</strong>计算每个个体的路径长度，以路径长度的倒数作为适应度。（即路径越长，适应度越差）</li><li><strong>定义杂交方法</strong>：若产生的随机数大于交叉概率，则直接将父本保留，否则进行交叉。随机选择父本2的一个片段，遍历父本1：若该城市号码不在父本2中，则将其插入到交叉结果，若到达交叉点1则将父本2的片段插入交叉结果。</li><li><strong>定义选择方法：</strong>每次选择适应度最低的个体，将其杀死。</li><li><strong>定义突变方法：</strong>若产生的随机数大于突变概率，则不进行突变。否则采用倒置变异法，随机选择该基因的某一片段进行逆序。</li></ol><h1 id="3-算法流程："><a href="#3-算法流程：" class="headerlink" title="3.算法流程："></a><strong>3.算法流程：</strong></h1><ol><li>确定种群规模N、杂交概率Pc、变异概率Pm、适应度函数F、终止条件。</li><li>产生初始种群，计算种群中每个个体的适应度。</li><li>开始进行进化操作：<ul><li>使用选择方法杀死种群中适应度较低的若干个体。（物竞天择，适者生存）</li><li>开始繁殖：选择适应度最高的个体作为父本1，随机选择一个个体作为父本2。使其进行杂交，产生新个体。直到当前个体数量恢复到原始种群数量。</li><li>使新产生的种群按突变概率进行突变。</li></ul></li><li>重复执行进化操作，直到达到迭代次数时，输出最优个体。</li></ol><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjujjhov2wj311w0ouwh5.jpg" srcset="/img/loading.gif" alt="image-20201019124155656"></p><h1 id="4-源代码："><a href="#4-源代码：" class="headerlink" title="4.源代码："></a>4.源代码：</h1><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<span class="hljs-keyword">import</span> random<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<span class="hljs-keyword">import</span> sys<span class="hljs-keyword">import</span> copy<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GA</span><span class="hljs-params">(object)</span>:</span>    <span class="hljs-comment"># 定义类属性</span>    <span class="hljs-comment"># 输入全国34个省、市、自治区直辖市的省会城市坐标</span>    city_x = [<span class="hljs-number">86.61</span>, <span class="hljs-number">89.13</span>, <span class="hljs-number">127.34</span>, <span class="hljs-number">126.32</span>, <span class="hljs-number">123.42</span>, <span class="hljs-number">112.17</span>, <span class="hljs-number">116.40</span>, <span class="hljs-number">106.27</span>, <span class="hljs-number">111.98</span>, <span class="hljs-number">115.21</span>, <span class="hljs-number">117.04</span>, <span class="hljs-number">97.07</span>, <span class="hljs-number">103.82</span>,              <span class="hljs-number">118.01</span>, <span class="hljs-number">108.94</span>, <span class="hljs-number">113.46</span>, <span class="hljs-number">117.28</span>, <span class="hljs-number">120.26</span>, <span class="hljs-number">121.46</span>, <span class="hljs-number">103.36</span>, <span class="hljs-number">112.29</span>, <span class="hljs-number">120.15</span>, <span class="hljs-number">107.51</span>, <span class="hljs-number">112.08</span>, <span class="hljs-number">115.89</span>, <span class="hljs-number">106.91</span>,              <span class="hljs-number">120.31</span>, <span class="hljs-number">101.71</span>, <span class="hljs-number">123.01</span>, <span class="hljs-number">108.67</span>, <span class="hljs-number">113.98</span>, <span class="hljs-number">110.03</span>, <span class="hljs-number">113.54</span>, <span class="hljs-number">114.17</span>]    city_y = [<span class="hljs-number">45.79</span>, <span class="hljs-number">30.66</span>, <span class="hljs-number">48.05</span>, <span class="hljs-number">44.38</span>, <span class="hljs-number">42.29</span>, <span class="hljs-number">42.81</span>, <span class="hljs-number">40.40</span>, <span class="hljs-number">36.76</span>, <span class="hljs-number">37.65</span>, <span class="hljs-number">38.44</span>, <span class="hljs-number">39.52</span>, <span class="hljs-number">35.62</span>, <span class="hljs-number">36.05</span>, <span class="hljs-number">36.37</span>,              <span class="hljs-number">34.46</span>, <span class="hljs-number">34.25</span>, <span class="hljs-number">31.86</span>, <span class="hljs-number">32.54</span>, <span class="hljs-number">31.28</span>, <span class="hljs-number">30.65</span>, <span class="hljs-number">30.98</span>, <span class="hljs-number">29.28</span>, <span class="hljs-number">29.63</span>, <span class="hljs-number">27.79</span>, <span class="hljs-number">27.97</span>, <span class="hljs-number">26.67</span>, <span class="hljs-number">26.07</span>, <span class="hljs-number">24.84</span>, <span class="hljs-number">23.54</span>,              <span class="hljs-number">22.68</span>, <span class="hljs-number">22.82</span>, <span class="hljs-number">19.33</span>, <span class="hljs-number">22.19</span>, <span class="hljs-number">22.32</span>]    city = [<span class="hljs-string">'乌鲁木齐'</span>, <span class="hljs-string">'拉萨'</span>, <span class="hljs-string">'哈尔滨'</span>, <span class="hljs-string">'长春'</span>, <span class="hljs-string">'沈阳'</span>, <span class="hljs-string">'呼和浩特'</span>, <span class="hljs-string">'北京'</span>, <span class="hljs-string">'银川'</span>, <span class="hljs-string">'太原'</span>, <span class="hljs-string">'石家庄'</span>, <span class="hljs-string">'天津'</span>, <span class="hljs-string">'西宁'</span>,            <span class="hljs-string">'兰州'</span>, <span class="hljs-string">'济南'</span>, <span class="hljs-string">'西安'</span>, <span class="hljs-string">'郑州'</span>, <span class="hljs-string">'合肥'</span>, <span class="hljs-string">'南京'</span>, <span class="hljs-string">'上海'</span>, <span class="hljs-string">'成都'</span>, <span class="hljs-string">'武汉'</span>, <span class="hljs-string">'杭州'</span>, <span class="hljs-string">'重庆'</span>, <span class="hljs-string">'长沙'</span>, <span class="hljs-string">'南昌'</span>,            <span class="hljs-string">'贵阳'</span>, <span class="hljs-string">'福州'</span>, <span class="hljs-string">'昆明'</span>, <span class="hljs-string">'台北'</span>, <span class="hljs-string">'南宁'</span>, <span class="hljs-string">'广州'</span>, <span class="hljs-string">'海口'</span>, <span class="hljs-string">'澳门'</span>, <span class="hljs-string">'香港'</span>]    city_num = <span class="hljs-number">34</span>  <span class="hljs-comment"># 城市数量</span>    max_population_size = <span class="hljs-number">100</span>    population_size = max_population_size  <span class="hljs-comment"># 种群大小</span>    killnum = <span class="hljs-number">40</span>  <span class="hljs-comment"># 每次迭代杀死多少个个体</span>    crossover_rate = <span class="hljs-number">0.8</span>  <span class="hljs-comment"># 交叉概率</span>    mutation_rate = <span class="hljs-number">0.05</span>  <span class="hljs-comment"># 突变概率</span>    iter = <span class="hljs-number">500</span>  <span class="hljs-comment"># 迭代次数</span>    best_dist = <span class="hljs-number">0</span>  <span class="hljs-comment"># 最优距离</span>    best_route = []  <span class="hljs-comment"># 最优路线</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-comment"># 定义实例属性：</span>        self.population = np.array([])  <span class="hljs-comment"># 种群数组</span>        self.fitness = np.array([])  <span class="hljs-comment"># 适应度数组</span>        self.population = self.createpop(self.population_size)  <span class="hljs-comment"># 创建初始种群</span>        self.fitness = self.get_fitness(self.population)  <span class="hljs-comment"># 计算初始化种群的适应度</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">createpop</span><span class="hljs-params">(self, size)</span>:</span>  <span class="hljs-comment"># 创建种群</span>        pop = []        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(size):            route = random.sample(range(<span class="hljs-number">0</span>, self.city_num), self.city_num)  <span class="hljs-comment">#从0-n截取n个数据（得到初始随机路径）</span>            pop.append(route)        <span class="hljs-keyword">return</span> np.array(pop)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_fitness</span><span class="hljs-params">(self, pop)</span>:</span>        fitness = np.array([])        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.population.shape[<span class="hljs-number">0</span>]):            d = self.get_distance(pop[i])  <span class="hljs-comment"># 取一个个体，计算该个体的距离</span>            fitness = np.append(fitness, <span class="hljs-number">1</span>/d)  <span class="hljs-comment"># np.array追加元素  以距离的倒数为适应度</span>        <span class="hljs-keyword">return</span> fitness    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_distance</span><span class="hljs-params">(self, route)</span>:</span>  <span class="hljs-comment"># 计算当前路径的长度</span>        d = <span class="hljs-number">0.0</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">-1</span>, self.city_num<span class="hljs-number">-1</span>):            x = (self.city_x[route[i]]-self.city_x[route[i+<span class="hljs-number">1</span>]])**<span class="hljs-number">2</span>            y = (self.city_y[route[i]]-self.city_y[route[i+<span class="hljs-number">1</span>]])**<span class="hljs-number">2</span>            d += np.sqrt(x+y)        <span class="hljs-keyword">return</span> d    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">kill2</span><span class="hljs-params">(self)</span>:</span>        worst_f_index = np.argmin(self.fitness)  <span class="hljs-comment"># 当前最差适应值的位置</span>        self.population = np.delete(self.population, worst_f_index, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 杀死若干个体</span>        self.population_size = self.population.shape[<span class="hljs-number">0</span>]        self.fitness = self.get_fitness(self.population)  <span class="hljs-comment"># 更新适应度</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">breed</span><span class="hljs-params">(self)</span>:</span>        <span class="hljs-keyword">while</span> self.population_size &lt; self.max_population_size:  <span class="hljs-comment"># 当前个体数小于种群数量时,进行繁衍</span>            best_f_index = np.argmax(self.fitness)  <span class="hljs-comment"># 当前最好适应值的位置</span>            p1 = self.population[best_f_index]  <span class="hljs-comment"># 选适应值最高的作为父本1</span>            p2 = self.population[np.random.randint(<span class="hljs-number">0</span>, self.population.shape[<span class="hljs-number">0</span>]<span class="hljs-number">-1</span>)]  <span class="hljs-comment"># 随机选父本2</span>            c = self.cross(p1, p2)            c = c[np.newaxis, :]  <span class="hljs-comment"># 将（34， 转换为（1,34）</span>            <span class="hljs-comment"># print('交叉成功：',c)</span>            <span class="hljs-comment"># print('shape',self.population.shape,c.shape)</span>            self.population = np.append(self.population, c, axis=<span class="hljs-number">0</span>)  <span class="hljs-comment"># 进行交叉，追加到种群</span>            self.fitness = self.get_fitness(self.population)  <span class="hljs-comment"># 更新适应度</span>            self.population_size = self.population.shape[<span class="hljs-number">0</span>]  <span class="hljs-comment"># 更新种群大小</span>            <span class="hljs-comment"># print(self.fitness.shape)</span>            <span class="hljs-comment"># print(self.fitness)</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">cross</span><span class="hljs-params">(self, parent1, parent2)</span>:</span>  <span class="hljs-comment"># 交叉p1和p2的片段</span>        <span class="hljs-keyword">if</span> np.random.rand() &gt; self.crossover_rate:  <span class="hljs-comment"># 大于交叉概率0.7直接替换</span>            <span class="hljs-keyword">return</span> parent1        <span class="hljs-keyword">else</span>:            index1 = np.random.randint(<span class="hljs-number">0</span>, self.city_num<span class="hljs-number">-1</span>) <span class="hljs-comment">#随机产生位置1 注意：randint可以取到第二个参数的位置</span>            index2 = np.random.randint(index1, self.city_num<span class="hljs-number">-1</span>)  <span class="hljs-comment"># 随机产生位置1之后的位置2</span>            tempGene = parent2[index1:index2]  <span class="hljs-comment"># 选取交叉的基因片段</span>            newGene = []            p1len = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> parent1:                <span class="hljs-keyword">if</span> p1len == index1:  <span class="hljs-comment"># 如果当前位置到达交叉点1</span>                    newGene.extend(tempGene)  <span class="hljs-comment"># 插入基因片段（将p2随机选取的一段作为新基因的开头）</span>                <span class="hljs-keyword">if</span> g <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> tempGene:                    newGene.append(g)  <span class="hljs-comment"># 若g没有在随机选的基因段里，则插入g</span>                p1len += <span class="hljs-number">1</span>            newGene = np.array(newGene)            <span class="hljs-keyword">if</span> newGene.shape[<span class="hljs-number">0</span>] != self.city_num:                print(<span class="hljs-string">'交叉错误'</span>)                <span class="hljs-keyword">return</span> self.createpop(<span class="hljs-number">1</span>)                <span class="hljs-comment"># return parent1</span>            <span class="hljs-keyword">return</span> newGene    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mutate</span><span class="hljs-params">(self, gene)</span>:</span>  <span class="hljs-comment"># 突变</span>        <span class="hljs-keyword">if</span> np.random.rand() &gt; self.mutation_rate:  <span class="hljs-comment"># 大于突变概率0.005（大部分）</span>            <span class="hljs-keyword">return</span> gene        <span class="hljs-keyword">else</span>:            index1 = np.random.randint(<span class="hljs-number">0</span>, self.city_num<span class="hljs-number">-1</span>)            index2 = np.random.randint(index1, self.city_num<span class="hljs-number">-1</span>)            newgene = self.reverse(gene, index1, index2)  <span class="hljs-comment"># 倒置变异法</span>            <span class="hljs-keyword">if</span> newgene.shape[<span class="hljs-number">0</span>] != self.city_num:                print(<span class="hljs-string">"突变错误！"</span>)                <span class="hljs-keyword">return</span> self.createpop(<span class="hljs-number">1</span>)            <span class="hljs-keyword">return</span> newgene    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reverse</span><span class="hljs-params">(self, gene, i, j)</span>:</span>  <span class="hljs-comment"># 翻转i-j之间的片段</span>        <span class="hljs-keyword">if</span> i &gt; j <span class="hljs-keyword">or</span> j &gt; self.city_num<span class="hljs-number">-1</span>:            print(<span class="hljs-string">"error"</span>)            <span class="hljs-keyword">return</span> gene        <span class="hljs-keyword">else</span>:            gg = np.copy(gene)            temp = gg[i:j]  <span class="hljs-comment"># 截取基因片段</span>            new = []            n = <span class="hljs-number">0</span>            <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> gg:                <span class="hljs-keyword">if</span> n == i:  <span class="hljs-comment"># 到达第i个位置</span>                    new.extend(temp[::<span class="hljs-number">-1</span>])  <span class="hljs-comment"># 插入逆序基因片段</span>                <span class="hljs-keyword">if</span> g <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> temp:  <span class="hljs-comment"># 跳过已存在的基因片段（发生在插入基因片段之后的位置）</span>                    new.append(g)                n += <span class="hljs-number">1</span>        <span class="hljs-keyword">return</span> np.array(new)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evolution</span><span class="hljs-params">(self)</span>:</span>  <span class="hljs-comment"># 进化</span>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.iter):  <span class="hljs-comment"># 迭代次数</span>            <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> range(self.killnum):  <span class="hljs-comment"># 杀死后20个个体</span>                self.kill2()  <span class="hljs-comment"># 物竞天择，适者生存，杀死一些较差的个体</span>            <span class="hljs-comment"># print("杀死完个体数", self.population_size)</span>            self.breed()  <span class="hljs-comment"># 开始繁殖  繁殖到达种群数量即停止</span>            <span class="hljs-comment"># print("繁殖完个体数", self.population_size)</span>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(self.population_size):                self.population[j] = self.mutate(self.population[j])  <span class="hljs-comment"># 突变</span>            best_f_index = np.argmax(self.fitness)  <span class="hljs-comment"># 当前最好适应值的位置</span>            self.best_route = self.population[best_f_index]            self.best_dist = self.get_distance(self.best_route)            print(<span class="hljs-string">'迭代次数：%d,最优距离：%s'</span> % (i, self.best_dist))        self.draw(self.best_route)        print(<span class="hljs-string">"最佳路线："</span>, [self.city[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> self.best_route])    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">draw</span><span class="hljs-params">(self, best)</span>:</span>  <span class="hljs-comment"># 绘图方法</span>        result_x = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> range(self.city_num + <span class="hljs-number">1</span>)]        result_y = [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> col <span class="hljs-keyword">in</span> range(self.city_num + <span class="hljs-number">1</span>)]        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(self.city_num):            result_x[i] = self.city_x[best[i]] * <span class="hljs-number">8</span> - <span class="hljs-number">575</span>            result_y[i] = (<span class="hljs-number">376</span> - self.city_y[best[i]] * <span class="hljs-number">8</span> + <span class="hljs-number">65</span>) * <span class="hljs-number">1.2</span>        result_x[self.city_num] = result_x[<span class="hljs-number">0</span>]        result_y[self.city_num] = result_y[<span class="hljs-number">0</span>]        <span class="hljs-comment"># print(result_x)</span>        <span class="hljs-comment"># print(result_y)</span>        img = Image.open(<span class="hljs-string">"./map.jpg"</span>)        plt.figure(<span class="hljs-string">"map"</span>)        plt.imshow(img)        plt.plot(result_x, result_y, marker=<span class="hljs-string">'o'</span>, mec=<span class="hljs-string">'r'</span>, mfc=<span class="hljs-string">'w'</span>, label=<span class="hljs-string">u'Route'</span>)        plt.legend()  <span class="hljs-comment"># 让图例生效</span>        plt.margins(<span class="hljs-number">0</span>)        plt.subplots_adjust(bottom=<span class="hljs-number">0.15</span>)        plt.xlabel(<span class="hljs-string">u"x"</span>)  <span class="hljs-comment"># X轴标签</span>        plt.ylabel(<span class="hljs-string">u"y"</span>)  <span class="hljs-comment"># Y轴标签</span>        plt.title(<span class="hljs-string">"TSP best:%05.3f iter:%d pop:%d kill:%d p:%d%%"</span> % (self.best_dist, self.iter, self.max_population_size, self.killnum, self.crossover_rate*<span class="hljs-number">100</span>))  <span class="hljs-comment"># 标题</span>        plt.show()        plt.close(<span class="hljs-number">0</span>)<span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:    np.set_printoptions(threshold=<span class="hljs-number">1e6</span>)  <span class="hljs-comment"># 设置打印数量的阈值</span>    tsp = GA()    tsp.evolution()    <span class="hljs-comment">#print(tsp.fitness)</span></code></pre><h1 id="5-实验验证："><a href="#5-实验验证：" class="headerlink" title="5.实验验证："></a>5.实验验证：</h1><p>实验选取全国34个省、市、自治区的省会城市作为样本，根据地图描绘出其大概坐标位置。使用遗传算法求取最短路径，并将其绘制在地图上。实验中动态调整交叉概率、种群大小、每次迭代杀死个体的数目。</p><div class="table-container"><table><thead><tr><th style="text-align:center">实验编号</th><th style="text-align:center">交叉概率</th><th style="text-align:center">种群大小</th><th style="text-align:center">杀死数目</th><th style="text-align:center">迭代次数</th><th style="text-align:center">最优解</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">0.85</td><td style="text-align:center">50</td><td style="text-align:center">10</td><td style="text-align:center">200</td><td style="text-align:center">238</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">0.7</td><td style="text-align:center">50</td><td style="text-align:center">20</td><td style="text-align:center">200</td><td style="text-align:center">217</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0.9</td><td style="text-align:center">50</td><td style="text-align:center">20</td><td style="text-align:center">300</td><td style="text-align:center">201</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">0.8</td><td style="text-align:center">100</td><td style="text-align:center">40</td><td style="text-align:center">500</td><td style="text-align:center">176</td></tr></tbody></table></div><h2 id="实验1："><a href="#实验1：" class="headerlink" title="实验1："></a><strong>实验1：</strong></h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjujm507ppj30hs0dcadd.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="实验2："><a href="#实验2：" class="headerlink" title="实验2："></a><strong>实验2：</strong></h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjujm5jo5nj30hs0dcn0g.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="实验3："><a href="#实验3：" class="headerlink" title="实验3："></a><strong>实验3：</strong></h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjujm3uls7j30hs0dc0w0.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="实验4："><a href="#实验4：" class="headerlink" title="实验4："></a><strong>实验4：</strong></h2><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjujm4i2oej30hs0dc41p.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>最短距离：176.056</strong> [‘天津’, ‘石家庄’, ‘呼和浩特’, ‘太原’, ‘西安’, ‘银川’, ‘兰州’, ‘西宁’, ‘乌鲁木齐’, ‘拉萨’, ‘成都’, ‘重庆’, ‘贵阳’, ‘昆明’, ‘南宁’, ‘海口’, ‘澳门’, ‘广州’, ‘香港’, ‘台北’, ‘福州’, ‘杭州’, ‘上海’, ‘南京’, ‘合肥’, ‘南昌’, ‘长沙’, ‘武汉’, ‘郑州’, ‘济南’, ‘沈阳’, ‘长春’, ‘哈尔滨’, ‘北京’]</p><h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p><a href="https://blog.csdn.net/jiang425776024/article/details/84532018" target="_blank" rel="noopener">https://blog.csdn.net/jiang425776024/article/details/84532018</a><br>        <a href="https://blog.csdn.net/u010451580/article/details/51178225" target="_blank" rel="noopener">https://blog.csdn.net/u010451580/article/details/51178225</a><br>        <a href="https://blog.csdn.net/woaixuexihhh/article/details/83826256" target="_blank" rel="noopener">https://blog.csdn.net/woaixuexihhh/article/details/83826256</a><br>        <a href="https://blog.csdn.net/sinat_31184961/article/details/85218294" target="_blank" rel="noopener">https://blog.csdn.net/sinat_31184961/article/details/85218294</a><br>        <a href="https://www.cnblogs.com/pengsixiong/p/4823473.html" target="_blank" rel="noopener">https://www.cnblogs.com/pengsixiong/p/4823473.html</a></p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>TSP</tag>
      
      <tag>GA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（10.16)</title>
    <link href="/2020/10/16/10-16/"/>
    <url>/2020/10/16/10-16/</url>
    
    <content type="html"><![CDATA[<p>最近主要阅读了两篇基于KeyPoint的目标检测论文：CenterNet和ExtremeNet，和一篇用卷积神经网络预测光流场的论文：FlowNet。</p><p><strong>阅读文献：</strong></p><p>[1] Duan K, Bai S, Xie L, et al. Centernet: Keypoint triplets for object detection[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 6569-6578.</p><p>[2] Zhou X, Zhuo J, Krahenbuhl P. Bottom-up object detection by grouping extreme and center points[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 850-859.</p><p>[3] Dosovitskiy A, Fischer P, Ilg E, et al. Flownet: Learning optical flow with convolutional networks[C]//Proceedings of the IEEE international conference on computer vision. 2015: 2758-2766.</p><h1 id="1-CenterNet"><a href="#1-CenterNet" class="headerlink" title="1.CenterNet"></a>1.CenterNet</h1><p>目标检测中基于关键点的方法通常会遇到大量不正确的边界框，可能是由于缺乏对被裁减掉区域的关注。本文探索了在每个被裁减的区域的视觉模式，提出了一个One-stage的关键点检测器，名为CenterNet。CenterNet将目标当做三个关键点来检测：左上角、右下角和中心点。此外文章提出了两个定制模型：Cascade corner pooling、Center pooling，用以增强左上角和右下角边角点收集到的信息、在中心区域提供更多可识别的信息。</p><p>  CornerNet的性能受限于其对目标全局信息的提取能力较弱，因为CornerNet通过一对边角点来构造一个目标，导致算法对目标边界敏感，不知道哪一对边角点应被分为同一组。为了解决这一问题，作者对CornerNet做了改进，为其赋予了在每个提议区域内感知视觉模式的能力，使其能自己判断每个边界框的正确性。</p><p>  在推理过程中，当用一对边角点生成一个proposal后，CenterNet通过检查是否有一个相同类的中心关键点落在该proposal的中心区域，来判断该proposal是否确实包含一个目标。</p><p><strong>CenterNet</strong></p><p>  以CornerNet为基线，在CornerNet的基础上增加了一个分支，用于预测中心关键点的偏移。首先使用CornerNet的方法生成top-k个边界框，通过以下步骤利用检测到的中心关键点滤除不正确的边界框：</p><p>(1) 根据评分选择前k个中心关键点。</p><p>(2) 使用相应的偏移将中心关键点映射回输入图像。</p><p>(3) 为每个检测到的边界框定义一个中心区域，并检查中心区域是否包含相同类的中心关键点。</p><p>(4) 若中心区域有相同类的中心关键点，则保留该边界框。边界框的评分为左上角、中心点、右下角分数的平均值。若中心区域没有中心关键点，则丢弃该边界框。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpmduj1j30fu08iwfj.jpg" srcset="/img/loading.gif" alt="img"></p><p>中心区域的尺寸选择会影响检测结果，较小的中心区域会导致小边界框低召回率，较大的中心区域导致大边界框低精确率。因此，作者提出了一种根据边界框尺寸，自动调整中心区域大小的方法:scale-aware central region。该方法旨在为较小的边界框生成较大的中心区域，为较大的边界框生成较小的中心区域。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpoqnp9j31ak0bqtmp.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>Center Pooling</strong></p><p>  用于预测中心关键点的分支，帮助中心关键点包含更多可识别的目标视觉模式，使其更容易察觉proposal的中心部分。通过在特征图上求出中心关键点的水平和垂直方向上的最大响应总和来预测中心关键点。</p><p>  目标的几何中心通常没有传达可识别的视觉模式，为解决这一问题，作者提出center pooling来捕捉更丰富、更可识别的视觉模式。首先使用backbone输出特征图，然后在水平和垂直方向上查找最大值，并将其加在一起，进而决定特征图的某个像素点是否是中心关键点。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpl2qc9j30ml08st9q.jpg" srcset="/img/loading.gif" alt="img"></p><p>  图（a）展示了Center Pooling的结构，为了得到水平方向上的最大值，只需将Left Pooling和Right Pooling连接在一起；为了得到垂直方向上的最大值，只需将Top Pooling和Bottom Pooling连接在一起。图（b）展示了Cascade Top Corner Pooling的结构，在Top Pooling之前，增加了一个Left Corner Pooling。</p><p><strong>Cascade Corner Pooling</strong></p><p>  边角点通常在目标之外，缺乏局部表观特征，Corner Pooling通过寻找边界方向上的最大值来寻找边角点，然而，这样使得边角点对边缘过于敏感。因此需要边角点“看见”目标的视觉模式。Cascade Corner Pooling为Corner pooling添加了察觉内部信息的能力。通过在用于预测边角点的特征图上，求边界和内部方向上的最大值之和来实现。首先沿着边界查找边界最大值，然后沿着边界最大值的位置查找内部最大值。（最顶部的竖直向下朝着底部查找，最左边的水平向右查找，最右边的水平向左查找，最下面的竖直向上查找）</p><p><strong>总结：</strong></p><p>  CenterNet使用一个三联组来检测目标，包括：一个中心点和两个边角点。CenterNet针对CornerNet过度关注边缘信息的缺点，增加了对目标中心点的检测，帮助更好的筛选候选框。</p><p><strong>创新点：</strong></p><p>CenterNet在top-left和bottom-right两条分支的基础上，新增了一条没有embeddings的分支，用于预测特征图上每个位置是中心点的概率。找出中心点后，看中心区域里面有没有中心点，如果有就检测成功，否则就失败；通过这一思路可以有力地筛去误检。</p><h1 id="2-ExtremeNet"><a href="#2-ExtremeNet" class="headerlink" title="2.ExtremeNet"></a>2.ExtremeNet</h1><p>随着深度学习的进步，目标检测从自底向上发展为为自顶向下的识别问题。本文展示了自底向上的方法仍然有竞争力，使用标准关键点检测网络，检测目标的四个极点（最顶部、最左、最右、最低部）和一个中心点。使用纯几何方式将四个极点分为一组。这样，目标检测变为纯粹的基于表观的关键点估计问题，而不是区域分类或盲目的特征学习。</p><p>作者认为矩形边界框不是一种自然地表示目标的方法，因为许多目标没有与坐标轴对其的框，若将其拟合为一个矩形框就会包含许多背景干扰像素点。并且矩形框没有传达目标形状和姿势等细节信息。</p><p>本文提出了一种自底向上的目标检测框架：ExtremeNet，使用最新的关键点估计框架，为每个目标类别预测四张多峰值的热图，来寻找目标的四个极点。此外，为每个类别使用一张热图来预测目标中心。最后使用纯几何的方法来将四个极点分为一组：当且仅当四个极点的几何中心在中心热图的预测值高于一个预定义的阈值时，将其分为一组。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpnsbx7j30ee0c90ym.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>与CornerNet的不同：</strong></p><p>ExtremeNet和CornerNet都是基于关键点的目标检测方法，但ExtremeNet与CornerNet有两个不同：关键点定义和分组。首先，关键点定义不同：CornerNet的边角点通常在目标之外，没有很强的表观特征，而ExtremeNet的极点在目标之上，是视觉可分的，并且与局部表观特征一致。其次，ExtremeNet使用几何方法为关键点分组，而CornerNet使用一个嵌入式向量。</p><p><strong>网络框架：</strong></p><p>  使用Hourglass网络来为每个类检测五个关键点（四个极点、一个中心点），训练设置、损失和偏移预测都与CornerNet相同（没有为中心热图预测偏移）。网络的输出为5XC张热图和4X2张偏移图。（C为类别数）</p><p>  <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpjdeb4j30u508o43b.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>Center Grouping:</strong></p><p>  一旦极点被提取出来，使用纯几何的方法将其归为一组。因为极点通常在目标的不同侧，使得将其归为一组变得复杂。联合嵌入方法[30]可能没有一个足够全局的视角来为其分组。本文提出的分组算法的输入为五张热图，对于每张热图首先根据峰值提取对应的关键点。然后采用穷举方法，根据从四张热图中提取的四个极点，计算其几何中心点。若该几何中心点在中心热图上的响应大于某个阈值，就用这四个极点生成一个边界框，评分为五张热图的均值。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzplwqxrj30dv0eftai.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>Ghost box suppression:</strong></p><p>  当出现3个连续的目标在一排或者在一列的时候，容易出现鬼影问题，比如，3个一排，如果选择了最左边的最左点，最右边的最右点，这样在判断中心的时候，可能选择了中间目标的中心点，这样这个大box也就被保留下来了，出现了误判，怎么解决这个问题呢？作者选择了一种后处理的方法：soft non-maxima suppression来减少ghost box。如果一个大框中，包含了多个目标（大于等于3个）就将该目标的得分除以2，通过这种方法来一定程度的抑制误检的出现。</p><p><strong>Edge aggregation：</strong></p><p>  由于极点的定义不唯一，例如目标的水平或垂直边界上的任何一个点都有可能被当做极点。因此该网络对任何排成一行的目标边界有较弱的响应，而非较强的单峰值响应。作者使用Edge aggregation（边缘聚类）来解决该问题，为每个极点提取局部最大值，对水平与竖直方向的极值点，分别向左右或上下两个方向聚类到极小值点。将第一个单调下降区间的点的score的和按一定权重累加到原极值点上，代替该极值点的分数。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpk7os7j30dl06sgop.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>Extreme Instance Segmentation：</strong></p><p>  作者提出了一个简单的方法来构建近似的目标mask：使用一个边界线的中心点为极点的八边形来近似表示目标。作者使用Deep Extreme Cut（DEXTR）进一步改进边界框分割。</p><p><strong>总结：</strong></p><p>  本文提出了一种基于自底向上极点估计的目标检测框架，提取四个极值点并用几何方法将其归为一组，根据提取到的极值点生成八边形mask，在目标检测和实例分割上均取得了较好的效果。</p><h1 id="3-FlowNet"><a href="#3-FlowNet" class="headerlink" title="3.FlowNet"></a>3.FlowNet</h1><p>CNN已经在多种计算机视觉任务上取得成功，但还没有成功应用于预测光流。光流估计需要精确的逐像素定位，同时需要找到两张输入图像之间的对应。这不仅涉及图像特征表达，而且要学习匹配两张图片不同位置的特征。本文使用CNN有监督学习，解决了光流估计问题。提出了两种架构：FlowNetSimple和FlowNetCorr，前者更为通用，后者包含了一层从不同图像位置收集特征向量的网络层。</p><p>本文与此前方法的最大不同是直接预测整个光流场。该网络结构受到逐像素预测任务的启发，如滑动窗口法虽然比较有效但计算量巨大，并且不能提取全局输出特征（如锋利的边缘），另一种是上采样特征图至全分辨率，可用于预测兴趣区域的像素值。本文不仅对粗糙预测进行上卷积，而且对整个粗糙特征图进行上卷积，使其能提取更高级别的信息来更好的预测。在网络的收缩部分将上卷积的结果与特征图联系起来。</p><p>  卷积神经网络善于在给出大量标记数据时预测输入输出关系，因此作者使用一个端到端的学习方法来预测光流：给出一个包含一对图像和真实的光流，训练网络来直接从图像预测x-y光流场。为了实现这个目标，该如何设计网络结构呢？</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpmvcwfj30nh0cfjv9.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>FlowNetSimple：</strong> </p><p>一个简单的选择是将输入图像堆叠在一起，喂给一个普通的网络，使其自己决定自己该如何处理这一对图片来提取移动信息。作者将这种仅由卷积层构成的网络结构称为“FlowNetSimple”。但是这种结构不能保证使用局部梯度优化（如随机梯度下降）能使网络收敛。</p><p><strong>FlowNetCorr：</strong></p><p>因此，需要手工设计一个特殊的网络：为两张输入图像创建两个相互独立，但具有相同处理流程的结构，在之后的步骤中再将其合并。使用这种结构，可以强迫网络首先独立的处理两张图片有意义的表示，然后在更高的等级将其合并。类似于使用标准匹配方法从两张图像的块中首次提取特征，在给出两张图像的特征表示后，如何找到其相关性呢？作者介绍了一个“correlation layer”来帮助网络在匹配过程中，在两张特征图间执行多块对比。作者将包含了”correlation layer”的网络结构称为“FlowNetCorr”。</p><p><strong>Refinement：</strong></p><p>由于池化操作导致分辨率减小，作者采用上卷积层（包括上采样和卷积）来优化粗糙的池化表示，以提供密集的逐像素预测。具体是在特征图上应用上卷积层，将其与网络收缩部分的相应特征图和一个上采样的粗糙光流预测连接起来。使用这种方法能同时保留粗特征图提供的高等级信息和低级特征图提供的精细局部信息。重复这一过程4次，每次增加2倍分辨率，最终的光流分辨率仍然比原图小4倍。作者发现更一步使用该方法不能显著提高效果，最终选用variation approach作为替代方案，该方案虽然需要更多的计算量，但可以得到平滑、子像素精确的光流场。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjrzpo8xh1j30cr04vjsh.jpg" srcset="/img/loading.gif" alt="img"></p><p>  此外，作者构建了一个合成数据集“<strong>Flying Chairs</strong>”（对3D椅子模型进行仿射变换得到），为网络提供高了泛化能力。在训练过程中采用了数据增强方法来避免过拟合（包括平移、旋转、缩放、增加高斯噪声、改变亮度、对比度、伽马值、颜色）。</p><p><strong>EPE(Endpoint Error)</strong>，是光流估计中标准的误差度量，是预测光流向量与真实光流向量的欧氏距离在所有像素上的均值。</p><p><strong>总结：</strong></p><p>  <strong>优点：</strong></p><ul><li>实现了用卷积神经网络从两张输入图片直接预测光流</li><li>在非真实的Flying Chairs数据集上训练的网络在真实光流数据集上也表现良好，击败了LDOF方法。</li><li>FlowNet能保留更多细节信息。</li><li>人工合成的Flying Chairs数据集提高了网络的泛化能力。</li></ul><p><strong>缺点：</strong></p><ul><li>FlowNetC更容易过拟合。</li><li>EPE比EpicFlow高，说明准确率比EpicFlow低</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CenterNet</tag>
      
      <tag>ExtremeNet</tag>
      
      <tag>FlowNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>目标检测论文</title>
    <link href="/2020/10/12/object-detection-papers/"/>
    <url>/2020/10/12/object-detection-papers/</url>
    
    <content type="html"><![CDATA[<p><strong>近期学长在研究方向上给了我一些指导，在此总结整理一下。</strong></p><ul><li>学长整理的视频目标检测综述：<a href="https://blog.csdn.net/breeze_blows/article/details/105323491" target="_blank" rel="noopener">https://blog.csdn.net/breeze_blows/article/details/105323491</a></li><li>学姐整理的视频目标检测综述：<a href="https://blog.csdn.net/sinat_31184961/article/details/103128931" target="_blank" rel="noopener">https://blog.csdn.net/sinat_31184961/article/details/103128931</a></li><li>学长的github：<a href="https://github.com/breezelj" target="_blank" rel="noopener">https://github.com/breezelj</a></li><li>学长整理的视频目标检测的文章：<a href="https://github.com/breezelj/video_object_detection_paper" target="_blank" rel="noopener">https://github.com/breezelj/video_object_detection_paper</a></li></ul><h1 id="视频目标检测论文"><a href="#视频目标检测论文" class="headerlink" title="视频目标检测论文"></a>视频目标检测论文</h1><ul><li>RDN: Jiajun Deng, Yingwei Pan, Ting Yao, Wengang Zhou, Houqiang Li, and Tao Mei. “Relation Distillation Networks for Video Object Detection”. ICCV(2019).</li><li>SELSA: Haiping Wu, Yuntao Chen, Naiyan Wang, Zhaoxiang Zhang. “Sequence Level Semantics Aggregation for Video Object Detection”. ICCV(2019).</li><li>LLTR: Mykhailo Shvets, Wei Liu, Alexander C. Berg. “Leveraging Long-Range Temporal Relationships Between Proposals for Video Object Detection”. ICCV(2019).</li><li>MEGA: Yihong Chen, Yue Cao, Han Hu, Liwei Wang. “Memory Enhanced Global-Local Aggregation for Video Object Detection”. CVPR(2020).</li><li>HVRNet: Mingfei Han, Yali Wang, Xiaojun Chang, and Yu Qiao Mining. “Mining Inter-Video Proposal Relations for Video Object Detection”. ECCV(2020). </li><li>FGFA: Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, Yichen Wei. “Flow-Guided Feature Aggregation for Video Object Detection”. ICCV(2017).</li></ul><h1 id="学长总结的目标检测论文-转"><a href="#学长总结的目标检测论文-转" class="headerlink" title="学长总结的目标检测论文(转)"></a>学长总结的目标检测论文(转)</h1><p>基于anchor的目标检测主要可以分为two-stage detector；one-stage detector</p><h2 id="1-two-stages："><a href="#1-two-stages：" class="headerlink" title="1.two-stages："></a>1.two-stages：</h2><ul><li>RCNN: Rich feature hierarchies for accurate object detection and semantic segmentation. CVPR014</li><li>Fast RCNN. ICCV2015</li><li>Faster RCNN: Towards real-time object detection with region proposal networks. NIPS2015</li><li>Mask RCNN. ICCV2017</li><li>Cascade RCNN: Delving into High Quality Object Detection. CVPR2018</li></ul><h2 id="2-one-stage"><a href="#2-one-stage" class="headerlink" title="2.one-stage:"></a>2.one-stage:</h2><ul><li>YOLOv1: You only look once: Unified, real-time object detection. CVPR2016</li><li>Yolo9000: better, faster, stronger. CVPR2017</li><li>YOLOv3: An Incremental Improvement</li><li>SSD: Single shot multibox detector. ECCV2016</li><li>Retinanet: Focal loss for dense object detection. ICCV2017</li></ul><p>近年来兴起的anchor-free的detector</p><h2 id="3-anchor-free："><a href="#3-anchor-free：" class="headerlink" title="3.anchor-free："></a>3.anchor-free：</h2><ul><li>CornerNet: Detecting Objects as Paired Keypoints. ECCV2018.</li><li>FCOS: Fully Convolutional One-Stage Object Detection. ICCV2019</li><li>Centernet:Objects as Points/ Keypoint triplets for object detection ICCV2019</li></ul><h2 id="4-一些通用的目标检测结构"><a href="#4-一些通用的目标检测结构" class="headerlink" title="4.一些通用的目标检测结构"></a>4.一些通用的目标检测结构</h2><ul><li><strong>OHEM:</strong> Training Region-based Object Detectors with Online Hard Example Mining. CVPR2016</li><li>FPN: Feature Pyramid Networks for Object Detection. CVPR2017</li></ul><h2 id="5-目标检测平台（集成了众多目标检测算法的代码实现）"><a href="#5-目标检测平台（集成了众多目标检测算法的代码实现）" class="headerlink" title="5.目标检测平台（集成了众多目标检测算法的代码实现）"></a>5.目标检测平台（集成了众多目标检测算法的代码实现）</h2><ul><li>mmdetection：<a href="https://github.com/open-mmlab/mmdetection" target="_blank" rel="noopener">https://github.com/open-mmlab/mmdetection</a></li><li>detectron2：<a href="https://github.com/facebookresearch/detectron2" target="_blank" rel="noopener">https://github.com/facebookresearch/detectron2</a></li></ul><h2 id="6-目标检测综述文章"><a href="#6-目标检测综述文章" class="headerlink" title="6.目标检测综述文章"></a>6.目标检测综述文章</h2><ul><li><p>Object Detection in 20 Years: A Survey</p></li><li><p>A Survey of Deep Learning-based Object Detection</p></li><li><p>Recent Advances in Deep Learning for Object Detection</p></li><li><a href="https://github.com/hoya012/deep_learning_object_detection#2014" target="_blank" rel="noopener">https://github.com/hoya012/deep_learning_object_detection#2014</a></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>目标检测</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（10.9)</title>
    <link href="/2020/10/09/10-9/"/>
    <url>/2020/10/09/10-9/</url>
    
    <content type="html"><![CDATA[<p>最近阅读集中在目标检测方向，学习了RetinaNet、CornerNet、FCOS三篇文章。其中RetinaNet提出了一个新的损失函数Focal Loss，CornerNet和FCOS是Anchor Free的代表性算法。</p><p><strong>阅读文章：</strong></p><p>[1] Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2980-2988.</p><p>[2] Law H, Deng J. Cornernet: Detecting objects as paired keypoints[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 734-750.</p><p>[3] Tian Z, Shen C, Chen H, et al. Fcos: Fully convolutional one-stage object detection[C]//Proceedings of the IEEE international conference on computer vision. 2019: 9627-9636.</p><h1 id="1-RetinaNet"><a href="#1-RetinaNet" class="headerlink" title="1. RetinaNet"></a>1. RetinaNet</h1><p>基于深度学习的目标检测方法有两种经典的结构：Two-Stage和One-Stage：</p><ul><li><strong>Two-Stage：</strong>例如R-CNN，第一步生成一个稀疏的候选目标位置集合，第二步使用卷积神经网络将每个候选位置分为某一类别的前景或背景，并进行精确的坐标回归。Two-Stage方法准确度较高，但速度相对较低。</li><li><strong>One-Stage：</strong>例如SSD、YOLO。此类方法摒弃了提取Proposal的过程，直接进行分类和回归，更快，更简单，但精度落后于Two-Stage。</li></ul><p>那么，到底是什么原因造成One-Stage方法的精度不如Two-Stage？One-Stage方法是否也能达到和Two-Stage一样的精度呢？作者认为在训练过程中，极端前景背景类别失衡是主要原因。</p><p>本文提出了一个可与最先进Two-stage检测器的精度匹配的One-Stage检测器。为了实现该结果，作者将训练过程中的类不平衡问题看作主要障碍，并提出了一个新的损失函数来消除该障碍。</p><h3 id="预备知识："><a href="#预备知识：" class="headerlink" title="预备知识："></a><strong>预备知识：</strong></h3><p>在继续学习RetinaNet之前，首先要搞清楚何为Hard/Easy Positive/Negative Example。从字面意思上理解，Hard为困难样本、较难分类的样本，Easy为容易分类的样本、Positive为正样本，Negative为负样本。正负样本比较好区分：bbox与ground-truth的IoU大于某一阈值（一般为0.5）即为正样本，IoU小于该阈值即为负样本。</p><p>Hard和Easy的区分主要看是否在前景和背景的过渡区域上。如果不在前景和背景的过渡区域上，分类较为明确，称为Easy。若在过渡区域上，则较难分类，称为Hard。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb4us3dj30ld0bwq44.jpg" srcset="/img/loading.gif" alt="img"></p><h4 id="类不平衡问题："><a href="#类不平衡问题：" class="headerlink" title="类不平衡问题："></a><strong>类不平衡问题：</strong></h4><p>在一张输入图片中，检测器对每张图片评估104-105个候选位置，但只有少数位置包含物体，故负样本多于正样本，出现类不平衡问题。类不平衡将引发两个问题：</p><ol><li>训练效率低，大多数位置是easy negative ，没有贡献有用的学习信号。（大多数负样本不在过渡区域，分类明确，称为easy negative。）</li><li>负样本过多造成loss太大，以至于淹没掉正样本的loss，不利于目标收敛，导致模型退化。</li></ol><p>上述问题2将导致无法得出一个能对模型训练提供正确指导的loss，训练步骤容易被已分类的背景样本所主导（而Two Stage方法得到proposal后，其候选区域要远远小于One Stage产生的候选区域，因此不会产生严重的类别失衡问题）。常用的解决此问题的方法就是负样本挖掘，或其它更复杂的用于过滤负样本从而使正负样本数维持一定比率的样本取样方法。本文提出了一个新的损失函数Focal Loss，更高效的替代之前处理类不平衡问题的方法。</p><h4 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss:"></a><strong>Focal Loss:</strong></h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnbal0hij30h40at3zv.jpg" srcset="/img/loading.gif" alt="img"></p><p>  Focal Loss被用来解决one-stage检测方案中训练时前景和背景极端不平衡的问题。Focal Loss是在交叉熵损失函数的基础上进行修改，首先回顾二分类的交叉熵(CE)损失：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb91d0gj30cl02s3yj.jpg" srcset="/img/loading.gif" alt="img"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb9wl8gj307r02gq2u.jpg" srcset="/img/loading.gif" alt="img"></p><p>交叉熵损失可以的图像如上图蓝色曲线所示，将模型预测得到的概率&gt;0.6的样本看作容易分类的样本，预测值较小的为Hard examples。最终，整个网络的损失是所有样本损失的累加，虽然Hard example的loss较高，但其数量较少；容易分类的样本虽然loss小，但数量多。导致全部累加后，大部分的loss来自Easy example，造成模型在优化过程中更关注容易分类的样本，忽略较难分类的样本。</p><p>在二分类交叉熵损失基础上引入权重参数α，得到平衡交叉熵损失：<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb5s86vj306j01b3yd.jpg" srcset="/img/loading.gif" alt="img">，对于类别1使用权重α，对于类别-1使用权重（1-α）。采用α加权可以平衡正负样本，但无法区分易分类样本和难分类样本。</p><p>训练过程中的类不平衡问题淹没了交叉熵损失。容易被分类的负样本构成了大部分损失，并支配着梯度。尽管α平衡了正负样本的重要性，但它没有区分easy/hard样本。作者重构了loss function来降低easy样本的权重，从而更关注hard negative的训练。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb6rrw2j309301e3yd.jpg" srcset="/img/loading.gif" alt="img"></p><p>从图中也可看出：当γ逐渐增大时，较容易分类的loss几乎为0，而Pt较小的部分（hard example）的loss依然较大，可以保证在累加后让hard examples贡献更多的loss，在训练时给予hard examples更多的优化。</p><p>  在实验时为上述Focal Loss增加一个α平衡因子，可以轻微提高精度。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb82fv9j309f019glh.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>RetinaNet</strong>：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb5c1omj30n306aq4r.jpg" srcset="/img/loading.gif" alt="img"></p><p>RetinaNet由一个基石网络(ResNet+FPN)和两个特定任务的子网络组成。基石网络负责在整个输入图像上计算卷积特征图，并且它不是一个自卷积神经网络。第一个子网络在基石网络的输出上做卷积目标分类，第二个子网络执行卷积边框回归。</p><p>输入图像经过Backbone后，可以得到P3-P7特征图金字塔（下标表示特征金字塔的层数，Pl的特征图分辨率比原始图像小2l），每层有C=256个通道。得到特征金字塔后，对特征金字塔的每一层分别使用两个子网络（分类子网络+检测框位置回归子网络）。</p><h3 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h3><ul><li>作者将极端类不平衡问题看作阻止one-stage方法超过two-stage方法的主要障碍。</li><li>为解决该问题，提出了一个新的损失函数Focal Loss，更高效的替代之前处理类不平衡问题的方法。</li><li>提出RetinaNet：ResNet+FPN+2个FCN子网络，精度超过Faster R-CNN。</li></ul><h1 id="2-CornerNet"><a href="#2-CornerNet" class="headerlink" title="2. CornerNet"></a>2. CornerNet</h1><h3 id="目前被经常使用的anchor-box有两个缺点："><a href="#目前被经常使用的anchor-box有两个缺点：" class="headerlink" title="目前被经常使用的anchor box有两个缺点："></a><strong>目前被经常使用的anchor box有两个缺点：</strong></h3><ul><li>需要一个非常大的anchor boxes集合。检测器需要判断每个anchor box是否与ground truth box 足够重叠。通常仅仅一小部分anchors与ground truth重叠，这就造成阳性anchors box和阴性anchors box 之间的巨大不平衡，并且减慢训练。</li><li>使用anchor box引入大量超参数和设计选择。包括：框的数量、框的尺寸、框的长宽比。</li></ul><h3 id="CornerNet："><a href="#CornerNet：" class="headerlink" title="CornerNet："></a><strong>CornerNet：</strong></h3><p>一种去掉anchor boxes的one-stage目标检测方法。CornerNet通过一对关键点（边界框左上角和右下角）来检测目标。使用一个卷积网络预测两个热图集合（相同目标类别的所有实例的），来表示不同目标类别的边角点位置，一个是左上角边角点的热图集合，一个是右下角边角点的热图集合。此外，网络为每一个被检测到的边角点预测一个嵌入式向量，这样同一目标的两个边角点的嵌入式向量之间的距离很小，就能基于这个距离将其分为一组。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb9g912j30nr06aacs.jpg" srcset="/img/loading.gif" alt="img"></p><p>使用沙漏网络作为CornerNet的Back-bone网络。沙漏网络后面跟着两个预测模块，一个模块预测左上角点，另一个预测右下角点。每个模块有他们自己的Corner Pooling模块来池化来自沙漏网络的特征，之后预测热图、嵌入式向量和偏移。</p><h3 id="Corner-pooling"><a href="#Corner-pooling" class="headerlink" title="Corner pooling:"></a><strong>Corner pooling:</strong></h3><p>一种新的池化层，通过编码明确的先验知识，可帮助卷积神经网络更好地定位边界框的角落。Corner Pooling在两个特征图上执行，从第一个特征图上（在每个像素位置）最大池化所有的特征向量，第二个特征图直接池化所有特征向量，然后将两个池化结果相加。</p><h3 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a><strong>总结：</strong></h3><h4 id="创新点："><a href="#创新点：" class="headerlink" title="创新点："></a><strong>创新点：</strong></h4><ul><li><p>提出CornerNet:一种新的目标检测方法，使用单个卷积神经网络，将目标边界框当做一组关键点（左上角、右下角）进行检测，消除了Anchor Boxes。</p></li><li><p>提出Corner Pooling，一种新的池化层，帮助网络更好的定位关键点。</p></li></ul><h4 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a><strong>缺点：</strong></h4><ul><li>CornerNet需要学习一个额外的距离向量，进行复杂的后处理来对属于相同实例的边角点分组。</li></ul><h1 id="3-FCOS"><a href="#3-FCOS" class="headerlink" title="3. FCOS"></a>3. FCOS</h1><p>当前所有主流检测器（例如Faster R-CNN、SSD、YOLO）都依赖于一个预定义的anchor boxes集合，一直以来人们相信使用anchor box是检测器成功的关键。尽管他们很成功，但了解基于anchor的检测器的缺点也很重要：</p><p>(1) 检测性能对anchor boxes的尺寸、长宽比、数量敏感。基于anchor的检测器的超参数需要被仔细调整。</p><p>(2) anchor boxes的尺寸和长宽比保持固定，检测器在候选目标形状出现大变化，特别是小目标时会遭遇困难。预定义的anchor boxes也会妨碍检测器的泛化能力，在不同的目标尺寸或长宽比的新检测任务上，anchor需要被重新设计。</p><p>(3) 为了实现高召回率，基于anchor的检测器需要在输入图像上密集的放置anchor box。（例如FPN上一张短边为800的图片有超过180K个anchor boxes）。这些anchor box在训练时绝大多数被标注为负样本。过多的负样本加重了训练期间的正负样本不平衡。</p><p>(4) Anchor box包含复杂的计算，例如计算和ground-truth bbox之间的IoU。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb7bb87j30dh07qtgs.jpg" srcset="/img/loading.gif" alt="img"></p><h3 id="全卷积One-stage检测器："><a href="#全卷积One-stage检测器：" class="headerlink" title="全卷积One-stage检测器："></a><strong>全卷积One-stage检测器：</strong></h3><p>检测器直接回归bbox，并将这些位置看作训练样本（而不是anchors)。如果某个位置(x,y)在任一ground-truth box中，那么将其看为正样本，c<em>为其类别标签，c</em>=0表示其为负样本（背景）。4维的实向量t<em> = {l</em>,t<em>,r</em>,b*}是位置回归目标，里面的元素是从该位置到bbox四条边的距离（如图左所示）。如果某个位置有多个边界框，将其看为模棱两可的样本（如图右所示），FCOS简单的选择最小区域的边界框作为回归目标。FCOS利用尽可能多的前景样本来训练回归器。这与基于anchor的检测器不同，它们仅将与GT的IoU足够高的样本看为正样本。这也是FCOS比基于anchor的检测器出色的原因之一。</p><h3 id="用FPN为FCOS进行多等级预测："><a href="#用FPN为FCOS进行多等级预测：" class="headerlink" title="用FPN为FCOS进行多等级预测："></a><strong>用FPN为FCOS进行多等级预测：</strong></h3><p><strong><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb66lp9j30ms09j76f.jpg" srcset="/img/loading.gif" alt="img"></strong></p><p>FCOS采用FPN在不同等级的特征图上检测不同尺寸的目标。P3-P7是用于最终预测的特征等级，P3-P5由backboneCNN的特征图产生。P6/P7分别通过在P5/P6上应用一个步长为2的卷积层产生。</p><p>与基于Anchor的检测器不同（它们为不同的特征层级分配不同尺寸的anchor），FCOS直接限制了每个层级上bbox回归的范围。FCOS首先为所有特征层级上每个位置计算l<em>,t</em>,r<em>,b</em>。如果某个位置满足max(l<em>,t</em>,r<em>,b</em>)&gt;mi 或 max(l<em>,t</em>,r<em>,b</em>)&lt;mi-1,就将其设为一个负样本，不再需要回归边界框。这里mi是该特征层级上需要去回归的最大距离。m2,m3,m4,m5,m6,m7分别为0,64,128,256,512,∞。(例如P3的范围是[0,64],P4的范围是[64,128])</p><h3 id="Center-ness："><a href="#Center-ness：" class="headerlink" title="Center-ness："></a><strong>Center-ness：</strong></h3><p>在FCOS上使用多层级的预测之后，FCOS的性能依然与基于Anchor的检测器有很大代沟。作者观察到这是因为许多位置预测的低质量的边界框远离目标的中心点。FCOS引入了“Center-ness”分支来预测某个像素与其对应bbox中心点的偏移。这个评分被被用于降低检测到的低质量的边界框的权重，并且在NMS中合并检测结果。Center-ness使得基于FCN的检测器比基于anchor的检测器效果更好。</p><p>  作者提出了一个简单高效的策略来压制检测到的低质量边界框，没有引入任何超参数。具体来说，FCOS增加了一个与分类分支并行的单层分支来预测一个位置的”Center-ness”。Center-ness描述了该位置到目标中心点位置的正规化距离。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gjmnb8kwmbj30av02eaaa.jpg" srcset="/img/loading.gif" alt="img"></p><p>在测试时最终评分（用于排列检测到的边界框）通过将预测到的Center-ness与相对应的类别评分相乘得到。Center-ness可以减少远离目标中心点的边界框的权重分数。因此，在最终的非极大值抑制过程中，这些低质量的边界框有很大概率被过滤掉，从而显著提高检测器的性能。</p><h3 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a><strong>总结：</strong></h3><h4 id="优点："><a href="#优点：" class="headerlink" title="优点："></a><strong>优点：</strong></h4><ul><li>FCOS使检测与许多其他FCN可解决的任务(如语义分割)统一起来，使重用这些任务中的思想变得更加容易。</li><li>将检测变为Proposal free和Anchor free，显著降低了设计参数的数量，使训练变的简单。</li><li>通过消除Anchor boxes，FCOS完全避免了Anchor boxes相关的复杂计算和超参数（例如在训练期间计算Anchor box和Ground truth box的IoU），并且以逐像素预测的方式进行目标检测，得以更快的训练和测试，同时减少训练期间内存用量。</li><li>FCOS可以作为Two-Stage检测器的区域建议网络(RPNs)，并显著优于基于Anchor的RPN。</li><li>FCOS检测器通过很小的修改就能立即扩展到解决其他视觉任务，包括实例分割和关键点检测。</li></ul><h3 id="附："><a href="#附：" class="headerlink" title="附："></a><strong>附：</strong></h3><p>FCOS论文中提到了BPR，之前对其不太了解，找来学习了一下。</p><p>BPR: best possible recall，文中定义为一个检测器能召回的ground-truth box的最大数量除以所有ground-truth的数量。</p><p>由于步长增大，通常最后的特征图都会面临较低的最大可能召回(best possible recall, BPR)问题。在anchor based detector中，可以通过降低正样本所需的IOU阈值来弥补，对于FCOS，乍一看由于较大stride，feature map上没有位置编码信息，因此人们可能认为BPR比基于anchor的检测器要低得多。作者通过实验证明，即使有较大的步长，基于FCN的FCOS仍然能够产生良好的BPR，甚至可以比基于Anchor的检测器的BPR更好，因此BPR实际上不是FCOS的问题。此外，通过多层级FPN预测，BPR可以得到进一步的提高可以达到RetinaNet最好的高度。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RetinaNet</tag>
      
      <tag>CornerNet</tag>
      
      <tag>FCOS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>设置ssh远程端口转发，实现内网穿透</title>
    <link href="/2020/10/07/ssh/"/>
    <url>/2020/10/07/ssh/</url>
    
    <content type="html"><![CDATA[<p>最近实验室给我分配了服务器账号，但该服务器的ip在实验室的局域网中，不是公网ip，无法在宿舍（外网）访问该服务器（但是该服务器可以访问公网）。我希望能在公网访问到实验室的服务器，于是学习配置了ssh远程端口转发（内网穿透），使用ssh -R可以轻松做到这个事情。在实现过程中遇到了很多坑，特此总结记录一下。</p><p>背景：一台服务器A（位于内网）、一台云虚拟主机B（有公网ip，外网可访问）。</p><p>目的：实现从外网访问服务器A。</p><h1 id="配置反向端口转发"><a href="#配置反向端口转发" class="headerlink" title="配置反向端口转发"></a>配置反向端口转发</h1><p>ssh -R命令的用法：ssh -R 远程地址:远程端口:目标地址:目标端口 user_name@ssh服务器</p><blockquote><p>ssh -R HostC:PortC:HostB:PortB  user@HostC </p><p>将本机可以访问到的HostB:PortB转发到 HostC的PortC端口，其中第一个HostC可以省略。</p></blockquote><p>例如：我想将本机的22端口映射到远程云主机的8888端口，则执行如下命令：</p><blockquote><p>ssh -N -R 8888:localhost:22 root@htx1998.cn</p></blockquote><p>其中 -R:反向端口转发;    -N :不执行远程命令，只做端口转发； -f 表示在后台运行;</p><p>在服务器A上执行上述命令后，就成功的将服务器A的22端口（用于ssh访问）反向转发到了htx1998.cn的8888端口，只要保持这个ssh不断，任何一台机器就能以htx1998.cn为跳板，通过ssh serveruser@htx1998.cn -p 8888  来连接到服务器A的22端口。</p><p>不过这样做可能并不稳定，若网络存在波动导致ssh -R的连接终端，则会失去控制。有个叫autossh的软件，可以自动的检测断线，并在断线的时候重连。</p><h2 id="安装-autossh-："><a href="#安装-autossh-：" class="headerlink" title="安装 autossh ："></a>安装 autossh ：</h2><blockquote><p>sudo apt-get install autossh </p></blockquote><p>安装完后，使用autossh代替ssh重新执行上述命令</p><blockquote><p> autossh -NfR 8888:localhost:22 root@htx1998.cn</p></blockquote><p>这里需要注意，由于使用-f 参数使该命令后台运行，导致无法输入远程云主机的密码，故需要配置公钥访问（详见后文）。</p><p>最终，在踩完下文的坑后，执行上述命令即可实现从外网以ssh serveruser@htx1998.cn -p 8888访问服务器A。</p><h2 id="坑：打开ssh隧道-应监听0-0-0-0，而不是127-0-0-1"><a href="#坑：打开ssh隧道-应监听0-0-0-0，而不是127-0-0-1" class="headerlink" title="坑：打开ssh隧道 应监听0.0.0.0，而不是127.0.0.1"></a>坑：打开ssh隧道 应监听0.0.0.0，而不是127.0.0.1</h2><p>在配置完反向端口转发后，发现在外网不能访问8888端口，查看云虚拟主机的防火墙配置，已经添加了8888端口的规则，依然提示connection refused。但登录云主机后，可以使用localhost:8888登录到服务器A，这就说明端口转发已经起作用了！</p><blockquote><p>netstat -anop | grep 8888</p><p> tcp        0      0 127.0.0.1:8888         0.0.0.0:*               LISTEN      1069/sshd: root  off (0.00/0/0)</p></blockquote><p>通过以上命令查看，发现8888端口是监听在127.0.0.1本地回环网卡上的，只能在本地服务器访问，无法从外部访问。需要将8888端口的监听地址设为0.0.0.0才行。</p><blockquote><p>修改vim /etc/ssh/sshd_config  （在外网服务器配置）<br>添加GatewayPorts yes<br>service sshd restart即可</p></blockquote><h1 id="配置公钥访问服务器："><a href="#配置公钥访问服务器：" class="headerlink" title="配置公钥访问服务器："></a>配置公钥访问服务器：</h1><p>在客户端用ssh-keygen -t rsa 生成公钥和私钥，一路回车，保存在root/.ssh/</p><blockquote><p>scp id_rsa.pub root@server:/root  把公钥上传到服务器</p><p>cat id_rsa.pub &gt;&gt;/root/.ssh/authorized_keys   将上传的公钥追加到authorized_keys的末尾<br>        chmod 600 authorized_keys    更改权限</p></blockquote><p>至此，实现免密码登录</p><h1 id="常用命令："><a href="#常用命令：" class="headerlink" title="常用命令："></a>常用命令：</h1><blockquote><p>netstat -aptn :查看当前开放的端口</p><p>lsof -i:8888  ：查看端口占用状态</p><p>netstat -anop | grep 8888  ：看8888端口是否监听</p><p>iptables —list ：看是否有系统防火墙限制</p><p>telnet ip port ：测试远程服务器端口是否开启 </p><p>ps -aux | grep 8888  ：查看含有8888的进程</p></blockquote><h1 id="参考文献："><a href="#参考文献：" class="headerlink" title="参考文献："></a>参考文献：</h1><p><a href="https://blog.csdn.net/autoliuweijie/article/details/80283689" target="_blank" rel="noopener">https://blog.csdn.net/autoliuweijie/article/details/80283689</a></p><p><a href="https://zhuanlan.zhihu.com/p/57630633" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/57630633</a></p><p><a href="https://blog.csdn.net/u012911347/article/details/80765894" target="_blank" rel="noopener">https://blog.csdn.net/u012911347/article/details/80765894</a></p><p><a href="https://blog.csdn.net/qm5132/article/details/83039405" target="_blank" rel="noopener">https://blog.csdn.net/qm5132/article/details/83039405</a></p><p><a href="https://www.jianshu.com/p/7f5f727a2b60" target="_blank" rel="noopener">https://www.jianshu.com/p/7f5f727a2b60</a></p><p><a href="https://blog.csdn.net/dreams_deng/article/details/78968416" target="_blank" rel="noopener">https://blog.csdn.net/dreams_deng/article/details/78968416</a></p><p><a href="https://www.cnblogs.com/ddif/p/9940571.html" target="_blank" rel="noopener">https://www.cnblogs.com/ddif/p/9940571.html</a></p><p><a href="https://blog.csdn.net/Lin_QC/article/details/91570316" target="_blank" rel="noopener">https://blog.csdn.net/Lin_QC/article/details/91570316</a></p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ssh</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>FCOS阅读笔记</title>
    <link href="/2020/10/04/FCOS/"/>
    <url>/2020/10/04/FCOS/</url>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>提出了一个全卷积one-stage目标检测器——FCOS，以逐像素的方式解决目标检测，类似语义分割。目前所有最先进的目标检测器依赖预定义的Anchor Boxes。相比之下，我们所提出的FCOS是anchor box free、proposal free的。通过消除预定义的anchor box集合，FCOS完全避免了anchor box相关的计算，例如在训练期间计算重叠率。更重要的是，我们避免了与anchor box相关的所有超参数，它们对检测性能非常敏感。仅仅通过非极大值抑制的后处理，FCOS在单模型单尺度测试中达到了44.7%的AP。因其更简单，超过了之前的one-stage检测器。第一次，我们展示了一个更加简单和灵活的检测框架，提高了检测精度。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p>目标检测是计算机视觉中一个基础而有挑战性的任务，需要算法为图片中每一个实例预测一个边界框和一个类别标签。当前所有主流检测器（例如Faster R-CNN、SSD、YOLO）都依赖于一个预定义的anchor boxes集合，一直以来人们相信使用anchor box是检测器成功的关键。尽管他们很成功，但了解基于anchor的检测器的缺点也很重要：</p><p>1)检测性能对anchor boxes的尺寸、长宽比、数量敏感。例如RetinaNet将超参数改变，在COCO上提高了4%的AP。因此基于anchor的检测器的超参数需要被仔细调整。</p><p>2）即使很仔细的设计，检测器在候选目标形状出现大变化，特别是小目标时会遭遇困难，因为anchor boxes的尺寸和长宽比保持固定。预定义的anchor boxes也会妨碍检测器的泛化能力，因为在不同的目标尺寸或长宽比的新检测任务上，anchor需要被重新设计。</p><p>3）为了实现高召回率，基于anchor的检测器需要在输入图像上密集的放置anchor box。（例如FPN上一张短边为800的图片有超过180K个anchor boxes）。这些anchor box在训练时绝大多数被标注为负样本。过多的负样本加重了训练期间的正负样本不平衡。</p><p>4）Anchor box包含复杂的计算，例如计算和ground-truth bbox之间的IoU。</p><p>最近全卷积网络在注入语义分割、深度估计、关键点检测等密集预测任务上取得巨大成功。作为高等级视觉任务之一，目标检测或许是唯一一个偏离了优雅的全卷积逐像素预测框架，主要是因为使用了anchor boxes。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7cdhiu4gj30dr0cyqbi.jpg" srcset="/img/loading.gif" alt="image-20200929110602589"></p><p>如图1左所示，4维向量描述了边界框的四条边的相对偏移量。图1右显示了高度重叠的边界框导致了棘手的模棱两可：在重叠区域中的像素中，不知道该回归哪个哪个边界框。</p><p>为了压制低质量检测器，我们引入了一个新奇的“Center-ness”分支（仅仅一层）来预测某个像素与其对应bbox中心点的偏移。这个评分被用于降低检测到的低质量的边界框的权重，并且在NMS中合并检测结果。这个简单高效的centerness分支，使得基于FCN的检测器比基于anchor的检测器效果更好。</p><p>新的检测框架有如下优点：</p><ul><li>检测现在已经与许多其他fcn可解决的任务(如语义分割)统一起来，使重用这些任务中的思想变得更加容易。</li><li>检测变为proposal free和anchor free，显著降低了设计参数的数量。为了实现高性能，设计参数通常需要启发式调节并涉及许多技巧。因此，我们新的检测器结构，使检测器，特别是训练，变的非常简单。</li><li>通过消除anchor boxes，我们的新检测器完全避免了anchor boxes相关的复杂计算，例如在训练期间的IoU计算和匹配anchor box和ground truth box，导致更快的训练和测试，同时减少训练期间内存用量。</li><li>我们还表明，所提出的FCOS可以作为两级检测器的区域建议网络(RPNs)，并显著优于基于Anchor的RPN。anchor-free检测器更简单并且性能高，我们建议社区重新考虑目标检测中anchor box的必要性，虽然目前anchor被认为是检测的实际标准。</li><li>所提出的检测器通过很小的修改就能立即扩展到解决其他视觉任务，包括实例分割和关键点检测。我们相信这种新方法可作为许多实例预测问题的新baseline。</li></ul><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h1><ul><li>基于Anchor的检测器：<ul><li>使用CNN的特征图，避免重复计算特征</li><li>需要过多的超参数来描述anchor的形状和标注每个anchor box是否为正负样本(通常使用IoU)</li><li>超参数对最终精度影响很大，需要被启发式调节。超参数是特定于检测任务的，使得检测任务从优雅的全卷积网络结构（被用于密集预测任务，如语义分割）中分离出来。</li></ul></li><li>Anchor free的检测器：<ul><li>YOLOv1(Anchor free）:只在目标中心点预测边界框，召回率低。</li><li>YOLOv2重新启用anchor box。</li><li>FCOS利用ground-truth中所有点来预测边界框，使用”center-ness“分支压制检测到的低质量边界框。</li><li>CornerNet(One-stage、AnchorFree）：通过检测一对bbox的边角点，并将他们分组以形成最终的检测结果。CornerNet需要复杂的后处理来对属于相同实例的边角点分组。需要为分组学习一个额外的距离度量。</li><li>DenseBox系列：不适用于通用目标检测（由于处理重叠的bbox比较困难）且召回率低。本文表明，这些问题可以使用多等级的FPN大大减轻。</li></ul></li></ul><h1 id="3-Our-Approach"><a href="#3-Our-Approach" class="headerlink" title="3. Our Approach"></a>3. Our Approach</h1><p>本部分首先在逐像素预测方式下修订目标检测。其次展示了我们如何使用多等级的预测来提高召回率、解决由重叠bbox造成的模棱两可的结果。</p><h2 id="3-1-全卷积One-stage检测器"><a href="#3-1-全卷积One-stage检测器" class="headerlink" title="3.1 全卷积One-stage检测器"></a>3.1 全卷积One-stage检测器</h2><p>我们的检测器直接回归bbox，将这些位置看作训练样本（而不是anchors),这与用于语义分割的FCN相同。</p><p>如果某个位置(x,y)在任一ground-truth box中，那么将其看为正样本，c*为其类别标签，c*=0表示其为负样本（背景）。4维的实向量t* = {l*,t*,r*,b*}是位置回归目标，里面的元素是从该位置到bbox四条边的距离。如果某个位置有多个边界框，那么将其看为模棱两可的样本，简单的选择最小区域的边界框作为回归目标。下一部分介绍的多等级预测将显著减少模棱两可样本的数量。</p><p>值得注意，FCOS利用尽可能多的前景样本来训练回归器。这与基于anchor的检测器不同，它们仅将与GT的IoU足够高的样本看为正样本。这也是FCOS比基于anchor的检测器出色的原因之一。</p><p><strong>网络输出</strong>：80维的分类标签向量p、4维的边界框坐标向量t={l,t,r,b}。我们训练C个二维分类器，而不是一个多类分类器。我们在backbone网络的特征图之后为每个类别和回归分支增加4个卷积层。我们在回归分支顶部采用exp(x)，因为回归目标总是正的。值得注意的是：FCOS比基于anchor的检测器的输出变量少了9倍。</p><p><strong>损失函数：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj79fdp92sj30bp07mq3q.jpg" srcset="/img/loading.gif" alt="image-20200929092402094"></p><p><strong>推理</strong>：FCOS的推理过程直截了当，将输入图像放入网络前向传播，并获得类别评分p<sub>x,y</sub>和回归预测tx,y（在特征图Fi的每一个位置）。将评分p<sub>x,y</sub>&gt;0.05的位置作为正样本。</p><h2 id="3-2-用FPN为FCOS进行多等级预测"><a href="#3-2-用FPN为FCOS进行多等级预测" class="headerlink" title="3.2 用FPN为FCOS进行多等级预测"></a>3.2 用FPN为FCOS进行多等级预测</h2><p>这里我们表明所提出的FCOS的两个可能的问题如何通过使用FPN多等级预测被解决。</p><p>1）CNN最终特征图的大步长（例如16）会导致相对低的BPR(best possible recall)。即使步长很大，基于FCN的FCOS仍人能产生一个好了BPR，比基于anchor的BPR更好。因此BPR确实不是FCOS的问题。通过多等级的FPN预测，BPR能被进一步的提高来匹配基于anchor的RetinaNet的最好BPR。</p><p>2）ground-truth boxes的重叠能导致棘手的模棱两可问题，重叠的位置应该回归哪个边界框？这个模棱两可导致基于FCN的检测器性能退化。本文表明该问题可被多等级预测很好的解决，</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7aewdwh8j30nq0aggns.jpg" srcset="/img/loading.gif" alt="image-20200929095812380"></p><p>追随FPN，我们在不同等级的特征图上检测不同尺寸的目标。P3-P7是用于最终预测的特征等级，P3-P5由backboneCNN的特征图产生（使用一个自顶向下的1x1的卷积层）。P6/P7分别通过在P5/P6上应用一个步长为2的卷积层产生。P3 P4 P5 P6 P7分别有8、16、32、64、128的步长。</p><p>与基于Anchor的检测器不同（它们为不同的特征层级分配不同尺寸的anchor），我们直接限制了每个层级上bbox回归的范围。具体来说，我们首先为所有特征层级上每个位置计算l*,t*,r*,b*。如果某个位置满足max(l,t,r,b)&gt;mi 或 max(l,t,r,b)&lt;mi-1,就将其设为一个负样本，不再需要回归边界框。这里mi是该特征层级上需要去回归的最大距离。在本工作中m2,m3,m4,m5,m6,m7分别为0,64,128,256,512,∞。因为不同尺寸的目标被分配到不同的特征层级，并且许多目标重叠发生在不同的目标尺寸之间。如果某个位置，即使使用了多层级预测，仍然被分配了多于一个的GT box，我们就简单的选择最小区域的ground-truth box作为其目标。我们的实验表明，多层级预测可以大大缓解此前提到的模棱两可问题，并提高基于FCN的检测器的性能到基于anchor的检测器的同级别性能。</p><p>我们共享不同特征层级的头部，不仅使检测器参数更高效，而且提升了检测器性能。我们观察到不同的特征层级需要回归不同的尺寸范围（例如P3的范围是[0,64],P4的范围是[64,128])，一次为不同特征层级使用相同的头部不合理。因此，我们摒弃标准的指数函数exp(x),使用exp(s<sub>i</sub>x)（有一个可训练的标量si）来自动的调整特征层级i的指数函数的基，这轻微提高了检测性能。</p><h2 id="3-3-Center-ness"><a href="#3-3-Center-ness" class="headerlink" title="3.3 Center-ness"></a>3.3 Center-ness</h2><p>在FCOS上使用多层级的预测之后，FCOS的性能依然与基于Anchor的检测器有很大代沟。我们观察到这是因为许多位置预测的低质量的边界框远离目标的中心点。</p><p>我们提出了一个简单但高效的策略来压制检测到的低质量边界框，没有引入任何超参数。具体来说，我们增加了一个与分类分支并行的单层分支来预测一个位置的”center-ness”。center-ness描述了该位置到目标中心点位置的正规化距离。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7by4hduhj30an020weg.jpg" srcset="/img/loading.gif" alt="image-20200929105116976"></p><p>使用开方来减慢center-ness的衰减，center-ness的范围为0-1，使用二进制交叉熵损失训练。在测试时最终评分（用于排列检测到的边界框）通过将预测到的center-ness与相对应的类别评分相乘得到。<strong>center-ness可以减少远离目标中心点的边界框的权重分数。</strong>因此，在最终的非极大值抑制过程中，这些低质量的边界框有很大概率被过滤掉，从而显著提高检测器的性能。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7cc923xij30dj0eptds.jpg" srcset="/img/loading.gif" alt="image-20200929110451262"></p><h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h1><p>略</p><h1 id="5-在Region-Proposal-网络上的扩展"><a href="#5-在Region-Proposal-网络上的扩展" class="headerlink" title="5. 在Region Proposal 网络上的扩展"></a>5. 在Region Proposal 网络上的扩展</h1><p>我们已经表明在one-stage检测器是，我们的FCOS比基于anchor的检测器能实现更好的性能。直觉上，FCOS也应该能替代Region Proposal 网络（RPNs）中的anchor box。我们用实验给出了证明。</p><h1 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h1><p>我们提出了一个anchor-free、proposal-free的one-stage检测器：FCOS。正如实验中所表明的，FCOS与机遇anchor的one-stage检测器相比更胜一筹，包括RetinaNet、YOLO、SSD，同时设计更简单。FCOS完全避免了所有关于anchor box的计算和超参数，并且以逐像素预测的方式解决目标检测，类似于语义分割等稠密预测任务。FCOS也在one-stage检测器中达到了最先进的性能。我们还表明FCOS能被用于two-stage检测器Faster R-CNN的RPN，并且比其RPN的性能有了很大提高。鉴于其有效性和效率，我们希望FCOS可作为当前主流基于anchor检测器的一种强大而简单的替代。我们也相信FCOS能被扩展到解决许多其他实例级识别任务。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FCOS</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CornerNet阅读笔记</title>
    <link href="/2020/09/29/CornerNet/"/>
    <url>/2020/09/29/CornerNet/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>提出CornerNet:一种新的目标检测方法，使用单个卷积神经网络，将目标边界框当做一组关键点（左上角、右下角）进行检测。消除了Anchor Boxes。</p><p>提出Corner Pooling，一种新的池化层，帮助网络更好的定位关键点。</p><p>实验表明，CornerNet在MS COCO数据集上，达到42.2%的AP。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>目前最先进目标检测方法的一个主要组成部分是Anchor Box，是用于检测候选的一些可变尺寸、可变长宽比的框。Anchor Box在one-stage检测器中广泛使用，实现高精度、高效率的结果。One stage检测器将anchor boxes密集的放在图像上，并通过对anchors进行评分、通过回归改善他们的坐标。</p><p>使用anchor box有两个缺点：</p><ul><li>需要一个非常大的anchor boxes集合。检测器被用来训练判断每个anchor box是否与ground truth box 足够重叠。结果，仅仅一小部分anchors与ground truth重叠，这就造成阳性anchors box和阴性anchors box 之间的巨大不平衡，并且减慢训练。</li><li>使用anchor box引入大量超参数和设计选择。包括：多少框？框的尺寸？框的长宽比。这些选择很大程度上通过特殊的启发式作出的，当联合多尺度结构时会变得更复杂，每个网络在多个分辨率下分别作出预测，每个尺度使用不同的特征和自己的anchor boxes集合。</li></ul><p>本文介绍了CornerNet，一种去掉anchor boxes的one-stage目标检测方法。通过一组左上角和右下角的关键点来检测目标。使用单个卷积神经网络预测一个（相同目标类别的所有实例的）左上角点的”热图、一个右下角点的”热图“、一个嵌入式向量。嵌入式向量用于将属于相同目标的关键点组合，即网络被训练用于为这些关键点预测相近的嵌入式向量。我们的方法极大简化了网络的输出，无需设计anchor boxes。</p><p>CornerNet另一个新颖的内容是corner pooling，一种新的池化层，可帮助卷积神经网络更好地定位边界框的角落。Corner Pooling在两个特征图上执行，从第一个特征图上（在每个像素位置）最大池化所有的特征向量，第二个特征图直接池化所有特征向量，然后将两个池化结果相加。</p><p>我们假设有两个原因使检测角落比检测边界框中心或proposals更好：1.边界框的中心很难定位，因为它依赖目标的4条边，而定位一个角落仅需要2条边，更容易些。Corner pooling更是如此，它编码了一些关于角落定义的明确的先验知识。2.角落提供了一个更高效的边界框空间密集离散化方法：我们仅需要O(wh)个边角点，来替代O(w<sup>2</sup>h<sup>2</sup>)可能的anchor boxes。</p><p>CornerNet在MS COCO数据集上达到42.2%的AP。通过消融研究表明：corner pooling对CornerNet的优越性能至关重要。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7euh1e5ij31no0u0kh5.jpg" srcset="/img/loading.gif" alt="image-20200929123132990"></p><h1 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h1><p>我们的方法受到”Associative Embedding in the context of muti-person pose estimation”的启发。这个方法在单个网络中监测和组合人类的关节。每个被检测到的关节有一个嵌入式向量。关节被组合在一起，基于他们嵌入式向量之间的距离。我们是第一个构想将目标检测任务当做一个<strong>检测任务</strong>和用嵌入式向量<strong>组合边角点</strong>。另一个创新点是他提出了conrer pooling层，帮助更好的定位边角点。此外，我们对沙漏结构进行了重大修改，加入了我们提出的新奇的focal loss变体，帮助更好的训练网络。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7ev3a77dj31wg0jie82.jpg" srcset="/img/loading.gif" alt="image-20200929123207572"></p><h1 id="3-CornerNet"><a href="#3-CornerNet" class="headerlink" title="3. CornerNet"></a>3. CornerNet</h1><h2 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h2><p>CornerNet通过一对关键点（边界框左上角和右下角）来检测目标。使用一个卷积网络预测两个热图集合，来表示不同目标类别的边角点位置，一个是左上角边角点的热图集合，一个是右下角边角点的热图集合。此外，网络为每一个被检测到的边角点预测一个嵌入式向量，这样，同一目标的两个边角点的嵌入式向量之间的距离很小。为了产生更紧的边界框，网络同时预测一个偏移量，来轻微的调整边角点的位置。通过预测热图、嵌入式向量、和偏移量，我们应用一个简单的后处理算法来获得最终高度边界框。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj50d6kkh1j31ru0ja4fx.jpg" srcset="/img/loading.gif" alt="image-20200927103924915"></p><p>图四是CornerNet的一个概述图，我们使用沙漏网络(Newell et al.,2016)作为CorNerNet的Back-bone网络。沙漏网络后面跟着<strong>两个预测模块</strong>，一个模块预测左上角点，另一个预测右下角点。每个模块有他们自己的CornerPooling模块来池化来自沙漏网络的特征，之后预测热图、嵌入式向量和偏移。我们没有使用不同尺度的特征来检测不同尺寸的目标，我们仅仅将这两个模块应用于沙漏网络的输出。</p><h2 id="3-2-检测Corners"><a href="#3-2-检测Corners" class="headerlink" title="3.2 检测Corners"></a>3.2 检测Corners</h2><p>我们预测两个热图集合，一个是左上角点的，一个是右下角点的。每个热图集合有C个通道（C是类别数目，尺寸为HxW,没有背景通道），每个通道是一个二进制的mask，指示属于同一个类别的边角点的位置。</p><p>每个边角点，有一个阳性的ground-truth位置，其他所有的位置是阴性的。在训练过程中不是相等的惩罚所有阴性位置，而是减少那些在阳性位置某个半径内的阴性位置的惩罚。这是因为一组错误的边角点检测结果，如果他们与各自的ground truth 位置很近，仍然可以预测一个与ground-truth重叠很大的边界框。如果在半径内的一组点所产生的边界框与ground-truth的IoU大于0.3，那么就使用这个半径值。给出这个半径后，惩罚减少的数量通过一个非正规化的2维高斯函数给出。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7eynmq1dj30w70u0qv5.jpg" srcset="/img/loading.gif" style="zoom: 33%;" /></p><p>p<sub>cij</sub>是在预测出的热图上，坐标(i,j)是类别c的分数。y<sub>cij</sub>是被非正规化高斯函数增大的ground-truth热图。我们设计了一个focal loss的变体：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7epq0p4zj30xk042t9c.jpg" srcset="/img/loading.gif" alt="image-20200929122659653"></p><p>许多方法用下采样层来聚集全局信息，减少内存使用。当他们被用于一张全卷积图像，输出的尺寸通常比原始图像小。当坐标点重新映射到原图时，会丢失一些精度，这会极大地影响小的边界框和其ground-truth之间的IoU。为了解决这个问题，我们在重映射到输入图像分辨率之前，通过预测位置偏移来轻微的调整边角点的位置。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7eq9qkisj30xg03q0sz.jpg" srcset="/img/loading.gif" alt="image-20200929122731092" style="zoom: 50%;" /></p><p>我们在训练时为ground-truth边角点应用平滑L1损失。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7eqtb4k8j30xi04agm0.jpg" srcset="/img/loading.gif" alt="image-20200929122802250" style="zoom: 50%;" /></p><h2 id="3-3-组合边角点"><a href="#3-3-组合边角点" class="headerlink" title="3.3 组合边角点"></a>3.3 组合边角点</h2><p>一张图片中可能会有多个目标，会有多个左上角点和多个右下角点被检测出来。我们需要确定一对左上角点和右下角点是否来自相同的边界框。我们的方法灵感来自于“Associative Embedding method”，Newell等人检测人类所有的关节，为每个关节生成一个嵌入式向量。基于这些嵌入式向量之间的距离来组合关节。</p><p>这种联合嵌入式的方法也适用于我们的任务。网络为每一个检测到的边角点预测一个嵌入式向量，这样如果一个左上角点和一个右下角点属于相同的边界框，那么他们之间的嵌入式向量距离应该很小。我们就能基于这个距离将其分为一组。嵌入式向量的实际值不重要，仅仅需要知道他们之间的距离。</p><p>我们使用一维的嵌入式向量，e<sub>tk</sub>是目标k左上角点的嵌入式向量，e<sub>bk</sub>是右下角的。我们使用“pull”损失来训练网络组合边角点，用”push”损失来训练网络分离边角点。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7ere15f3j30y00em3zs.jpg" srcset="/img/loading.gif" alt="image-20200929122835309" style="zoom: 33%;" /></p><p>e<sub>k</sub>是e<sub>tk</sub>和e<sub>bk</sub>的平均值，△=1。与偏移损失类似，我们只对ground-truth边角点位置应用该损失。</p><h2 id="3-4-Corner-Pooling"><a href="#3-4-Corner-Pooling" class="headerlink" title="3.4 Corner Pooling"></a>3.4 Corner Pooling</h2><p>为了确定某个像素点是否是一个左上角点，我们需要在目标最顶部的边界水平向右看，在最底部的边界水平向左看来判断是否为右下角点。因此我们提出Corner Pooling来通过编码明确的先验知识，更好的定位边角点。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7ertzyuoj30xu0bi75q.jpg" srcset="/img/loading.gif" alt="image-20200929122901224" style="zoom: 33%;" /></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7evuogg6j31wi0sq0zd.jpg" srcset="/img/loading.gif" alt="image-20200929123252338"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7esfenkcj31wo0lo77e.jpg" srcset="/img/loading.gif" alt="image-20200929122935161"></p><h2 id="3-5-Hourglass-Network"><a href="#3-5-Hourglass-Network" class="headerlink" title="3.5 Hourglass Network"></a>3.5 Hourglass Network</h2><p>CornerNet 采用沙漏网络作为backbone网络。沙漏网络首先被用于人类姿态估计任务。它是一个全卷积神经网络，由一到多个沙漏模块组成。一个沙漏网络首先通过一系列卷积核最大池化层，对输入特征图进行下采样。然后通过一系列上采样层和最大池化层，将特征上采样回原始分辨率。因为细节在最大池化层中丢失，添加skip层用于将细节加回已上采样的特征。沙漏网络在一个统一的结构中捕获所有局部和全局特征。当多个沙漏模块被堆叠到网络中，沙漏模块能重新处理特征，来捕获更高层级的信息。这些性质也使得沙漏网络成为目标检测的一个理想选择。</p><p>本文的沙漏网络由两个沙漏模块组成，并对沙漏模块的结构做了一些修改。</p><p>与Newell等人相同，我们也在训练中增加一个中间超视图。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj7esxzvh5j31wk0q6n29.jpg" srcset="/img/loading.gif" alt="image-20200929123004633"></p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><p>略</p><h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h1><p>我们提出了一种新的目标检测方法，通过一组边角点来检测边界框。我们在MSCOCO数据集上评估CornerNet，证明了有竞争力的结果。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>CornerNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020中秋博饼</title>
    <link href="/2020/09/26/MidAutumn/"/>
    <url>/2020/09/26/MidAutumn/</url>
    
    <content type="html"><![CDATA[<p>​    2020.9.26，厦门观音山沙滩，海鲜自助烧烤+中秋博饼~</p><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=542126758&bvid=BV12i4y1774V&cid=239342170&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" > </iframe></div>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
      <category>旅行日记</category>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RetinaNet阅读笔记</title>
    <link href="/2020/09/26/RetinaNet/"/>
    <url>/2020/09/26/RetinaNet/</url>
    
    <content type="html"><![CDATA[<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Two- Stage :sparse</p><p>One-stage:dense(更快，更简单，但精度落后于Two-Stage，极端前景背景类别失衡是主要原因）</p><p><strong>FocalLoss在稀疏的hard examples上训练，避免大量easy negatives在训练时淹没检测器</strong>。</p><p>通过reshape标准交叉熵损失，降低已经很好被分类的样本的损失，从而解决类不平衡问题。</p><p>设计和训练了一个简单的稠密检测器：RetinaNet，达到先前One-stage检测器的速度，并且超过最先进的two-stage检测器的精度。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><p>Two-stage（proposal-driven 机制）例如RCNN框架，第一步生成一个稀疏的候选目标位置集合，第二步使用卷积神经网络将每个候选位置分为前景的类别之一或背景。通过一系列改进，two-stage达到了最佳精度。</p><p>One-stage检测器对目标位置、尺寸、长宽比进行有规律的、密集的采样（例如YOLO、SSD）相对于最先进的two-stage检测器提升了10-40%的精度。</p><p>本文提出了一个可与最先进two-stage检测器的精度匹配的One-Stage检测器。为了实现该结果，作者将训练过程中的类不平衡问题看作主要障碍，并提出了一个新的损失函数来消除该障碍。</p><p>类不平衡问题在像RCNN一样的检测器中通过两步级联和抽样启发被解决。</p><h4 id="Two-stage"><a href="#Two-stage" class="headerlink" title="Two-stage:"></a>Two-stage:</h4><p>Proposal Stage：Selective Search、EdgeBoxes、DeepMask、RPN</p><p>​            快速的缩小候选目标位置的数量到一个很小的值（1-2k),并且滤除大部分背景样本</p><p>Second分类步骤：执行启发式采样，例如固定前景和背景的比例（1：3）或者online hard example mining(OHEM),以在前景和背景之间维持一个可管理的平衡。</p><h4 id="One-stage"><a href="#One-stage" class="headerlink" title="One-stage:"></a>One-stage:</h4><p>必须处理在图像中定期采样的大量候选目标位置。在实践中意味着枚举100k个位置，密集的覆盖空间位置、尺度和长宽比。类似的启发采样是低效的，因为训练步骤容易被已分类的背景样本所主导。这些低效率是目标检测中的一个经典问题，通常通过bootstrapping或hard example mining解决。</p><p>本文提出了一个新的损失函数，更高效的替代之前处理类不平衡问题的方法。损失函数是动态缩放交叉熵损失，当对正确类的信心增加时，比例因子衰减为0。直观的说，这个比例因子能自动降低简单例的贡献权重，快速的将模型集中在难例上。Focal损失的具体形式不重要，其他实例也能实现相似的结果。</p><p>为了证明所提出的focal loss的有效性，我们设计了一个简单的one-stage检测器RetinaNet，由它对输入图片的目标位置进行密集采样命名。它的设计特点是一个高效的网络特征金字塔和使用锚框。</p><h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h1><p>经典的目标检测器:滑动窗口  boosted object  HOG  DPMs </p><h5 id="Two-stage检测器："><a href="#Two-stage检测器：" class="headerlink" title="Two-stage检测器："></a>Two-stage检测器：</h5><ul><li>First stage:生成稀疏的候选proposal集合，应当包含所有目标同时滤除主要的negative位置</li><li>Second-stage:将proposal分类为前景或背景。（R-CNN将Second-stage分类器升级到卷积神经网络）【区域建议网络RPN将生成proposal和Second-stage分类器 整合为单个卷积神经网络，形成Faster R-CNN的框架】</li></ul><h5 id="One-stage检测器："><a href="#One-stage检测器：" class="headerlink" title="One-stage检测器："></a>One-stage检测器：</h5><p>OverFeat第一个现代的基于深度网络的one-stage的检测器。SSD和YOLO更新了One-stage方法，他们的速度可以，但精度不如two-stage方法。</p><p>RetinaNet检测器的设计与以前的密集检测器有许多相似之处，特别是RPN引入的“锚”的概念，以及SSD和FPN[中引入的特征金字塔的使用。特别强调：我们简单检测器实现最高的结果不是基于网络设计的创新，而是由于所提出的Focal Loss。</p><h5 id="类不平衡："><a href="#类不平衡：" class="headerlink" title="类不平衡："></a>类不平衡：</h5><p>Boosted detectors、DPMs、SSD面临较大的类不平衡，检测器对每张图片评估10<sup>4</sup>-10<sup>5</sup>个候选位置，但只有少数位置包含物体。</p><p>类不平衡导致两个问题：</p><ol><li>训练效率低，<strong>大多数位置是easy negative</strong> ，没有贡献有用的学习信号</li><li>Easy negative能淹没训练，导致模型退化。</li></ol><p>一个常见的解决方案是执行某种形式的硬负挖掘[33,37,8,31,22]，在训练中采样硬例子或更复杂的采样/重新分配权重方案[2]。相比之下，我们证明了我们所提出的focal loss自然地处理了one-stage检测器所面临的类不平衡，并允许我们高效地训练所有的例子，而不需要采样,不需要担心easy negatives淹没损失和计算梯度。</p><h5 id="鲁棒估计："><a href="#鲁棒估计：" class="headerlink" title="鲁棒估计："></a>鲁棒估计：</h5><p>人们对设计<strong>鲁棒损失</strong>函数很感兴趣，例如Huberloss，通过降低有很大错误的样本的损失，来减少异常值outliers(hard examples)的贡献。相比之下，我们的focal loss没有解决异常值(outliers)，而是<strong>降低inliers(easy examples)的权重,</strong>这样即使他们的数量很大，他们对总损失的贡献也很小。换句话说，focal loss扮演了与robust loss对立的角色，它关注在稀疏的hard examples上的训练。</p><h1 id="3-Focal-Loss"><a href="#3-Focal-Loss" class="headerlink" title="3.Focal Loss"></a>3.Focal Loss</h1><p>Focal Loss被用来解决one-stage检测方案中训练时前景和背景极端不平衡的问题。从二分类的交叉熵(CE)损失来介绍Focal Loss，</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1wva3ph1j30dr02a74a.jpg" srcset="/img/loading.gif" alt="image-20200924182142381" style="zoom: 67%;" /></p><p>y=±1指定ground-truth类，p∈[0,1]是模型对于类标签y=1的概率。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xd6svy0j30cw0390ss.jpg" srcset="/img/loading.gif" alt="image-20200924183857022" style="zoom:67%;" /></p><p>CE损失在图1中用蓝线画出，一个值得注意的性质是：即使样本已经被很容易的分类(p&gt;&gt;0.5)，CE损失依然很大。当把大量easy样本加和起来，这些小的损失能淹没稀有的类别。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xen7zh1j30gb0aj75c.jpg" srcset="/img/loading.gif" alt="image-20200924184020821" style="zoom:67%;" /></p><h2 id="3-1-平衡的交叉熵"><a href="#3-1-平衡的交叉熵" class="headerlink" title="3.1 平衡的交叉熵"></a>3.1 平衡的交叉熵</h2><p>解决类不平衡的一种常用方法是对类1引入权重因子α。在实际应用中，α可采用逆类频率设置，也可作为交叉验证设置的超参数。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1xsqbgn0j30bg01aq2s.jpg" srcset="/img/loading.gif" alt="image-20200924185352955"></p><p>这种损失是CE的简单扩展，我们将其作为我们提出的Focal损失的实验基线。</p><h2 id="3-2-Focal-Loss-定义"><a href="#3-2-Focal-Loss-定义" class="headerlink" title="3.2 Focal Loss 定义"></a>3.2 Focal Loss 定义</h2><p>正如实验将显示的那样，在密集检测器的训练过程中 遇到的较大的类不平衡压倒了交叉熵损失。容易被分类的负样本构成了大部分损失，并支配着梯度。尽管α平衡了正负样本的重要性，但他没有区分easy/hard样本。<strong>我们重构了loss function来降低easy样本的权重，关注hard negative的训练。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1ymj45y2j30ci01mmx1.jpg" srcset="/img/loading.gif" alt="image-20200924192231368"></p><p>性质1：当一个样本被误分类并且概率很低，调制因子趋近于1，损失不受影响。当概率趋近于1，因子趋近于0，这样就降低了已经被良好分类的样本的损失占比。</p><p>性质2：聚焦参数γ平滑的调整easy样本的权重降低率。当γ=0时 FL=CE，随着γ的增加调制因子的影响也最值增加。（我们发现γ=2在实验中效果最佳）。</p><p>直观的说，调制因子减少了easy样本的损失贡献，并且扩展了损失较低样本的范围。例如当γ=2，一个样本被分类的概率为0.9，FL损失比CE损失低100倍，当概率增加到0.968，损失低了1000倍。这反过来增加了校正误分类样本的重要性。（它的损失降低了4倍，当概率&lt;0.5,γ=2）</p><p>在实验中，使用一个FL的α-balanced变体：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gj1zsm7emej30cm01nt8l.jpg" srcset="/img/loading.gif" alt="image-20200924200258905" style="zoom:67%;" /></p><p>我们在实验中采用这种形式，因为他比不适用α-balanced形式产生了轻微的精度提高。最后，我们注意到损失层的实现将计算p的sigmoid操作与损失计算相结合，从而获得更大的数值稳定性。</p><h2 id="3-3-类不平衡和模型初始化"><a href="#3-3-类不平衡和模型初始化" class="headerlink" title="3.3 类不平衡和模型初始化"></a>3.3 类不平衡和模型初始化</h2><p>二分类通常默认初始化为相同的输出概率（y=-1or1）。在这这样的初始化条件下，在类不平衡的情况下，由于频繁类别的损失会主导整个损失，造成训练早期的不稳定。为了解决这个问题，我们引入了一个先验概念，用于在训练开始时模型对稀有类（前景）估计的p值。我们用pi表示先验值，并设置它，以便模型对稀有类示例的估计p很低，例如0.01。我们注意到，这是模型初始化的变化(见§4.1)，而不是损失函数的变化。我们发现，在严重的类不平衡情况下，这可以提高交叉熵和FL损失的训练稳定性。</p><h2 id="3-4-类不平衡和Two-stage检测器"><a href="#3-4-类不平衡和Two-stage检测器" class="headerlink" title="3.4 类不平衡和Two-stage检测器"></a>3.4 类不平衡和Two-stage检测器</h2><p>Two-stage检测器经常使用交叉熵损失训练，而不用α-balanced或者我们提出的FL。相反，他们解决类不平衡问题通过两种机制：(1).两步级联 (2).偏置小批采样。第一个级联步骤是一个目标提议机制，将几乎无限目标可能位置集合减小到1000-2000。重要的是，被选择的proposals并不是随机的，而是可能与真实的目标位置相对应，从而消除大多数easy negatives。在训练第二阶段时，通常使用偏置抽样来构建minibatches，包含例如1:3的正例与负例。这个比率就像一个通过采样实现的隐式α平衡因子。我们提出的FocalLoss是用来在one-stage检测系统中<strong>通过损失函数</strong>解决这些机制。</p><h1 id="4-RetinaNet-Detector"><a href="#4-RetinaNet-Detector" class="headerlink" title="4. RetinaNet Detector"></a>4. RetinaNet Detector</h1><p>RetinaNet是一个单一的、统一的网络，由一个基石网络和两个特定任务的子网络组成。基石网络负责在整个输入图像上计算卷积特征图，并且它不是一个自卷积神经网络。第一个子网络在基石网络的输出上做卷积目标分类，第二个子网络执行卷积边框回归。</p><h4 id="特征金字塔基石"><a href="#特征金字塔基石" class="headerlink" title="特征金字塔基石"></a>特征金字塔基石</h4><p>采用FPN作为RetinaNet的基石网络，FPN使用自顶向下的路径和侧面连接扩展了一个标准的卷积神经网络，这样网络可以从单分辨率输入图像高效的构建一个rich、多尺度的特征金字塔。金字塔的每一个等级被用来检测不同尺度的目标。FPN从FCN提高了多尺度预测。在ResNet的顶部建立FPN结构。</p><h4 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h4><p>与RPN网络类似，也使用anchors来产生proposals。特征金字塔的每层对应一个anchor面积，为了产生更加密集的coverage，增加了三个面积比例 {2<sup>0</sup>,2<sup>1/2</sup>,2<sup>2/3</sup>}（即使用当前anchor对应的面积分别乘以相应的比例，形成三个尺度），然后anchors的长宽比仍为{1:2,1:1,2:1} ，因此<strong>特征金字塔的每一层对应A = 9种Anchors。</strong></p><p>我们使用和RPN变体相似的平移不变的Anchor boxes，在每一个金字塔等级有9个Anchors。<strong>每个Anchors有一个长度为K的one-hot向量（作为分类信息），和4个边界框回归目标向量。</strong>0.5的IoU阈值的Anchors被分配到ground-truth目标框，[0,0.4)的IoU被分配到背景。每个Anchor最多被分配到一个目标框，并且把长度为K的标签向量的相对应的位置设为1，其他为0 。如果一个Ancho没有被分配，它或许在[0.4,0.5)之间发生了重叠，我们在训练期间忽略它。每个Anchor和被分配的目标框之间的偏移被计算为边界框回归目标。</p><h4 id="分类子网络"><a href="#分类子网络" class="headerlink" title="分类子网络"></a>分类子网络</h4><p>分类子网络预测了A个Anchors和K个目标类别在每个空间位置存在的概率。该子网络的参数与所有级别的金字塔共享。从金字塔等级输入的特征图有C个通道，子网络应用3x3的卷积层，C个过滤器跟着一个ReLU激活函数，后面跟着一个有KA个滤波器的3x3的卷积层。最终每个空间位置输出KA个二进制的预测，后接Sigmoid激活。</p><p>与RPN相比，我们的目标分类子网络更深，仅使用3x3的卷积，没有与边界框回归子网络共享参数。</p><h4 id="边界框回归子网络"><a href="#边界框回归子网络" class="headerlink" title="边界框回归子网络"></a>边界框回归子网络</h4><p>与目标分类子网络平行，我们为<strong>每一个金字塔层级</strong>增加了另一个小的FCN，来回归每个anchorBox和附近的ground-truth的偏移。除了在结束时每个空间位置改为输出一个4A个线性输出，其他的设计和边界框回归子网络完全一致。我们使用类怀疑边界框回归器，使用更少的参数，同样高效。</p><h2 id="4-1推理和训练"><a href="#4-1推理和训练" class="headerlink" title="4.1推理和训练"></a>4.1推理和训练</h2><h4 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h4><p>RetinaNet形成了一个由ResNet-FPN作为基石的简单的FCN网络，包括分类子网络、边界框回归子网络。因此，推理只涉及在网络上传播一张图片。为了提高速度，我们只对每个FPN层上分数最高的1k个预测解码边界框预测（在阈值检测置信度为0.05之后）。将所有层级的顶部预测合并，执行阈值为0.5的非极大值抑制，产生最终的检测。</p><h4 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h4><p>采用FocalLoss作为分类子网络输出的损失。γ=2效果比较好，γ=[0.5,5]时RetinaNet相对鲁棒。在训练RetinaNet时，FocalLoss被应用于每个被采样的图片中所有(大约100k个)anchors上。这与使用启发式抽样(RPN)或硬实例挖掘(OHEM, SSD)为每个mini选择一小组Anchors(例如，256)的常见做法形成了对比。整张图片的focalloss是所有大约100k个anchors 的focalloss的总和。被已分配ground-truth box的anchors的数量正规化（不是所有anchors）因为绝大多数的anchors是easy negatives，损失在focal loss下可忽略不计。最后我们注意到，被分类给稀有类别的α权重也有一个稳定的范围，他与γ相互作用，两者必须同时选择。通常α应该被轻微减小，γ应该被增加。（γ=2，α=0.25时最佳）</p><h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5.实验"></a>5.实验</h1><p>略</p><h1 id="6-结论"><a href="#6-结论" class="headerlink" title="6.结论"></a>6.结论</h1><p>本文将类不平衡看作阻止one-stage目标检测器超过two-stage方法表现的主要障碍。为了解决这个问题，我们提出了focal loss，他对交叉熵损失应用了一个可调制的项，使得学习过程更关注hard negative样本。我们的方法简单高效。我们设计了一个全卷积one-stage检测器，报告大量实验分析，证明它的效果。表明他实现了最先进的速度和精度。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>RetinaNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（9.18)</title>
    <link href="/2020/09/18/9-18/"/>
    <url>/2020/09/18/9-18/</url>
    
    <content type="html"><![CDATA[<p>本次主要对GoogLeNet系列的学习进行完善，学习了其Inception V2、V3、V4版本。（注：此处对GoogLeNet版本的划分以v4版论文中的划分为标准。v2版本为Batch Normalization的论文。此前作者把在《Rethinking the Inception Architecture for Computer Vision》提出的网络结构叫做v2，此处将其称为v3。）</p><p><strong>参考文献：</strong></p><p>[1] Ioffe S, Szegedy C. Batch normalization: Accelerating deep network training by reducing internal covariate shift[J]. arXiv preprint arXiv:1502.03167, 2015.</p><p>[2] Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the inception architecture for computer vision[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2016: 2818-2826.</p><p>[3] Szegedy C, Ioffe S, Vanhoucke V, et al. Inception-v4, inception-resnet and the impact of residual connections on learning[J]. arXiv preprint arXiv:1602.07261, 2016.</p><h1 id="1-Batch-Normalization（GoogLeNet-Inception-V2）"><a href="#1-Batch-Normalization（GoogLeNet-Inception-V2）" class="headerlink" title="1.Batch Normalization（GoogLeNet Inception V2）"></a>1.Batch Normalization（GoogLeNet Inception V2）</h1><p>Inception V2相比Inception V1进行了如下改进:</p><ul><li>提出Batch Normalization，加快模型训练速度；</li><li>使用两个3x3的卷积代替5x5的大卷积，降低了参数数量并减轻了过拟合；</li><li>增大学习速率并加快学习率衰减速度以适用BN规范化后的数据；</li><li>去除Dropout并减轻L2正则化（因BN已起到正则化的作用）以加快训练速度，并且不会产生过拟合；</li><li>更彻底地对训练样本进行打乱（可以防止同一个例子一起出现在一个mini-batch中）；</li><li>减少数据增强过程中对数据的光照扭曲（因为BN训练更快，每个样本被训练的次数更少，通过更少地扭曲它们，可以让训练器关注更多的“真实”图像）。</li><li>去除 LRN(Local Response Normalization)即局部响应归一化，LRN函数最早的出现在AlexNet,其类似Dropout的功能,其防止数据过拟合而提出的一种处理方法。</li></ul><p>经过以上改进之后，Inception V2在达到Inception V1的准确率时快了14倍，并且模型在收敛时准确率的上限更高。</p><p><strong>创新点：</strong></p><p><strong>1.吸收了VGGNet的优点，利用多个小尺度卷积代替一个大尺度卷积，节省计算量。</strong></p><p>大尺寸的卷积核可以带来更大的感受野，但也意味着会产生更多的参数。InceptionV2将卷积核拆分：使用小的感受野代替大的感受野，v1版本中使用了5×5的卷积核，v2版本使用两个连续的3×3的卷积核替代它。这样保证了感受野的范围还减少了参数量。并且增加了网络的深度，使表达能力更强。</p><p><strong>2.提出Batch Normalization方法</strong></p><p>BN 是一个非常有效的正则化方法，可以让大型卷积网络的训练速度加快很多倍，同时收敛后的分类准确率也可以得到大幅提高。BN 在用于神经网络某层时，会对每一个 mini-batch 数据的内部进行标准化（normalization）处理，使输出规范化到 N(0,1) 的正态分布，减少了 Internal Covariate Shift（内部协变量偏移）。BN的论文中提出，传统的深度网络再训练时，每一层的输入的分布都在变化，导致训练变得困难，我们只能使用一个很小的学习速率解决这个问题。而对每一层使用BN之后，我们就可以有效的解决这个问题，学习速率可以增大很多倍，达到之前的准确率所需要的迭代次数只有1/4，训练时间大大缩短。而且还可以继续训练，最终超过Inception V1。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1givqiowqrmj30r40jeael.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></p><p><strong>总结：</strong></p><ul><li>使用较大的学习率而不用关心梯度爆炸或消失等问题。</li><li>降低了模型效果对初始权重的依赖</li><li>作为一个正则化器，在某些情况下不需要Dropout这种降低收敛速度的方法，从而加速收敛。</li><li>即使不适用ReLU也能缓解激活函数饱和的问题。</li><li>能够学习到当前层到下一层的分布缩放系数（包括scaling γ和shift β）</li></ul><h1 id="2-GoogLeNet-Inception-V3"><a href="#2-GoogLeNet-Inception-V3" class="headerlink" title="2.GoogLeNet Inception V3"></a>2.GoogLeNet Inception V3</h1><p><strong>文章的第一部分提出了四个网络结构设计准则：</strong></p><ul><li>避免特征表示瓶颈，尤其在网络的前若干层。<ul><li>神经网络层与层之间进行信息传递时，要避免极端压缩，即数据的尺寸不能减小的太快。（feature map的大小从输入到输出随着层数的加深逐步变小，通道数量逐渐增加，这个变化过程一定要缓慢的柔和的变小）</li></ul></li><li>更高维度的表示在网络中更容易局部处理。<ul><li>在卷积网络中增加每个图块的激活允许更多解耦的特征。所产生的网络将训练更快。</li></ul></li><li>空间聚合可以在较低维度嵌入上完成，而不会在表示能力上造成许多或任何损失。</li><li>平衡网络的宽度和深度</li></ul><p><strong>第二部分提出了网络的改进方法：</strong></p><ul><li><strong>卷积分解</strong></li></ul><p>Inception V3 学习了 VGG 用两个3x3的卷积代替5x5的大卷积，在降低参数、保持感受野范围的同时建立了更多的非线性变换，使得 CNN 对特征的学习能力更强。如此可以有效地只使用约(3x3 + 3x3)/(5x5)=72%的计算开销。大量实验表明，这样做并不会造成表达能力的下降，增加了网络的非线性，可以处理更多、更丰富的空间特征，增加了特征多样性。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1givqin9u4wj308t08swft.jpg" srcset="/img/loading.gif" alt="img"><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1givqiobf4zj307c0a70th.jpg" srcset="/img/loading.gif" alt="img"></p><ul><li><strong>非对称分解</strong></li></ul><p>GoogLeNet团队考虑了用3个3x1的卷积取代3x3的卷积，即任意nxn的卷积都可以通过1xn卷积后接nx1卷积来替代。在网络的前期使用这种分解效果并不好，在中度大小的特征图（大小在12-20之间）上使用的效果较好。</p><ul><li><strong>利用辅助分类器</strong></li></ul><p>作者认为辅助分类器起着正则化项的作用。辅助分类器最初的动机是将有用的梯度推向较低层，使其立即有用，并通过抵抗非常深的网络中的消失梯度问题来提高训练过程中的收敛。作者发现辅助分类器在训练早期并没有改善收敛，在接近训练结束时使用辅助分类器的网络达到了更稳定的水平。</p><ul><li><strong>通过标签平滑进行模型正则化（LSR）</strong></li></ul><p>对于多分类的样本标注一般是one-hot的，例如[0,0,0,1]，使用类似交叉熵的损失函数会使得模型学习中对ground truth标签分配过于置信的概率，并且由于ground truth标签的logit值与其他标签差距过大导致，出现过拟合，导致降低泛化性。一种解决方法是加正则项，即对样本标签给个概率分布做调节，使得样本标注变成“soft”的，例如[0.1,0.2,0.1,0.6]，作者发现这种方式在实验中降低了top-1和top-5的错误率0.2%。</p><ul><li><strong>降低特征图大小</strong></li></ul><p>通常有两种方法可以降低特征图大小：先池化再做Inception卷积、先做Inception卷积再做池化。先池化会造成特征却是，先做Inception计算量很大。为了同时保持特征表示，并且降低计算量，Inceptionv3采用两个并行化的模块来降低计算量（卷积、池化并行执行，最后合并）</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1givqipbn24j30fh08u759.jpg" srcset="/img/loading.gif" alt="img"></p><h1 id="3-GoogLeNet-Inception-V4"><a href="#3-GoogLeNet-Inception-V4" class="headerlink" title="3.GoogLeNet Inception V4"></a>3.GoogLeNet Inception V4</h1><p>  Inception V4 相比 V3 主要是结合了微软的 ResNet，ResNet主要关注加大网络深度后的收敛问题，而Inception更关注特征维度上的利用。</p><p>Inception结构有着良好的性能，并且计算量低。作者将Inception 结构与Residual Connection相结合，显著加速了训练过程。具有三个残差连接和一个Inception-v4的网络，在ImageNet classification（CLS）挑战的测试集上实现了3.08％的top-5错误率。</p><p><strong>残差模块的缩放：</strong></p><p>  作者发现，如果滤波器数量超过1000，残差网络开始出现不稳定，同时网络会在训练过程早期便会出现“死亡”，意即经过成千上万次迭代，在平均池化之前的层开始只生成0。通过降低学习率，或增加额外的batch-normalization都无法避免这种状况。</p><p>作者给出的解决办法是，在Residual function之后、激活函数之前，使用一个activation scaling的操作稳定训练，其中缩放因子为0.1到0.3。</p><p><strong>总结：</strong></p><p>本文提出了三种新的网络结构：</p><p>（1）Inception-ResNet-v1：混合Inception版本，它的计算效率同Inception-v3；</p><p>（2）Inception-ResNet-v2：更加昂贵的混合Inception版本，同明显改善了识别性能；</p><p>（3）Inception-v4：没有残差链接的纯净Inception变种，性能如同Inception-ResNet-v2。作者研究了引入残差连接如何显著的提高inception网络的训练速度。而且仅仅凭借增加的模型尺寸，最新的模型（带和不带残差连接）都优于以前的网络。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GoogLeNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——Batch Normalization(GoogLeNet-V2)</title>
    <link href="/2020/09/15/GoogLeNet-v2/"/>
    <url>/2020/09/15/GoogLeNet-v2/</url>
    
    <content type="html"><![CDATA[<h1 id="批量归一化：通过减少内部协变量偏移来加速深度网络训练"><a href="#批量归一化：通过减少内部协变量偏移来加速深度网络训练" class="headerlink" title="批量归一化：通过减少内部协变量偏移来加速深度网络训练"></a>批量归一化：通过减少内部协变量偏移来加速深度网络训练</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>训练深度神经网络非常复杂，因为在训练过程中，随着先前各层的参数发生变化，各层输入的分布也会发生变化。这通过要求较低的学习率和仔细的参数初始化来减慢训练速度，并且很难训练具有饱和非线性的模型。我们将这种现象称为<em>内部协变量偏移</em>，并通过标准化层输入来解决这个问题。我们的方法力图使标准化成为模型架构的一部分，并为<em>每个训练中的小批量数据</em>执行标准化。批标准化使我们能够使用更高的学习率，并且不用太注意初始化。它也作为一个正则化器，在某些情况下不需要Dropout。将批量标准化应用到最先进的图像分类模型上，批标准化在取得相同的精度的情况下，减少了14倍的训练步骤，并以显著的差距击败了原始模型。使用批标准化网络的组合，我们改进了在ImageNet分类上公布的最佳结果：达到了<code>4.9％ top-5</code>的验证误差（和<code>4.8％</code>测试误差），超过了人类评估者的准确性。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>深度学习极大地推动了视觉，语音和许多其他领域的最新发展。随机梯度下降（SGD）已被证明是一种训练深层网络的有效方法，并且动量（Sutskever等，2013）和Adagrad（Duchi等，2011）等SGD变体已被用来实现最先进的性能。SGD优化网络参数Θ，以最小化损失</p><script type="math/tex; mode=display">\Theta=argmin \frac{1}{N}\sum_{i=1}^{N}ℓ（X_i,\Theta)</script><p>x<sub>1…N</sub>是训练数据集。使用SGD，训练将逐步进行，在每一步中，我们考虑一个大小为m的<em>小批量数据</em>x<sub>1…m</sub>。通过计算$\frac{1}{m}\frac{\partialℓ(x_i,\Theta)}{\partial\Theta}$，使用小批量数据来近似损失函数关于参数的梯度。使用小批量样本，而不是一次一个样本，在一些方面是有帮助的。首先，小批量数据的梯度损失是训练集上的梯度估计，其质量随着批量增加而提升。第二，由于现代计算平台提供的并行性，对一个批次的计算比单个样本计算m次效率更高。</p><p>虽然随机梯度是简单有效的，但它需要仔细调整模型的超参数，特别是优化中使用的学习速率以及模型参数的初始值。训练的复杂性在于每层的输入受到前面所有层的参数的影响——因此当网络变得更深时，网络参数的微小变化就会被放大。</p><p>层输入的分布变化是一个问题，因为这些层需要不断适应新的分布。当学习系统的输入分布发生变化时，据说会经历<em>协变量转移</em>（Shimodaira，2000）。这通常是通过域适应（Jiang，2008）来处理的。然而，协变量漂移的概念可以扩展到整个学习系统之外，应用到学习系统的一部分，例如子网络或一层。考虑网络计算</p><script type="math/tex; mode=display">ℓ=F_2(F_1(u,\Theta_1),\Theta_2)</script><p>F1和F2是任意变换，学习参数Θ1，Θ2以便最小化损失ℓ。学习Θ2可以看作输入x=F1(u,Θ1)送入到子网络ℓ=F2(x,Θ2)。</p><p>例如，梯度下降步骤</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gio7sby4h0j30ha04ut8z.jpg" srcset="/img/loading.gif" alt="image-20200912220127281" style="zoom: 50%;" /></center><p>（对于批大小m和学习率α）与输入为x的单独网络F2完全等价。因此，输入分布特性使训练更有效——例如训练数据和测试数据之间有相同的分布——也适用于训练子网络。因此x的分布在时间上保持固定是有利的。然后，Θ2不必重新调整来补偿x分布的变化。</p><p>子网络输入的固定分布对于子网络外的层也有积极的影响。考虑一个激活函数为z=g(Wu+b)的层，u是层输入，权重矩阵W和偏置向量b是要学习的层参数，$g(x)=\frac{1}{1+exp(−x)}$。随着|x|的增加，g′(x)趋向于0。这意味着对于x=Wu+b的所有维度，除了那些具有小的绝对值之外，流向u的梯度将会消失，模型将缓慢的进行训练。然而，由于x受W,b和下面所有层的参数的影响，训练期间那些参数的改变可能会将x的许多维度移动到非线性的饱和状态并减慢收敛。这个影响随着网络深度的增加而放大。在实践中，饱和问题和由此产生的梯度消失通常通过使用修正线性单元(Nair &amp; Hinton, 2010) ReLU(x)=max(x,0)，仔细的初始化(Bengio &amp; Glorot, 2010; Saxe et al., 2013)和小的学习率来解决。然而，如果我们能保证非线性输入的分布在网络训练时保持更稳定，那么优化器将不太可能陷入饱和状态，训练将加速。</p><p>我们把训练过程中深度网络内部结点的分布变化称为<em>内部协变量转移</em>。消除它可以保证更快的训练。我们提出了一种新的机制，我们称为为<em>批标准化</em>，它是减少内部协变量转移的一个步骤，这样做可以显著加速深度神经网络的训练。它通过标准化步骤来实现，标准化步骤修正了层输入的均值和方差。批标准化减少了梯度对参数或它们的初始值尺度上的依赖，对通过网络的梯度流动有有益的影响。这允许我们使用更高的学习率而没有发散的风险。此外，批标准化使模型正则化并减少了对Dropout(Srivastava et al., 2014)的需求。最后，批标准化通过阻止网络陷入饱和模式让使用饱和非线性成为可能。</p><p>在4.2小节，我们将批标准化应用到性能最好的ImageNet分类网络上，并且表明我们可以使用仅7％的训练步骤来匹配其性能，并且可以进一步超过其准确性一大截。通过使用批标准化训练的网络的集合，我们取得了top-5错误率，其改进了ImageNet分类上已知的最佳结果。</p><h1 id="2-减少内部协变量偏移"><a href="#2-减少内部协变量偏移" class="headerlink" title="2. 减少内部协变量偏移"></a>2. 减少内部协变量偏移</h1><p>由于训练过程中网络参数的变化，我们将<em>内部协变量转移</em>定义为网络激活分布的变化。为了改善训练，我们寻求减少内部协变量转移。随着训练的进行，通过固定层输入x的分布，我们期望提高训练速度。众所周知(LeCun et al., 1998b; Wiesler &amp; Ney, 2011)如果对网络的输入进行白化，网络训练将会收敛的更快——即输入线性变换为具有零均值和单位方差，并去相关。当每一层观察下面的层产生的输入时，实现每一层输入进行相同的白化将是有利的。通过白化每一层的输入，我们将采取措施实现输入的固定分布，消除内部协变量转移的不良影响。</p><p>我们考虑在每个训练步骤或在某些间隔来白化激活值，通过直接修改网络或根据网络激活值来更改优化方法的参数(Wiesler et al., 2014; Raiko et al., 2012; Povey et al., 2014; Desjardins &amp; Kavukcuoglu)。然而，如果这些修改分散在优化步骤中，那么梯度下降步骤可能会试图以要求正规化进行更新的方式来更新参数，这会降低梯度下降步骤的影响。例如，考虑一个层，其输入u加上学习到的偏置b，通过减去在训练集上计算的激活值的均值对结果进行归一化：x̂=x−E[x]，x=u+b, X={x1…N}是训练集上x值的集合，$E[x]=\frac1N∑^N_{i=1}x_i$。如果梯度下降步骤忽略了E[x]对b的依赖，那它将更新b←b+Δb，其中Δb∝−∂ℓ/∂x̂。然后u+(b+Δb)−E[u+(b+Δb)]=u+b−E[u+b]。因此，结合b的更新和接下来标准化中的改变会导致层的输出没有变化，从而导致损失没有变化。随着训练的继续，b将无限增长而损失保持不变。如果标准化不仅中心化而且缩放了激活值，问题会变得更糟糕。我们在最初的实验中已经观察到了这一点，当标准化参数在梯度下降步骤之外计算时，模型会爆炸。</p><p>上述方法的问题是梯度下降优化没有考虑到标准化中发生的事实。为了解决这个问题，我们希望确保对于任何参数值，网络<em>总是</em>产生具有所需分布的激活值。这样做将允许关于模型参数损失的梯度来解释标准化，以及它对模型参数Θ的依赖。设x为层的输入，将其看作向量，X是这些输入在训练集上的集合。标准化可以写为变换</p><script type="math/tex; mode=display">x̂=Norm(x,X)</script><p>它不仅依赖于给定的训练样本x而且依赖于所有样本X——它们中的每一个都依赖于Θ，如果x是由另一层生成的。对于反向传播，我们将需要计算雅可比行列式</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gip9tbg5psj30fy03a74i.jpg" srcset="/img/loading.gif" alt="image-20200913195712646" style="zoom: 50%;" /></center><p>忽略后一项会导致上面描述的爆炸。在这个框架中，白化层输入是昂贵的，因为它要求计算协方差矩阵$Cov[x]=E_{x∈X}[xx^T]−E[x]E[x]^T$和它的平方根倒数，从而生成白化的激活$Cov[x]^{−1/2}(x−E[x])$和这些变换进行反向传播的偏导数。这促使我们寻求一种替代方案，以可微分的方式执行输入标准化，并且在每次参数更新后不需要对整个训练集进行分析。</p><p>以前的一些方法（例如（Lyu＆Simoncelli，2008））使用通过单个训练样本计算的统计信息，或者在图像网络的情况下，使用给定位置处不同特征图上的统计。然而，通过丢弃激活值绝对尺度改变了网络的表示能力。我们希望通过对相对于整个训练数据统计信息的单个训练样本的激活值进行归一化来保留网络中的信息。</p><h1 id="3-通过Mini-Batch统计进行标准化"><a href="#3-通过Mini-Batch统计进行标准化" class="headerlink" title="3. 通过Mini-Batch统计进行标准化"></a>3. 通过Mini-Batch统计进行标准化</h1><p>由于每一层输入的整个白化是代价昂贵的并且不是到处可微分的，因此我们做了两个必要的简化。首先是我们将单独标准化每个标量特征，从而代替在层输入输出对特征进行共同白化，使其具有零均值和单位方差。对于具有d维输入x=(x(1)…x(d))的层，我们将标准化每一维</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gipcgq77alj30b804gwem.jpg" srcset="/img/loading.gif" alt="image-20200913212857571" style="zoom:50%;" /></center><p>其中期望和方差在整个训练数据集上计算。如(LeCun et al., 1998b)中所示，这种标准化加速了收敛，即使特征没有去相关。</p><p>注意简单标准化层的每一层输入可能会改变层可以表示什么。例如，标准化sigmoid的输入会将它们约束到非线性的线性状态。为了解决这个问题，我们要确保<em>插入到网络中的变换可以表示恒等变换</em>。为了实现这个，对于每一个激活值x(k)，我们引入成对的参数γ(k)，β(k)，它们会归一化和移动标准化值：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gipd6lxsrsj30bq02m3yi.jpg" srcset="/img/loading.gif" alt="image-20200913215350237" style="zoom:50%;" /></center><p>这些参数与原始的模型参数一起学习，并恢复网络的表示能力。实际上，通过设置$γ(k)=\sqrt{Var[x^{(k)}]}$和$β(k)=E[x^{(k)}]$，我们可以重新获得原始的激活值，如果这是要做的最优的事。</p><p>每个训练步骤的批处理设置是基于整个训练集的，我们将使用整个训练集来标准化激活值。然而，当使用随机优化时，这是不切实际的。因此，我们做了第二个简化：由于我们在随机梯度训练中使用小批量，因此每个小批量都会产生每次激活的均值和方差的估计值。这样，用于标准化的统计信息可以完全参与梯度反向传播。注意，通过计算每一维的方差而不是联合协方差，可以实现小批量的使用；在联合情况下，将需要正则化，因为小批量大小可能小于白化的激活值的数量，从而导致单个协方差矩阵。</p><p>考虑一个大小为m的小批量数据B。由于标准化被单独地应用于每一个激活，所以让我们集中在一个特定的激活x(k)，为了清晰忽略k。在小批量数据里我们有这个激活的m个值，</p><script type="math/tex; mode=display">B= \{ x_{1...m} \}</script><p>设标准化值为x̂1…m，它们的线性变换为y1…m。我们把变换</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giqzpkcmcej30am01odfs.jpg" srcset="/img/loading.gif" alt="image-20200915073847452" style="zoom:50%;" /></center><p>看作<em>批标准化变换</em>。我们在算法1中提出了BN变换。在算法中，为了数值稳定，ϵ是一个加到小批量数据方差上的常量。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giqzqt2k5aj30qw0m2jui.jpg" srcset="/img/loading.gif" alt="image-20200915074001053" style="zoom:50%;" /></center><p>BN变换可以添加到网络上来操纵任何激活。在公式y=BNγ,β(x)中，我们指出参数γ和β需要进行学习，但应该注意到在每一个训练样本中BN变换不单独处理激活。相反，BNγ,β(x)取决于训练样本和<em>小批量数据中的其它样本</em>。缩放和移动的值y传递到其它的网络层。标准化的激活值x̂在我们的变换内部，但它们的存在至关重要。只要每个小批量的元素从相同的分布中进行采样，如果我们忽略ϵ，那么任何x̂值的分布都具有期望为0，方差为1。这可以通过观察∑mi=1x̂i=0和1m∑mi=1x̂2i=1看到，并取得预期。每一个标准化的激活值$\hat{x}^{(k)}$可以看作由线性变换y(k)=γ(k)x̂(k)+β(k)组成的子网络的输入，接下来是原始网络的其它处理。所有的这些子网络输入都有固定的均值和方差，尽管这些标准化的x̂(k)的联合分布可能在训练过程中改变，但我们预计标准化输入的引入会加速子网络的训练，从而加速整个网络的训练。</p><p>在训练过程中我们需要通过这个变换反向传播损失ℓ的梯度，以及计算关于BN变换参数的梯度。我们使用的链式法则如下（简化之前）：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir051pussj30pe0eagnj.jpg" srcset="/img/loading.gif" alt="image-20200915075342426" style="zoom:50%;" /></center><p>因此，BN变换是将标准化激活引入到网络中的可微变换。这确保了在模型训练时，层可以继续学习输入分布，表现出更少的内部协变量转移，从而加快训练。此外，应用于这些标准化的激活上的学习到的仿射变换允许BN变换表示恒等变换并保留网络的能力。</p><h2 id="3-1-批标准化网络的训练和推断"><a href="#3-1-批标准化网络的训练和推断" class="headerlink" title="3.1 批标准化网络的训练和推断"></a>3.1 批标准化网络的训练和推断</h2><p>为了<em>批标准化</em>一个网络，根据算法1，我们指定一个激活的子集，然后在每一个激活中插入BN变换。任何以前接收x作为输入的层现在接收BN(x)作为输入。采用批标准化的模型可以使用批梯度下降，或者用小批量数据大小为m&gt;1的随机梯度下降，或使用它的任何变种例如Adagrad (Duchi et al., 2011)进行训练。依赖小批量数据的激活值的标准化可以有效地训练，但在推断过程中是不必要的也是不需要的；我们希望输出只确定性地取决于输入。为此，一旦网络训练完成，我们使用总体统计来进行标准化</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir09r48gnj309403ujre.jpg" srcset="/img/loading.gif" alt="image-20200915075814112" style="zoom:50%;" /></center><p>而不是小批量数据统计。跟训练过程中一样，如果忽略ϵ，这些标准化的激活具有相同的均值0和方差1。我们使用无偏方差估计$Var[x]=\frac{m}{m−1}E_\Beta[σ^2_\Beta]$，其中期望是在大小为m的小批量训练数据上得到的，σ2是其样本方差。使用这些值移动平均，我们在训练过程中可以跟踪模型的准确性。由于均值和方差在推断时是固定的，因此标准化是应用到每一个激活上的简单线性变换。它可以进一步由缩放γ和转移β组成，以产生代替BN(x)的单线性变换。算法2总结了训练批标准化网络的过程。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir0dyyq7vj30rm12ajy5.jpg" srcset="/img/loading.gif" alt="image-20200915080216895" style="zoom:50%;" /></center><h2 id="3-2-批标准化卷积网络"><a href="#3-2-批标准化卷积网络" class="headerlink" title="3.2. 批标准化卷积网络"></a>3.2. 批标准化卷积网络</h2><p>批标准化可以应用于网络的任何激活集合。这里我们专注于仿射变换和元素级非线性组成的变换：</p><p>z=g(Wu+b)</p><p>其中W和b是模型学习的参数，g(⋅)是非线性例如sigmoid或ReLU。这个公式涵盖了全连接层和卷积层。我们在非线性之前通过标准化x=Wu+b加入BN变换。我们也可以标准化层输入u，但由于u可能是另一个非线性的输出，它的分布形状可能在训练过程中改变，并且限制其第一矩或第二矩不能去除协变量转移。相比之下，Wu+b更可能具有对称，非稀疏分布，即“更高斯”（Hyvärinen＆Oja，2000）；对其标准化可能产生具有稳定分布的激活。</p><p>注意，由于我们对Wu+b进行标准化，偏置b可以忽略，因为它的效应将会被后面的中心化取消（偏置的作用会归入到算法1的β）。因此，z=g(Wu+b)被z=g(BN(Wu))替代，其中BN变换独立地应用到x=Wu的每一维，每一维具有单独的成对学习参数γ(k)，β(k)。</p><p>另外，对于卷积层我们希望标准化遵循卷积特性——为的是同一特征映射的不同元素，在不同的位置，以相同的方式进行标准化。为了实现这个，我们在所有位置联合标准化了小批量数据中的所有激活。在算法1中，我们让是跨越小批量数据的所有元素和空间位置的特征图中所有值的集合——因此对于大小为m的小批量数据和大小为p×q的特征映射，我们使用有效的大小为m′=|B|=m⋅pq的小批量数据。我们每个特征映射学习一对参数γ<sup>(k)</sup>和β<sup>(k)</sup>，而不是每个激活。算法2进行类似的修改，以便推断期间BN变换对在给定的特征映射上的每一个激活应用同样的线性变换。</p><h2 id="3-3-批标准化可以提高学习率"><a href="#3-3-批标准化可以提高学习率" class="headerlink" title="3.3 批标准化可以提高学习率"></a>3.3 批标准化可以提高学习率</h2><p>在传统的深度网络中，学习率过高可能会导致梯度爆炸或梯度消失，以及陷入差的局部最小值。批标准化有助于解决这些问题。通过标准化整个网络的激活值，在数据通过深度网络传播时，它可以防止层参数的微小变化被放大。例如，这使sigmoid非线性更容易保持在它们的非饱和状态，这对训练深度sigmoid网络至关重要，但在传统上很难实现。</p><p>批标准化也使训练对参数的缩放更有弹性。通常，大的学习率可能会增加层参数的缩放，这会在反向传播中放大梯度并导致模型爆炸。然而，通过批标准化，通过层的反向传播不受其参数缩放的影响。实际上，对于标量a，BN(Wu)=BN((aW)u)</p><p>因此<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir17x42cej30d804gglw.jpg" srcset="/img/loading.gif" alt="image-20200915083103735" style="zoom:50%;" /></p><p>因此标量不影响层的雅可比行列式，从而不影响梯度传播。此外，因此更大的权重会导致<em>更小的</em>梯度，并且批标准化会稳定参数的增长。</p><p>我们进一步推测，批标准化可能会导致雅可比行列式的奇异值接近于1，这被认为对训练是有利的(Saxe et al., 2013)。考虑具有标准化输入的两个连续的层，并且变换位于这些标准化向量之间：ẑ=F(x̂)。如果我们假设x̂和ẑ是高斯分布且不相关的，那么F(x̂)≈Jx̂是对给定模型参数的一个线性变换，x̂和ẑ有单位方差，并且I=Cov[ẑ]=JCov[x̂]JT=JJT。因此，J是正交的，其保留了反向传播中的梯度大小。尽管上述假设在现实中不是真实的，但我们希望批标准化有助于梯度传播更好的执行。这有待于进一步研究。</p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="4-1-随时间激活"><a href="#4-1-随时间激活" class="headerlink" title="4.1 随时间激活"></a>4.1 随时间激活</h2><p>为了验证内部协变量转移对训练的影响，以及批标准化对抗它的能力，我们考虑了在MNIST数据集上预测数字类别的问题(LeCun et al., 1998a)。我们使用非常简单的网络，28x28的二值图像作为输入，以及三个全连接层，每层100个激活。每一个隐藏层用sigmoid非线性计算y=g(Wu+b)，权重W初始化为小的随机高斯值。最后的隐藏层之后是具有10个激活（每类1个）和交叉熵损失的全连接层。我们训练网络50000次迭代，每份小批量数据中有60个样本。如第3.1节所述，我们在网络的每一个隐藏层后添加批标准化。我们对基准线和批标准化网络之间的比较感兴趣，而不是实现在MNIST上的最佳性能（所描述的架构没有）。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir1eu093cj30qa0jyq7f.jpg" srcset="/img/loading.gif" alt="image-20200915083742349" style="zoom:50%;" /></center><blockquote><p>图1。(a)使用批标准化和不使用批标准化训练的网络在MNIST上的测试准确率，以及训练的迭代次数。批标准化有助于网络训练的更快，取得更高的准确率。(b，c)典型的sigmoid在训练过程中输入分布的演变，显示为15%，50%，85%。批标准化使分布更稳定并降低了内部协变量转移。</p></blockquote><p>图1(a)显示了随着训练进行，两个网络在提供的测试数据上正确预测的分数。批标准化网络具有更高的测试准确率。为了调查原因，我们在训练过程中研究了原始网络N和批标准化网络NtrBN(Alg. 2)中的sigmoid输入。在图1(b，c)中，我们显示，对于来自每个网络的最后一个隐藏层的一个典型的激活，其分布如何演变。原始网络中的分布随着时间的推移而发生显著变化，无论是平均值还是方差，都会使后面的层的训练复杂化。相比之下，随着训练的进行，批标准化网络中的分布更加稳定，这有助于训练。</p><h2 id="4-2-ImageNet分类"><a href="#4-2-ImageNet分类" class="headerlink" title="4.2. ImageNet分类"></a>4.2. ImageNet分类</h2><p>我们将批标准化化应用于在ImageNet分类任务（Russakovsky等，2014）上训练的Inception网络的新变种（Szegedy等，2014）。网络具有大量的卷积和池化层，和一个softmax层用来在1000个可能之中预测图像的类别。卷积层使用ReLU作为非线性。与（Szegedy等人，2014年）中描述的网络的主要区别是5×5卷积层被两个连续的3x3卷积层替换，最多可以有128个滤波器。该网络包含13.6⋅10<sup>6</sup>个参数，除了顶部的softmax层之外，没有全连接层。在其余的文本中我们将这个模型称为Inception。训练在大型分布式架构（Dean et al，2012）上进行，10个模型副本中的每一个都使用了5个并行步骤，使用异步带动量的SGD（Sutskever等，2013），小批量数据大小为32。随着训练进行，所有网络都通过计算验证准确率@1来评估，即每幅图像使用单个裁剪图像，在1000个可能性中预测正确标签的概率。</p><p>在我们的实验中，我们评估了几个带有批标准化的Inception修改版本。在所有情况下，如第3.2节所述，批标准化以卷积方式应用于每个非线性的输入，同时保持架构的其余部分不变。</p><h3 id="4-2-1-加速BN网络"><a href="#4-2-1-加速BN网络" class="headerlink" title="4.2.1. 加速BN网络"></a>4.2.1. 加速BN网络</h3><p>将批标准化简单添加到网络中不能充分利用我们方法的优势。为此，我们进行了以下修改：</p><p><em>提高学习率</em>。在批标准化模型中，我们已经能够从高学习率中实现训练加速，没有不良的副作用（第3.3节）。</p><p><em>删除Dropout</em>。如第3.4节所述,删除Dropout批标准化实现了一些与Dropout相同的目标。从修改后的BN-Inception状态中删除Dropout可以加快训练速度，而不会增加过度拟合的情况。</p><p><em>减少L2全中正则化</em>。虽然在Inception中模型参数的L2损失会控制过拟合，但在修改的BN-Inception中，损失的权重减少了5倍。我们发现这提高了在提供的验证数据上的准确性。</p><p><em>加速学习率衰减</em>。在训练Inception时，学习率呈指数衰减。因为我们的网络训练速度比Inception更快，所以我们将学习速度降低加快6倍。</p><p><em>删除局部响应归一化</em>。虽然Inception和其它网络（Srivastava等人，2014）从中受益，但是我们发现使用批标准化它是不必要的。</p><p><em>更彻底地搅乱训练样本</em>。我们启用了分布内部搅乱训练数据，这样可以防止同一个例子一起出现在一个mini-batch中。这导致验证准确率提高了约1％，这与批标准化作为正则化项的观点是一致的：它每次被看到时都会影响一个样本，在我们的方法中内在的随机化应该是最有益的。</p><p><em>减少光照扭曲</em>。因为批标准化网络训练更快，并且观察每个训练样本更少的次数，所以通过更少地扭曲它们，我们让训练器关注更多的“真实”图像。</p><h3 id="4-2-2-单网络分类"><a href="#4-2-2-单网络分类" class="headerlink" title="4.2.2. 单网络分类"></a>4.2.2. 单网络分类</h3><p>我们评估了下面的网络，所有的网络都在LSVRC2012训练数据上训练，并在验证数据上测试：</p><p><em>Inception</em>：在4.2小节开头描述的网络，以0.0015的初始学习率进行训练。</p><p><em>BN-Baseline</em>：每个非线性之前加上批标准化，其它的与Inception一样。</p><p><em>BN-x5</em>：带有批标准化的Inception，修改在4.2.1小节中。初始学习率增加5倍到了0.0075。原始Inception增加同样的学习率会使模型参数达到机器无限大。</p><p><em>BN-x30</em>：类似于<em>BN-x5</em>，但初始学习率为0.045（Inception学习率的30倍）。</p><p><em>BN-x5-Sigmoid</em>：类似于<em>BN-x5</em>，但使用sigmoud非线性$g(t)=\frac1{1+exp(−x)}$来代替ReLU。我们也尝试训练带有sigmoid的原始Inception，但模型保持在相当于机会的准确率。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir2czy07hj30pw0jy0v8.jpg" srcset="/img/loading.gif" alt="image-20200915091032564" style="zoom:50%;" /></center><blockquote><p>图2。Inception和它的批标准化变种在单个裁剪图像上的验证准确率以及训练步骤的数量。</p></blockquote><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir2fiswe9j30oc0gswhk.jpg" srcset="/img/loading.gif" alt="image-20200915091258256" style="zoom:50%;" /></center><blockquote><p>图3。对于Inception和它的批标准化变种，达到Inception最大准确率(72.2%)所需要的训练步骤数量，以及网络取得的最大准确率。</p></blockquote><p>在图2中，我们显示了网络的验证集准确率，作为训练步骤次数的函数。Inception网络在31⋅10<sup>6</sup>次训练步骤后达到了72.2％的准确率。图3显示，对于每个网络，达到同样的72.2％准确率需要的训练步骤数量，以及网络达到的最大验证集准确率和达到该准确率的训练步骤数量。</p><p>通过仅使用批标准化（BN-Baseline），我们在不到Inception一半的训练步骤数量内将准确度与其相匹配。通过应用4.2.1小节中的修改，我们显著提高了网络的训练速度。<em>BN-x5</em>需要比Inception少14倍的步骤就达到了72.2％的准确率。有趣的是，进一步提高学习率（BN-x30）使得该模型最初训练有点慢，但可以使其达到更高的最终准确率。这种现象是违反直觉的，应进一步调查。在6⋅10<sup>6</sup>步骤之后，BN-x30达到74.8％的准确率，即比Inception达到72.2％的准确率所需的步骤减少了5倍。</p><p>我们也证实了尽管训练这样的网络是众所周知的困难，但是当使用sigmoid作为非线性时，内部协变量转移的减少允许具有批标准化的深层网络被训练。的确，BN-x5-Sigmoid取得了69.8％的准确率。没有批标准化，使用sigmoid的Inception从未达到比1/1000准确率更好的结果。</p><h3 id="4-2-3-组合分类"><a href="#4-2-3-组合分类" class="headerlink" title="4.2.3. 组合分类"></a>4.2.3. 组合分类</h3><p>目前在ImageNet大型视觉识别竞赛中报道的最佳结果是传统模型（Wu et al。，2015）的Deep Image组合和（He等，2015）的组合模型。后者报告了ILSVRC测试服务器评估的<code>4.94％</code>的top-5错误率。这里我们在测试服务器上报告<code>4.82％</code>的测试错误率。这提高了以前的最佳结果，并且根据（Russakovsky等，2014）这超过了人类评估者的评估准确率。</p><p>对于我们的组合，我们使用了6个网络。每个都是基于BN-x30的，进行了以下一些修改：增加卷积层中的初始权重；使用Dropout（丢弃概率为5％或10％，而原始Inception为40％）；模型最后的隐藏层使用非卷积批标准化。每个网络在大约6⋅106个训练步骤之后实现了最大的准确率。组合预测是基于组成网络的预测类概率的算术平均。组合和多裁剪图像推断的细节与（Szegedy et al，2014）类似。</p><p>我们在图4中证实了批标准化使我们能够在ImageNet分类挑战基准上设置新的最佳结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gir2uu2kvcj315k0c6juh.jpg" srcset="/img/loading.gif" alt="image-20200915092741097"></p><blockquote><p>图4。批标准化Inception与以前的最佳结果在提供的包含5万张图像的验证集上的比较。组合结果是在测试集上由测试服务器评估的结果。BN-Inception组合在验证集的5万张图像上取得了<code>4.9% top-5</code>的错误率。所有报道的其它结果是在验证集上。</p></blockquote><h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h1><p>我们提出了一种新颖的机制，大大加快了深度网络的训练。他是基于前提协变量转移的，已知其会使机器学习系统的训练复杂化，也适用于子网络和层，并且从网络的内部激活中去除它可能有助于训练。这确保了标准化可以被用来训练网络的任何优化方法进行恰当的处理。为了让深度网络训练中常用的随机优化方法可用，我们对每个小批量数据执行标准化，并通过标准化参数来反向传播梯度。批标准化每个激活只增加了两个额外的参数，这样做可以保持网络的表示能力。我们提出了一个用于构建，训练和执行推断的批标准化网络算法。所得到的网络可以用饱和非线性进行训练，能更容忍增加的训练率，并且通常不需要Dropout来进行正则化。</p><p>仅仅将批标准化添加到了最新的图像分类模型中便在训练中取得了实质的加速。通过进一步提高学习率，删除Dropout和应用批标准化所提供的其它修改，我们只用了少部分的训练步骤就达到了以前的技术水平——然后在单网络图像分类中击败了最先进的技术。此外，通过组合多个使用批标准化训练的模型，我们在ImageNet上的表现显著优于最好的已知系统。</p><p>我们的方法与（Gülçehre＆Bengio，2013）的标准化层相似，尽管这两个方法解决的目标不同。批标准化寻求在整个训练过程中激活值的稳定分布，并且对非线性的输入进行归一化，因为这时更有可能稳定分布。相反，标准化层被应用于非线性的输出，这导致了更稀疏的激活。我们没有观察到非线性输入是稀疏的，无论是有批标准化还是没有批标准化。批标准化的其它显著差异包括学习到的缩放和转移允许BN变换表示恒等，卷积层处理以及不依赖于小批量数据的确定性推断。</p><p>在这项工作中，我们没有探索批标准化可能实现的全部可能性。我们的未来工作包括将我们的方法应用于循环神经网络（Pascanu et al.，2013），其中内部协变量转移和梯度消失或爆炸可能特别严重，这将使我们能够更彻底地测试假设标准化改善了梯度传播（第3.3节）。需要对批标准化的正则化属性进行更多的研究，我们认为这是BN-Inception中删除丢弃时我们观察到的改善的原因。我们计划调查批标准化是否有助于传统意义上的域自适应——即网络执行标准化是否能够更容易泛化到新的数据分布，也许仅仅是对总体均值和方差的重新计算（Alg.2）。最后，我们认为，该算法的进一步理论分析将允许更多的改进和应用。</p><blockquote><p>本文转自：<a href="https://blog.csdn.net/quincuntial/article/details/78124582" target="_blank" rel="noopener">https://blog.csdn.net/quincuntial/article/details/78124582</a></p><p>作者：Tyan</p></blockquote><h1 id="附：原文"><a href="#附：原文" class="headerlink" title="附：原文"></a>附：原文</h1><div class="row">    <embed src="/pdf/GoogLeNetV2.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GoogLeNet</tag>
      
      <tag>Batch Normalization</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（9.4)</title>
    <link href="/2020/09/04/9-4/"/>
    <url>/2020/09/04/9-4/</url>
    
    <content type="html"><![CDATA[<p>  本次阅读首先学习了CVPR2020上的SOTA目标检测方法：EfficientDet[1]。之前在学习SSD算法时，其网络结构采用了VGG16中的一些网络层，之前也在很多地方见过将VGG16作为对比参照。YOLO中也采用了GoogLeNet架构。此前的学习过程中一直对VGG16、GoogLeNet不太熟悉，故本次补充学习了4个基础的CNN经典模型：LeNet[2]、AlexNet[3]、VGGNet[4]、GoogLeNet[5]。</p><p><strong>参考文献：</strong></p><p>[1] Tan M, Pang R, Le Q V. Efficientdet: Scalable and efficient object detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 10781-10790.</p><p>[2]LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.</p><p>[3]Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.</p><p>[4]Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.</p><p>[5] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 1-9.</p><h1 id="1-EfficientDet"><a href="#1-EfficientDet" class="headerlink" title="1.  EfficientDet"></a>1.  EfficientDet</h1><p>这是Google Brain团队提出的目标检测方法，发表在CVPR2020上。提出了BiFPN和EfficientDet。BiFPN是一种加权双向特征金字塔网络，可快速的进行多尺度特征融合。此外，提出了一种复合缩放方法，可同时对所有主干网络、特征网络的深度、宽度、分辨率进行统一缩放。通过将EfficientNet的主干网络和所提出的BiFPN和复合缩放方法相结合，提出了一个新的目标检测器系列：EfficientDet，与之前的目标检测器相比，其准确率更高，参数和FLOPs更少。EfficientDet-D7在COCO上实现最先进的52.2 AP，使用52M参数和325B FLOPs，与之前的检测器相比，其FLOP减少了13倍– 42倍，参数缩小了4-9倍。（FLOPs：是floating point operations的缩写（s表复数），意指浮点运算数，可理解为计算量。可以用来衡量算法/模型的复杂度。）</p><p><strong>BiFPN：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9hltx2j30vf0b1wh8.jpg" srcset="/img/loading.gif" alt="img"></p><p>多尺度特征融合旨在聚合不同分辨率的特征。常规的自上而下的FPN固有地受到单向信息流的限制。为了解决这个问题，PANet添加了一个额外的自下而上的路径聚合网络，如图（b）所示。NAS-FPN 采用神经架构搜索来搜索更好的跨尺度特征网络拓扑，但是在搜索过程中需要数千个GPU小时，并且发现的网络不规则且难以解释或修改，如图（c）所示。作者研究发现PANet的精度比FPN和NAS-FPN更好，但需要更多的参数和计算成本。</p><p>为了提高模型效率，文章针对跨尺度连接提出了几种优化方法，形成BiFPN：</p><ul><li>删除只有一个输入边的节点。（如果一个节点只有一个输入边且没有特征融合，那么对于致力于融合不同特征的特征网络的贡献将较小。）</li><li>如果原始输入与输出节点处于同一级别，则在原始输入和输出节点之间添加一条额外的边。（以便在不增加Cost的情况下融合更多功能。）</li><li>将每个双向（自上而下和自下而上）路径视为一个特征网络层，并重复这一相同层多次以实现更高的水平的特征融合。</li></ul><p><strong>EfficientDet：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9o5f3tj30vt0bp0vg.jpg" srcset="/img/loading.gif" alt="img"></p><p>采用ImageNet预训练的EfficientNets作为骨干网络。使用BiFPN用作特征网络，从骨干网络中获取3-7级的特征，并反复应用自上而下和自下而上的双向特征融合。这些融合的特征被馈送到一个分类和Box预测网络，以分别生成目标类别和边界框预测，类和边框网络权重在所有级别的特征之间共享。</p><p><strong>复合缩放：</strong></p><p>  以前的工作主要是通过使用更大的主干网（例如ResNeXt或AmobaNet），使用更大的输入图像，或堆叠更多的FPN层来扩大基线检测器的规模。这些方法通常是无效的，因为它们只关注单个或有限的缩放维度。本文提出了一种新的目标检测复合缩放方法，它使用一个简单的复合系数φ来联合放大主干网、BiFPN网络、类/框网络和分辨率的所有维度。由于目标检测器比图像分类模型具有更大的缩放维度，所有维度的网格搜索成本高得令人望而却步。因此，文章使用了基于启发式的缩放方法，但仍然遵循联合放大所有维度的主要思想。</p><p><strong>骨干网络</strong>——重用EfficientNet-B0到B6的相同宽度/深度缩放系数，这样就可以重用ImageNet预训练CheckPoints。</p><p><strong>BiFPN网络</strong>——线性增加BiFPN深度Dbifpn， BiFPN 的宽度Wbifpn呈指数增长。文章使用一个数值列表{1.2、1.25、1.3、1.35、1.4、1.45}执行网格搜索，并选择最佳值1.35作为BiFPN的宽度缩放因子。</p><p>BiFPN宽度和深度按以下公式缩放：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giexboeqlbj317602qdj7.jpg" srcset="/img/loading.gif" alt="image-20200904210930570" style="zoom: 25%;" /></center><p><strong>框/类预测网络</strong>——将其宽度固定为与BiFPN相同（即Wpred = Wbifpn），但使用以下等式线性增加深度（层数）：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giexcb8dr9j318603a77s.jpg" srcset="/img/loading.gif" alt="image-20200904211007253" style="zoom:25%;" /></center><p><strong>输入图像分辨率</strong>——由于在BiFPN中使用了特征级别3-7，输入分辨率能被27=128整除，因此使用以下等式线性增加分辨率：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giexcrqku1j30zu03adib.jpg" srcset="/img/loading.gif" alt="image-20200904211033181" style="zoom:25%;" /></center><p>根据不同的φ值，将EfficientDet-D0（φ= 0）发展为D7（φ= 7），其中序号越大，缩放尺寸越大，训练所需参数越多，同时精确度越高。</p><p><strong>实验：</strong></p><p>  实验方面，作者分别测试了EfficientDet用于目标检测和语义分割。</p><p>目标检测方面，EfficientDet-D0达到了与YOLOv3相似的精度，FLOPs少了28倍。与RetinaNet 和Mask-RCNN 相比，EfficientDet-D1达到了相似的精度，参数减少了8倍，FLOP减少了21倍。在高精度体制下，EfficientDet也始终比NAS-FPN好，EfficientDet-D7实现了针对单模型单尺度的最先进的52.2AP（在测试集上）和51.8的 AP（在验证集上）。</p><p>语义分割方面，作者修改了EfficientDet模型进行测试。将特征级别{P2，P3，…，P7}保留在BiFPN中，仅将P2用于最终的每像素分类。基于EfficientDet-D4的模型使用ImageNet预训练的EfficientNet-B4骨干网（大小与ResNet-50相似）。在相同的单模型单标度设置下，作者的模型比DeepLabV3 +的现有技术提高了1.7％的精度，而FLOP减少了9.8倍。这些结果表明，Efficient-Det在语义分割方面也很有前途。</p><h4 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h4><p><strong>创新点：</strong></p><p>1） 提出了一种双向特征金字塔型网络（BiFPN），实现高效的双向跨尺度连接和更高水平的加权特征融合。</p><p>2） 提出了一种新的复合缩放方法，联合放大主干网、BiFPN网络、类/框网络和分辨率的所有维度。</p><p>3） 基于BiFPN，提出了EfficientDet系列检测模型，在目标检测和语义分割上均取得了良好的效果。</p><p><strong>优点：</strong></p><p>1） EfficientDet实现了速度和精度的极佳平衡，同时参数和FLOP更少。</p><p>2） 遵循one-stage设计，结构简单、高效。</p><p><strong>缺点：</strong></p><p>1） 设计基于经验法则，难以解释的参数多。</p><h1 id="2-LeNet"><a href="#2-LeNet" class="headerlink" title="2.  LeNet"></a>2.  LeNet</h1><p>LeNet 诞生于 1994 年，经过多次迭代，于1998年发表此篇论文，并最终命名为 LeNet5。LeNet5是最早的卷积神经网络之一，推动了深度学习领域的发展。LeNet5通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个架构也是目前大量神经网络架构的基础。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9n7k5zj30jg05dglx.jpg" srcset="/img/loading.gif" alt="img"></p><p>LeNet5由7层CNN（不包含输入层）组成，上图中输入的原始图像大小是32×32像素，卷积层用Ci表示，子采样层（pooling，池化）用Si表示，全连接层用Fi表示。</p><p><strong>1、C1层（卷积层）：6@28×28</strong></p><p>该层使用了6个卷积核，每个卷积核的大小为5×5，这样就得到了6个特征图。特征图大小为（32-5+1）×（32-5+1）= 28×28；由于使用了权值共享，所以使用相同卷积核每个神经元均使用的参数相同。参数个数为（5×5+1）×6= 156（其中5×5为卷积核大小，1为偏置参数，6为特征图数量）。每个特征图有28×28个神经元，每个神经元参数为5×5+1，共有6幅特征图，因此该层的连接数为（5×5+1）×6×28×28=122304。</p><p><strong>2、S2层（下采样层，池化层）：6@14×14</strong></p><p>池化单元大小为2×2，因此，6个特征图的大小经池化后即变为14×14。这一层的计算过程是：2×2 单元里的值相加，然后再乘以训练参数w，再加上一个偏置参数b（每一个特征图共享相同的w和b)，然后取sigmoid值（S函数：0-1区间），作为对应的该单元的值。每个特征图都共享相同的w和b两个参数，因此需要2×6=12个参数。每个特征图有14×14个神经元，每个池化单元连接数为2×2+1（1为偏置量），因此，该层的连接数为（2×2+1）×14×14×6 = 5880</p><p><strong>3、C3层（卷积层）：16@10×10</strong></p><p>C3层有16个卷积核，卷积核大小为5×5，故C3层的特征图大小为（14-5+1）×（14-5+1）= 10×10。C3与S2并不是全连接，而是按照下表部分连接。即本层每个特征图对应的卷积核，和上一层的多个特征图进行卷积。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9om7fjj307o031jrj.jpg" srcset="/img/loading.gif" alt="img"></p><p>计算过程为：用3个（或4个/5个）卷积模板分别与S2层的3个（或4个/5个）feature maps进行卷积，然后将卷积的结果相加求和，再加上一个偏置，再取sigmoid得出卷积后对应的feature map了。</p><p>权值数： (5x5x3+1)x6 + (5x5x4+1)x9 + 5x5x6+1 = 456 + 909+151 = 1516，</p><p>连接数： (5x5x3+1)x10x10x6+(5x5x4+1)x10x10x9+(5x5x6+1)x10x10 = 45600+90900+15100=151600</p><p>进行部分连接的原因是为了打破对称性，提取深层特征。因为特征不是对称的，因此需要打破这种对称，以提取到更重要的特征。</p><p><strong>4、S4（下采样层，池化层）：16@5×5</strong></p><p>池化单元大小为2×2，与C3层一样，共有16个特征图，特征图大小为5x5。与S2类似，所需要参数个数为16×2 = 32。连接数为（2×2+1）×5×5×16 = 2000.</p><p><strong>5、C5层（卷积层）：120</strong></p><p>该层有120个卷积核，每个卷积核的大小仍为5×5，因此有120个特征图。由于S4层的大小为5×5，而该层的卷积核大小也是5×5，因此特征图大小为（5-5+1）×（5-5+1）= 1×1。这样该层就刚好变成了全连接，这只是巧合，如果原始输入的图像比较大，则该层就不是全连接了。参数数目为120×（5×5×16+1） = 48120. 由于该层的特征图大小刚好为1×1，因此连接数和参数数目相同，为48120×1×1=48120。</p><p><strong>6、F6层（全连接层）：84</strong></p><p>  F6层之所以选84个单元，是由于其对应于一个7×12的比特图。该层有84个特征图，特征图大小与C5一样都是1×1，与C5层全连接。参数数量为（120+1）×84=10164。连接数与参数数量一样，也是10164。</p><p><strong>7、OUTPUT层（输出层）：10</strong></p><p>  输出层也是全连接层，共有10个节点，分别代表数字0到9。参数个数为84×10=840。连接数与参数个数一样，也是840。该层采用径向基函数（RBF）的网络连接方式。假设x是上一层的输入，y是RBF的输出，则RBF输出的计算方式是：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9jlkxsj30a4034aa2.jpg" srcset="/img/loading.gif" alt="img" style="zoom: 50%;" /></center><p>上式wij 的值由i的比特图编码确定。i从0到9，j取值从0到7x12-1。RBF输出的值越接近于0，表示当前网络输入的识别结果与字符i越接近。</p><h4 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a><strong>总结：</strong></h4><p>  LeNet-5 网络虽然很小，但是它包含了深度学习的基本模块：卷积层，池化层，全连接层。是其他深度学习模型的基础。</p><h1 id="3-AlexNet"><a href="#3-AlexNet" class="headerlink" title="3.  AlexNet"></a>3.  AlexNet</h1><p>这篇文章在2012年发表，作者是多伦多大学的Alex等人。文章中的模型参加的竞赛是ImageNet LSVRC-2010，该ImageNet数据集有1.2 million幅高分辨率图像，总共有1000个类别。在测试集上取得了37.5％和17.0％的top-1和top-5的错误率，这样的结果在当时已经超过了之前的先进水平。AlexNet具有6000万个参数和650,000个神经元的神经网络，由五个卷积层，三层全连接网络，最终的输出层是1000通道的softmax。为了加快训练速度，作者使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，采用了Dropout正则化方法，该方法证明是非常有效的。AlexNet利用了两块GPU进行计算，大大提高了运算效率，并且在ILSVRC-2012竞赛中获得了top-5测试的15.3%错误率，此前获得第二名的方法的错误率是 26.2%，这在当时给学术界和工业界带来很大冲击。</p><p><strong>整体结构：</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9jzmxzj319a0em772.jpg" srcset="/img/loading.gif" alt="img"></p><p>网络结构图包含上下两个部分，分别对应两个GPU（只在特定的网络层两块GPU才进行交互）。网络包含了八层权重;前五个是卷积层，其余三个为全连接层。最后的全连接层的输出被送到1000维的softmax函数，其产生1000个类分布的预测。第一个卷积层的输入为224×224×3的图像，对其使用96个大小为11×11×3、步长为4（步长表示内核映射中相邻神经元感受野中心之间的距离）的内核来处理输入图像。第二个卷积层将第一个卷积层的输出（已进行响应归一化以及池化）作为输入，并使用256个内核处理图像，每个内核大小为5×5×48。第三个、第四个和第五个卷积层彼此连接而中间没有任何池化或归一化层。第三个卷积层有384个内核，每个的大小为3×3×256，其输入为第二个卷积层的输出(已归一化和池化）。第四个卷积层有384个内核，每个内核大小为3×3×192。第五个卷积层有256个内核，每个内核大小为3×3×192。全连接层各有4096个神经元。</p><p><strong>ReLU激活函数：</strong></p><p>传统的神经网络普遍使用Sigmoid或者tanh等非线性函数作为激励函数，然而它们容易出现梯度弥散或梯度饱和的情况。当输入的值非常大或非常小的时候，Sigmoid函数的函数值趋于不变，导数趋于0，这样在进行反向传播时，多个很小的导数累积起来，将导致梯度越来越小，权重更新较慢，神经网络变的难以学习。</p><p>在AlexNet中，作者使用了ReLU激活函数，函数式为f(x)=max(0,x)，当输入<0时，输出是0；当输入>0时，输出等于输入。由于ReLU是线性的，且导数始终为1，计算量大大减少，收敛速度会比Sigmoid/tanh快。</p><p><strong>数据增强：</strong></p><p> 神经网络由于训练的参数较多，需要比较多的训练数据，否则很容易过拟合。减小过拟合的最简单且最常用的方法就是，使用标签保留转换（label-preserving transformations）人为地放大数据集。</p><p>AlexNet使用以下操作进行数据增强：</p><ol><li><p>随机裁剪，对256×256的图片进行随机裁剪到224×224，然后进行水平翻转。在测试时，提取5个224×224的图像块（四个角块和中心块）以及它们的水平映射（总共包括10个块）来进行预测，并通过十个块上的softmax层求预测结果的均值。</p></li><li><p>测试时，对左上、右上、左下、右下、中间分别做了5次裁剪,共10个裁剪，然后翻转，之后对结果求平均。</p></li><li><p>对RGB空间做PCA（主成分分析），然后对主成分做一个（0, 0.1）的高斯扰动，也就是对颜色、光照作变换，这使错误率又降低了1%。</p></li></ol><p><strong>重叠池化：</strong></p><p>  在传统池化方法中，相邻池化单元之间互不重叠，池化窗口的大小与步长相同。更准确地说，一个池化层可以被认为是由一些间隔为s个像素的池化单元组成的网格，每个都表示了一个以池化单元的位置为中心的大小为z×z的邻域。如果令s = z，就可以得到CNN中常用的传统的局部池化。如果令s&lt;z，我们得到重叠池化。AlexNet在整个网络中使s=2,z=3，即重叠池化。与非重叠方案s= 2，z = 2相比，重叠池化将top-1和top-5的错误率分别降低了0.4％和0.3％。同时作者发现，有重叠池化的模型很难过拟合。</p><p><strong>局部响应归一化：</strong></p><p>  此前使用sigmoid或tanh激活函数的值域都是有范围的，但是AlexNet使用ReLU函数作为激活函数，其值域可以非常大，没有一个固定的区间。所以需要对ReLU得到的结果进行归一化。归一化（normalization）的目的是“抑制”，使用局部归一化的方案有助于增加泛化能力。LRN的核心思想就是利用临近的数据做归一化，LRN将top-1和top-5的错误率分别降低了1.4％和1.2％。</p><p><strong>DropOut：</strong></p><p>  DropOut是AlexNet中一个很大的创新，是现在神经网络中的必备结构之一。引入Dropout主要是为了防止过拟合。DropOut会以50%的概率将隐含层的神经元输出置为0,被置0的神经元不参与网络的前馈和反向传播。因此，每次给网络提供了输入后，神经网络都会采用一个不同的结构，但是这些结构都共享权重。这种技术减少了神经元复杂的共同适应，因为神经元无法依赖于其他特定的神经元而存在。所以，它被迫学习更强大更鲁棒的功能，使得这些神经元可以与其他神经元的许多不同的随机子集结合使用。</p><p><strong>多GPU训练：</strong></p><p>  目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。AlexNet当时使用了GTX580的GPU进行训练，由于单个GTX 580 GPU只有3GB内存，这限制了在其上训练的网络的最大规模，因此他们在每个GPU中放置一半核（或神经元），将网络分布在两个GPU上进行并行计算，大大加快了AlexNet的训练速度，GPU只在某些特定层间才进行通信。</p><h4 id="总结：-2"><a href="#总结：-2" class="headerlink" title="总结："></a><strong>总结：</strong></h4><p>创新点：</p><ul><li>使用了非线性激活函数ReLU、多个GPU训练，提高训练速度。</li><li>采用DropOut方法、数据增强方法，防止过拟合。</li><li>改进池化方法，使用重叠池化，提高精度防止过拟合。</li></ul><h1 id="4-VGGNet"><a href="#4-VGGNet" class="headerlink" title="4.  VGGNet"></a><strong>4.</strong>  <strong>VGGNet</strong></h1><p>VGGNet由牛津大学计算机视觉组（Visual Geometry Group）和Google DeepMind公司于2014年提出。VGGNet探索了卷积神经网络的深度与其性能之间的关系，成功地构筑了16~19层深的卷积神经网络，证明了增加网络的深度能够在一定程度上影响网络最终的性能，使错误率大幅下降，同时拓展性又很强，迁移到其它图片数据上的泛化性也非常好。VGGNet获得了ILSVRC 2014年比赛的亚军和定位项目的冠军，在top5上的错误率为7.5%。目前为止，VGGNet依然被用来提取图像的特征。</p><p>VGGNet全部使用3<em>3的卷积核和2</em>2的池化核，通过不断加深网络结构来提升性能。网络层数的增长并不会带来参数量上的爆炸，因为参数量主要集中在最后三个全连接层中。同时，两个3<em>3卷积层的串联相当于1个5</em>5的卷积层，3个3<em>3的卷积层串联相当于1个7</em>7的卷积层，即3个3<em>3卷积层的感受野大小相当于1个7</em>7的卷积层。但是3个3<em>3的卷积层参数量只有7</em>7的一半左右，同时前者可以有3个非线性操作，而后者只有1个非线性操作，这样使得前者对于特征的学习能力更强。使用多个较小卷积核的卷积层代替一个卷积核较大的卷积层，一方面可以减少参数，另一方面相当于进行了更多的非线性映射，增加了网络的拟合表达能力。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9ldsw6j30u00uqte0.jpg" srcset="/img/loading.gif" alt="img"></p><p>VGGNET的网络结构如图所示，VGGNET包含多层网络，深度从11层到19层不等，较为常用的是VGG16和VGG19。VGGNet把网络分成了5段，每段都把多个3<em>3的卷积网络串联在一起，每段卷积后面接一个最大池化层，最后面是3个全连接层和一个softmax层。在这篇论文中分别使用了A、A-LRN、B、C、D、E这6种网络结构进行测试，这6种网络结构相似，都是由5层卷积层、3层全连接层组成，其中区别在于每个卷积层的子层数量不同，从A至E依次增加（子层数量从1到4），总的网络深度从11层到19层（添加的层以粗体显示），表格中的卷积层参数表示为“conv⟨感受野大小⟩-通道数⟩”。其中，网络结构D就是著名的VGG16，网络结构E就是著名的VGG19。在VGGNet中每层卷积层中包含2~4个卷积操作，卷积核的大小是3</em>3，卷积步长是1，池化核是2*2，步长为2。VGGNet最明显的改进就是降低了卷积核的尺寸，增加了卷积的层数。</p><p>作者通过网络A-LRN发现，AlexNet曾经用到的LRN层（local response normalization，局部响应归一化）并没有带来性能的提升，因此在其它组的网络中均没再出现LRN层。VGGNet使用了Multi-Scale的方法做数据增强，将原始图像缩放到不同尺寸S，然后再随机裁切224′224的图片，这样能增加很多数据量，对于防止模型过拟合有很不错的效果。实践中，作者令S在[256,512]这个区间内取值，使用Multi-Scale获得多个版本的数据，并将多个版本的数据合在一起进行训练。</p><h4 id="总结：-3"><a href="#总结：-3" class="headerlink" title="总结："></a><strong>总结：</strong></h4><p>  创新点：</p><ul><li>使用更小的3x3的卷积核和更深的网络。减少参数，增加非线性映射，提升对特征的学习能力。</li><li>验证了几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）更好。证明了分类任务可以通过使用小的卷积核、增加CNN的深度来提高精度。</li><li>在VGGNet的卷积结构中，引入1*1的卷积核，在不影响输入输出维度的情况下，引入非线性变换，增加网络的表达能力，降低计算量。</li><li>训练时，先训练级别简单（层数较浅）的VGGNet的A级网络，然后使用A网络的权重来初始化后面的复杂模型，加快训练的收敛速度。</li><li>采用了Multi-Scale的方法来训练和预测。可以增加训练的数据量,防止模型过拟合,提升预测准确率。</li></ul><p>优点：</p><ul><li>通过增加网络深度有效提升性能。</li><li>结构简洁，整个VGG16网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。</li></ul><p>缺点：</p><ul><li>耗费更多的计算资源，内存占用大</li></ul><h1 id="5-GoogLeNet（Inception-V1）"><a href="#5-GoogLeNet（Inception-V1）" class="headerlink" title="5.GoogLeNet（Inception V1）"></a>5.GoogLeNet（Inception V1）</h1><p>2014年的ImageNet挑战赛(ILSVRC14)，GoogLeNet获得了第一名、VGG获得了第二名，这两类模型结构的共同特点是层次更深了。VGG继承了LeNet以及AlexNet的一些框架结构，而GoogLeNet则做了更加大胆的网络结构尝试，虽然深度只有22层，但大小却比AlexNet和VGG小很多，GoogLeNet参数为500万个，AlexNet参数个数是GoogleNet的12倍，VGGNet参数又是AlexNet的3倍，因此在内存或计算资源有限时，GoogleNet是比较好的选择；从模型结果来看，GoogLeNet的性能却更加优越。</p><p>之前的AlexNet、VGG等结构都是通过增大网络的深度（层数）来获得更好的训练效果，但层数的增加会带来很多负作用：当参数过多，训练数据有限时，很容易产生过拟合。此外还有计算复杂度大、梯度消失、梯度爆炸等。解决这些问题的方法当然就是在增加网络深度和宽度的同时减少参数，inception的提出从另一种角度来提升训练结果：更高效的利用计算资源，在相同的计算量下能提取到更多的特征，从而提升训练结果。Inception通过构造一种“基础神经元”结构，来搭建一个稀疏性、高计算性能的网络结构。其历经了V1、V2、V3、V4多个版本的发展，本次主要学习其V1版本。</p><p><strong>Inception architecture：</strong></p><p>此前的神经网络（AlexNet、VGG），都是只有一条主线，而IA是“分叉-汇聚”型网络，在一层网络中存在多个不同尺度的kernels，卷积完毕后再汇聚。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9kxt7uj30pe0cg759.jpg" srcset="/img/loading.gif" alt="img"></p><p>Inception模块的基本机构如图所示，整个inception结构就是由多个这样的inception模块串联起来的。该结构将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加，之所以选1、3、5是为了方便对齐，只需将padding设为0，1，2，步长都为1即可获得相同尺寸的输出以叠加），这样一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。网络卷积层能够提取输入的每一个细节信息，同时5x5的滤波器也能够覆盖大部分接受层的的输入。池化操作可以减少空间大小，降低过度拟合。在每一个卷积层后都要做一个ReLU操作，以增加网络的非线性特征。</p><p>以上这个Inception原始版本，所有的卷积核都直接在上一层的所有输出上来做。这样5x5的卷积核所需的计算量太大，造成了特征图的厚度很大。为了避免这种情况，在3x3前、5x5前、max pooling后分别加上了1x1的卷积核，以起到降低特征图厚度、增加网络的非线性程度的作用，这也就形成了Inception v1的网络结构，如下图所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1giex9mbvohj30wc0gwtaj.jpg" srcset="/img/loading.gif" alt="img"></p><p>inception结构的主要贡献有两个：1.使用1x1的卷积来进行降维。2.在多个尺寸上同时进行卷积再聚合。</p><p><strong>辅助分类器：</strong></p><p>在神经网络训练过程中，往往训练到最后，网络最开始的几层就“训不动了”，即出现了梯度消失问题。GoogLeNet加入了两个auxiliary classifiers（简称AC），用于辅助训练，加速网络卷积。这两个AC在训练的时候也跟着学习，同时把自己学习到的梯度反馈给网络。</p><p>辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重（0.3）加到最终分类结果中，这样相当于做了模型融合，同时给网络增加了反向传播的梯度信号，一定程度解决了更深网络带来的梯度消失问题。也提供了额外的正则化，有利于整个网络的训练。在实际测试的时候，这两个额外的AC会被去掉。</p><p><strong>取消全连接层：</strong></p><p>VGG网络之所以参数众多、占用资源大，主要是因为最后有两个4096的全连接层。为了压缩网络参数，GoogLeNet网络的最后采用了average pooling（平均池化层）来代替全连接层，可以将准确率提高0.6%。但是，在网络最后还是加了一个全连接层，主要是为了方便对输出进行灵活调整,方便他人进行迁移学习。虽然移除了全连接，但是网络中依然使用了Dropout。</p><h4 id="总结：-4"><a href="#总结：-4" class="headerlink" title="总结："></a><strong>总结：</strong></h4><p>主要贡献：</p><ul><li>提出Inception architecture 并对其进行优化。</li><li>取消全连接层，使用平均池化层替代。压缩网络参数。</li><li>使用Auxiliary Classifiers加速网络卷积。</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GoogLeNet</tag>
      
      <tag>EfficientDet</tag>
      
      <tag>LeNet</tag>
      
      <tag>AlexNet</tag>
      
      <tag>VGGNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>网站新域名htx1998.cn正式启用</title>
    <link href="/2020/09/02/newblog/"/>
    <url>/2020/09/02/newblog/</url>
    
    <content type="html"><![CDATA[<p><a href="http://htx1998.ml" target="_blank" rel="noopener">之前的网站</a>部署在国外服务器上，导致在国内访问时速度较慢，而且在微信中访问时会弹出中间页，提示”非微信官方网页”，这导致浏览体验变得极差。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gichb6iw5wj30u01t0grh.jpg" srcset="/img/loading.gif" alt="image-20200902182416296" style="zoom: 25%;" /></center><p>因此，考虑将博客迁移到国内的阿里云，同时进行合法的ICP备案、公安备案，做一个守法的好公民。</p><h4 id="1-域名和虚拟云主机的购买，新的域名为：htx1998-cn"><a href="#1-域名和虚拟云主机的购买，新的域名为：htx1998-cn" class="headerlink" title="1. 域名和虚拟云主机的购买，新的域名为：htx1998.cn"></a>1. 域名和虚拟云主机的购买，新的域名为：htx1998.cn</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gichan8lxzj30jp0rjtg7.jpg" srcset="/img/loading.gif" alt="htx1998.cn-3"></p><h4 id="2-网站备案"><a href="#2-网站备案" class="headerlink" title="2. 网站备案"></a>2. 网站备案</h4><p>之后需要在工业和信息化部申请实名备案，网站才能正常解析。然而备案的过程并非一帆风顺，期间由于我之前有过一个网站备案，需要先将之前的主体注销，才能注册新的。第二次申请由于备注信息填写的不完整，导致被驳回。第三次申请，终于在2020年9月2日获得工信部的批准，取得了ICP备案号：豫ICP备2020026254号。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gichep1lvrj311m0u0qab.jpg" srcset="/img/loading.gif" alt=""></p><center> <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gichf96iidj30u01asdsx.jpg" srcset="/img/loading.gif" alt="image-20200902182810990" style="zoom:33%;" /></center><p>至此，网站迁移工作全部完成，经测试，访问速度大幅提升，分享到微信也没有中间页弹出。</p><p>都说博客是“孤芳自赏”，建站不易，还是希望大家没事多来踩踩。我会在我的博客里分享我的学习笔记，也会多分享一些生活中有意思的事情，毕竟只有学术的东西太过单调乏味。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——AlexNet</title>
    <link href="/2020/08/29/AlexNet/"/>
    <url>/2020/08/29/AlexNet/</url>
    
    <content type="html"><![CDATA[<h1 id="ImageNet-Classification-with-Deep-Convolutional-Neural-Networks"><a href="#ImageNet-Classification-with-Deep-Convolutional-Neural-Networks" class="headerlink" title="ImageNet Classification with Deep Convolutional Neural Networks"></a>ImageNet Classification with Deep Convolutional Neural Networks</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们训练了一个庞大的深层卷积神经网络，将ImageNet LSVRC-2010比赛中的120万张高分辨率图像分为1000个不同的类别。在测试数据上，我们取得了37.5％和17.0％的top-1和top-5的错误率，这比以前的先进水平要好得多。具有6000万个参数和650,000个神经元的神经网络由五个卷积层组成，其中一些随后是最大池化层，三个全连接层以及最后的1000个softmax输出。为了加快训练速度，我们使用非饱和神经元和能高效进行卷积运算的GPU实现。为了减少全连接层中的过拟合，我们采用了最近开发的称为“dropout”的正则化方法，该方法证明是非常有效的。我们还在ILSVRC-2012比赛中使用了这种模型的一个变种，取得了15.3％的top-5测试失误率，而第二名的成绩是26.2％。</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>现有的目标检测方法本质上使用机器学习方法。为了改善他们的表现，我们可以收集更大的数据集，训练更强大的模型，并使用更好的技术来防止过拟合。直到最近，标记好图像的数据集相对还较小——大约上万的数量级（例如，NORB [16]，Caltech-101/256 [8,9]和CIFAR-10/100 [12]）。使用这种规模的数据集可以很好地解决简单的识别任务，特别是如果他们增加了保留标签的转换。例如，目前MNIST数字识别任务的最低错误率（&lt;0.3％）基本达到了人类的识别水平[4]。但是物体在现实环境中可能表现出相当大的变化性，所以要学会识别它们，就必须使用更大的训练集。事实上，小图像数据集的缺点已是众所周知（例如，Pinto[21]），但直到最近才可以收集到数百万的标记数据集。新的大型数据集包括LabelMe [23]，其中包含数十万个完全分割的图像，以及ImageNet [6]，其中包含超过15,000万个超过22,000个类别的高分辨率图像。</p><p>要从数百万图像中学习数千个类别，我们需要一个具有强大学习能力的模型。然而，目标识别任务的巨大复杂性意味着即使是像ImageNet这样大的数据集也不能完美地解决这个问题，所以我们的模型也需要使用很多先验知识来弥补我们数据集不足的问题。卷积神经网络（CNN）就构成了一类这样的模型[16,11,13,18,15,22,26]。它们的容量可以通过改变它们的深度和宽度来控制，并且它们也对图像的性质（即统计量的定态假设以及像素局部依赖性假设）做出准确而且全面的假设。因此，与具有相同大小的层的标准前馈神经网络相比，CNN具有更少的连接和参数，因此它们更容易训练，而其理论最优性能可能稍微弱一些。</p><p>尽管CNN具有很好的质量，并且尽管其局部结构的效率相对较高，但将它们大规模应用于高分辨率图像时仍然显得非常昂贵。幸运的是，当前的GPU可以用于高度优化的二维卷积，能够加速许多大型CNN的训练，并且最近的数据集（如ImageNet）包含足够多的标记样本来训练此类模型，而不会出现严重的过拟合。</p><p>本文的具体贡献如下：我们在ILSVRC-2010和ILSVRC-2012比赛中使用的ImageNet子集上训练了迄今为止最大的卷积神经网络之一[2]，并在这些数据集上取得了迄今为止最好的结果。我们编写了一个高度优化的2D卷积的GPU实现以及其他训练卷积神经网络的固有操作，并将其公开。我们的网络包含许多新的和不同寻常的功能，这些功能可以提高网络的性能并缩短训练时间，详情请参阅第3节。我们的网络规模较大，即使有120万个带标签的训练样本，仍然存在过拟合的问题，所以我们采用了几个有效的技巧来阻止过拟合，在第4节中有详细的描述。我们最终的网络包含五个卷积层和三个全连接层，并且这个深度似乎很重要：我们发现去除任何卷积层（每个卷积层只包含不超过整个模型参数的1%的参数）都会使网络的性能变差。</p><p>最后，网络的规模主要受限于目前GPU上可用的内存量以及我们可忍受的训练时间。我们的网络在两块GTX 580 3GB GPU上需要花费五到六天的时间来训练。我们所有的实验都表明，通过等待更快的GPU和更大的数据集出现，我们的结果可以进一步完善。</p><h1 id="2-数据集"><a href="#2-数据集" class="headerlink" title="2.数据集"></a>2.数据集</h1><p>ImageNet是一个拥有超过1500万个已标记高分辨率图像的数据集，大概有22,000个类别。图像都是从网上收集，并使用Amazon-Mechanical Turk群智工具人工标记。从2010年起，作为Pascal视觉对象挑战赛的一部分，这是每年举办一次的名为ImageNet大型视觉识别挑战赛（ILSVRC）的比赛。 ILSVRC使用的是ImageNet的一个子集，每1000个类别中大约有1000个图像。总共有大约120万张训练图像，50,000张验证图像和150,000张测试图像。</p><p>ILSVRC-2010是ILSVRC中的唯一可以使用测试集标签的版本，因此这也正是我们进行大部分实验的版本。由于我们也在ILSVRC-2012比赛中引入了我们的模型，因此在第6部分中，我们也会给出此版本数据集的结果，尽管这个版本的测试集标签不可用。在ImageNet上，习惯上使用两种错误率：top-1和top-5，其中top-5错误率是正确标签不在被模型认为最可能的五个标签之中的测试图像的百分率。</p><p>mageNet由可变分辨率的图像组成，而我们的系统需要固定的输入尺寸。因此，我们将图像下采样到256×256的固定分辨率。给定一个矩形图像，我们首先重新缩放图像，使得短边长度为256，然后从结果中裁剪出中心的256×256的图片。除了将每个像素中减去训练集的像素均值之外，我们没有以任何其他方式对图像进行预处理。所以我们在像素的（中心）原始RGB值上训练了我们的网络。</p><h1 id="3-结构"><a href="#3-结构" class="headerlink" title="3.结构"></a>3.结构</h1><h2 id="3-1ReLU非线性单元"><a href="#3-1ReLU非线性单元" class="headerlink" title="3.1ReLU非线性单元"></a>3.1ReLU非线性单元</h2><p>对一个神经元模型的输出的常规套路是，给他接上一个激活函数：$f(x)=tanh(x)$或者$f(x)=(1+e^{−x})^{−1}$.就梯度下降法的训练时间而言，这些饱和非线性函数比非饱和非线性函数如$f(x)=max(0,x)$慢得多。根据Nair和Hinton的说法[20]，我们将这种非线性单元称为——修正非线性单元（Rectified Linear Units (ReLUs)）。使用ReLUs做为激活函数的卷积神经网络比起使用tanh单元作为激活函数的训练起来快了好几倍。这个在图1中可被证明，该图展示了对于一个特定的四层CNN，CIFAR-10数据集训练中的误差率达到25%所需要的迭代次数。从这张图的结果可以看出，如果我们使用传统的饱和神经元模型来训练CNN，那么我们将无法为这项工作训练如此大型的神经网络。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gia3wdqpmmj30k60gkmxv.jpg" srcset="/img/loading.gif" alt="image-20200831170906584" style="zoom:50%;" /></center><blockquote><p>图1：带有ReLUs的四层卷积神经网络（实线）在CIFAR-10上达到25％的训练错误率，是具有tanh的同等神经网络（虚线）的六倍。每个网络的学习率是独立选择的，以使训练尽快。没有使用任何形式的正则化。这里展示的效果的大小随网络架构的不同而变化，但是具有ReLU的网络持续学习的速度要比具有饱和神经元的等效学习速度快几倍。</p></blockquote><p>我们并不是第一个考虑在CNN中替换掉传统神经元模型的。例如，Jarrett等人[11]声称，非线性函数$f(x)=|tanh(x)|$在他们的对比度归一化问题上，再接上局部均值池化单元，在Caltech-101数据集上表现的非常好。然而，在这个数据集中，主要担心的还是防止过拟合，所以他们观察到的效果与我们在使用ReLU时观察到的训练集的加速能力还是不一样。加快训练速度对大型数据集上训练的大型模型的性能有很大的影响。</p><h2 id="3-2-在多GPU上训练"><a href="#3-2-在多GPU上训练" class="headerlink" title="3.2 在多GPU上训练"></a>3.2 在多GPU上训练</h2><p>单个GTX 580 GPU只有3GB内存，这限制了可以在其上训练的网络的最大尺寸。事实证明，120万个训练样本足以训练那些因规模太大而不适合使用一个GPU训练的网络。因此，我们将网络分布在两个GPU上。目前的GPU很适合于跨GPU并行化操作，因为它们能够直接读写对方的内存，而无需通过主机内存。我们采用的并行化方案基本上将半个内核（或神经元）放在各个GPU上，另外还有一个技巧：GPU只在某些层间进行通信。这意味着，例如，第3层的内核从第2层的所有内核映射（kernel maps）中获取输入。然而，第4层中的内核又仅从位于同一GPU上的第3层中的那些内核映射获取输入。选择连接模式对于交叉验证是一个不小的问题，但这使得我们能够精确调整通信量，直到它的计算量的达到可接受的程度。</p><p>由此产生的架构有点类似于Ciresan等人使用的“柱状”CNN[5]，除了我们的每列不是独立的之外（见图2）。与一个GPU上训练的每个卷积层只有一半的内核数量的网络相比，该方案分别将我们的top-1和top-5错误率分别降低了1.7％和1.2％。双GPU网络的训练时间比单GPU网络更少。</p><h2 id="3-3-局部响应归一化"><a href="#3-3-局部响应归一化" class="headerlink" title="3.3 局部响应归一化"></a>3.3 局部响应归一化</h2><p>ReLU具有理想的属性，它们不需要对输入进行归一化来防止它们饱和。如果至少有一些训练实例为ReLU产生了正的输入，那么这个神经元就会学习。设$a^i_{x,y}$表示第i个内核计算(x,y)位置的ReLU非线性单元的输出，而响应归一化（Local Response Normalization）的输出值定义为$b^i_{x,y}$：</p><script type="math/tex; mode=display">b^i_{x,y}=\frac { a^i_{x,y} }{ (k+\alpha\sum_{j=max(0,i-n/2) }^{ min(N-1,i+n/2) }(\alpha^j_{x,y})^2)^\beta }</script><p>其中，求和部分公式中的nn表示同一个位置下与该位置相邻的内核映射的数量，而NN表示这一层所有的内核数（即通道数）。内核映射的顺序当然是任意的，并且在训练之前就已经定好了。这种响应归一化实现了一种模仿真实神经元的横向抑制，从而在使用不同内核计算的神经元输出之间产生较大的竞争。常数k、n、α和β都是超参数（hyper-parameters），它们的值都由验证集决定。我们取k=2、n=5、$α=10^{−4}$、β=0.75。我们在某些层的应用ReLU非线性单元后再使用这种归一化方法（参见第3.5节）。</p><p>这个方案与Jarrett等人[11]的局部对比归一化方案有些相似之处，但我们的被更准确地称为“亮度归一化”，因为我们没有减去均值。响应归一化将我们的top-1和top-5的错误率分别降低了1.4％和1.2％。我们还验证了这种方案在CIFAR-10数据集上的有效性：没有进行归一化的四层CNN实现了13％的测试错误率，而进行了归一化的则为11％。</p><h2 id="3-4-重叠池化"><a href="#3-4-重叠池化" class="headerlink" title="3.4 重叠池化"></a>3.4 重叠池化</h2><p>CNN中的池化层汇集了相同内核映射中相邻神经元组的输出。在传统方法中，相邻池化单元之间互不重叠（例如[17,11,4]）。更准确地说，一个池化层可以被认为是由一些间隔为s个像素的池化单元组成的网格，每个都表示了一个以池化单元的位置为中心的大小为z×z的邻域。如果我们令s = z，我们就可以得到CNN中常用的传统的局部池化。如果令s&lt;z，我们得到重叠池化。我们在整个网络中使s=2,z=3。与非重叠方案s= 2，z = 2相比，此方案将top-1和top-5的错误率分别降低了0.4％和0.3％，这产生了等效尺寸的输出。我们在训练中观察到，有重叠池化的模型很难过拟合。</p><h2 id="3-5-整体结构"><a href="#3-5-整体结构" class="headerlink" title="3.5 整体结构"></a>3.5 整体结构</h2><p>现在我们已经准备好描述CNN的整体架构了。如图2所示，这个网络包含了八层权重;前五个是卷积层，其余三个为全连接层。最后的全连接层的输出被送到1000维的softmax函数，其产生1000个类分布的预测。我们的网络最大化多项逻辑回归目标，这相当于在预测的分布下最大化训练样本中正确标签对数概率的平均值。</p><p>第二，第四和第五个卷积层的内核仅与上一层存放在同一GPU上的内核映射相连（见图2）。第三个卷积层的内核连接到第二层中的所有内核映射。全连接层中的神经元连接到前一层中的所有神经元。响应归一化层紧接着第一个和第二个卷积层。  在3.4节中介绍的最大池化层，后面连接响应归一化层以及第五个卷积层。将ReLU应用于每个卷积层和全连接层的输出。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi94fti718j319a0em77q.jpg" srcset="/img/loading.gif" alt="image-20200830204209962"></p><blockquote><p>图2：一个我们CNN结构的插图，明确地显示两个gpu之间职责的划分。一个GPU运行在图形顶部的分层部分，而另一个运行图形底部的分层部分。GPU只在某些层上通信。网络的输入为150,528维，网络其余层中的神经元数量为253,440–186,624–64,896–64,896–43,264–4096–4096-1000。</p></blockquote><p>第一个卷积层的输入为224×224×3的图像，对其使用96个大小为11×11×3、步长为4（步长表示内核映射中相邻神经元感受野中心之间的距离）的内核来处理输入图像。第二个卷积层将第一个卷积层的输出（已进行响应归一化以及池化）作为输入，并使用256个内核处理图像，每个内核大小为5×5×48。第三个、第四个和第五个卷积层彼此连接而中间没有任何池化或归一化层。第三个卷积层有384个内核，每个的大小为3×3×256，其输入为第二个卷积层的输出(已归一化和池化）。第四个卷积层有384个内核，每个内核大小为3×3×192。第五个卷积层有256个内核，每个内核大小为3×3×192。全连接层各有4096个神经元。</p><h1 id="4-减少过拟合"><a href="#4-减少过拟合" class="headerlink" title="4. 减少过拟合"></a>4. 减少过拟合</h1><p>我们的神经网络架构拥有6000万个参数。尽管ILSVRC的1000个类别使得每个训练样本从图像到标签的映射被限制在了10 bit之内，但这不足以保证训练这么多参数而不出现过拟合。下面，我们将介绍对付过度拟合的两个方法。</p><h2 id="4-1-数据增强"><a href="#4-1-数据增强" class="headerlink" title="4.1 数据增强"></a>4.1 数据增强</h2><p>减小过拟合的最简单且最常用的方法就是，使用标签保留转换（label-preserving transformations，例如[25,4,5]），人为地放大数据集。我们采用两种不同形式的数据增强方法，它们都允许通过很少的计算就能从原始图像中生成转换图像，所以转换后的图像不需要存储在硬盘上。在我们实现过程中，转换后的图像是使用CPU上的Python代码生成的，在生成这些转换图像的同时，GPU还在训练上一批图像数据。所以这些数据增强方案在计算上实际上是很高效的。</p><p>数据增强的第一种形式包括平移图像和水平映射。我们通过从256×256图像中随机提取224×224的图像块（及其水平映射）并在这些提取的图像块上训练我们的网络来做到这一点。如果没有这个方案，我们的网络就可能会遭受大量的的过拟合，可能会迫使我们不得不使用更小的网络。在测试时，网络通过提取5个224×224的图像块（四个角块和中心块）以及它们的水平映射（因此总共包括10个块）来进行预测，并通过十个块上的softmax层求预测结果的均值。</p><p>第二种形式的数据增强包括改变训练图像中RGB通道的亮度。具体而言，我们在整个ImageNet训练集的图像的RGB像素值上使用PCA。对于每个训练图像，我们增加多个通过PCA找到的主成分，大小与相应的特征值成比例，乘以一个随机值，该随机值属于均值为0标准差为0.1的高斯分布。因此，对于每个图像的RGB像素有$I_{xy}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]^T$，我们加入如下的值：</p><script type="math/tex; mode=display">[P1,P2,P3][\alpha_1\lambda_1,\alpha_2\lambda_2,\alpha_3\lambda_3]^T</script><p>其中，$P_i$和$λ_i$分别是3x3的RGB协方差矩阵的第i个特征向量和第i个的特征值，而$α_i$是前面所说的随机值。对于一张特定训练图像中的所有像素，每个$α_i$只会被提取一次，直到这张图片再次用于训练时，才会重新提取随机变量。这个方案近似地捕捉原始图像的一些重要属性，对象的身份不受光照的强度和颜色变化影响。这个方案将top-1错误率降低了1％以上。</p><h2 id="4-2-Dropout"><a href="#4-2-Dropout" class="headerlink" title="4.2 Dropout"></a>4.2 Dropout</h2><p>结合许多不同模型的预测结果是减少测试错误率的一种非常成功的方法[1,3]，但对于已经花费数天时间训练的大型神经网络来说，它似乎成本太高了。然而，有一种非常有效的<strong>模型组合方法</strong>，在训练期间，只需要消耗1/2的参数。这个新发现的技术叫做“Dropout”[10]，它会以50%的概率将隐含层的神经元输出置为0。以这种方法被置0的神经元不参与网络的前馈和反向传播。因此，每次给网络提供了输入后，神经网络都会采用一个不同的结构，但是这些结构都共享权重。这种技术减少了神经元复杂的共同适应，因为神经元无法依赖于其他特定的神经元而存在。因此，它被迫学习更强大更鲁棒的功能，使得这些神经元可以与其他神经元的许多不同的随机子集结合使用。在测试时，我们使用了所有的神经元，并将它们的输出乘以0.5。这是一个与采用指数级成倍增长的dropout的网络产生的预测结果分布的几何平均值的合理近似。</p><p>我们在图2中的前两个全连接层上使用了dropout。没有dropout，我们的网络会出现严重的过拟合。Dropout大概会使达到收敛的迭代次数翻倍。</p><h1 id="5-训练细节"><a href="#5-训练细节" class="headerlink" title="5. 训练细节"></a>5. 训练细节</h1><p>我们使用随机梯度下降法来训练我们的模型，每个batch有128个样本，动量（momentum）为0.9，权重衰减（weight decay）为0.0005。我们发现这种较小的权重衰减对于模型的训练很重要。换句话说，权重衰减在这里不仅仅是一个正则化方法：它减少了模型的训练误差。权重ω的更新法则是：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gia1nspxrvj30ps05qwew.jpg" srcset="/img/loading.gif" alt="image-20200831155137366" style="zoom:50%;" /></center><p>其中，ii表示当前的迭代次数，vv表示动量（momentum），ε表示学习率， $⟨\frac{∂L}{∂ω}|_{ωi}⟩_{Di}$是第ii批次的目标函数关于w的导数（wi的偏导数）$Di$的平均值。</p><p>我们使用标准差为0.01、均值为0的高斯分布来初始化各层的权重。我们使用常数1来初始化了网络中的第二个、第四个和第五个卷积层以及全连接层中的隐含层中的所有偏置参数。这种初始化权重的方法通过向ReLU提供了正的输入，来加速前期的训练。我们使用常数0来初始化剩余层中的偏置参数。</p><p>我们对所有层都使用相同的学习率，在训练过程中又手动进行了调整。我们遵循的启发式方法是：以当前的学习速率训练，验证集上的错误率停止降低时，将学习速率除以10。学习率初始时设为0.01，并且在终止前减少3次。我们使用120万张图像的训练集对网络进行了大约90次迭代的训练，这在两块NVIDIA GTX 580 3GB GPU上花费了大约5到6天的时间。</p><h1 id="6-结果"><a href="#6-结果" class="headerlink" title="6. 结果"></a>6. 结果</h1><p>我们在ILSVRC-2010上取得的结果如表1所示。我们的网络的top-1和top-5测试集错误率分别为37.5％和17.0％。在ILSVRC-2010比赛期间取得的最佳成绩是47.1％和28.2％，其方法是对六种不同的稀疏编码模型所产生的预测结果求平均[2]。此后公布的最佳结果为45.7％、25.7％，其方法是对两种经过密集采样的特征[24]计算出来的Fisher向量（FV）训练的两个分类器取平均值。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gia2d2yq7pj30j006at9j.jpg" srcset="/img/loading.gif" alt="image-20200831161557022" style="zoom:50%;" /></center><blockquote><p>表1：在ILSVRC-2010测试集上的结果对比。斜体是其他人实现的最好结果。</p></blockquote><p>我们还在ILSVRC-2012竞赛中使用了我们的模型，并在表2中给出了我们的结果。由于ILSVRC-2012测试集标签未公开，因此我们无法给出我们测试过的所有模型在测试集上的错误率。在本节的其余部分中，我们将验证集和测试集的错误率互换，因为根据我们的经验，它们之间的差值不超过0.1％（见表2）。本文描述的CNN的top-5错误率达到了18.2％。对五个相似CNN的预测结果计算均值，得到的错误率为16.4％。单独一个CNN，在最后一个池化层之后，额外添加第六个卷积层，对整个ImageNet Fall 2011 release(15M images, 22K categories)进行分类，然后在ILSVRC-2012上“微调”（fine-tuning）网络，得到的错误率为16.6％。对整个ImageNet Fall 2011版本的数据集下预训练的两个CNN，求他们输出的预测值与前面提到的5个不同的CNN输出的预测值的均值，得到的错误率为15.3％。比赛的第二名达到了26.2％的top-5错误率，他们的方法是：对几个在特征取样密度不同的Fisher向量上训练的分类器的预测结果取平均的方法[7]。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gia2purvq1j30t408smyk.jpg" srcset="/img/loading.gif" alt="image-20200831162813886" style="zoom:50%;" /></center><blockquote><p>表2:ILSVRC-2012验证和测试集的错误率比较。斜体是他人获得的最佳结果。对带有星号*的模型进行了“预训练”，以对整个ImageNet 2011 Fall release进行分类。有关详细信息，请参见第6节。</p></blockquote><p>最后，我们还在ImageNet Fall 2009版本的数据集上报告错误率，总共有10,184个类别和890万张图像。在这个数据集中，我们遵循文献中的使用一半图像用于训练，一半图像用于测试的惯例。由于没有建立测试集，所以我们的拆分方法有必要与先前作者使用的拆分方法不同，但这并不会对结果产生显著的影响。我们在这个数据集上的top-1和top-5错误率分别是67.4％和40.9％，是通过前面描述的网络获得的，但是在最后的池化层上还有额外的第6个卷积层。该数据集此前公布的最佳结果是78.1％和60.9％[19]。</p><h2 id="6-1-定性评估"><a href="#6-1-定性评估" class="headerlink" title="6.1 定性评估"></a>6.1 定性评估</h2><p>图3显示了由网络的两个数据连接层学习得到的卷积核。该网络已经学习到许多频率和方向提取的内核，以及各种色块。请注意两个GPU所展现的不同特性，这也是3.5节中介绍的限制互连的结果。GPU1上的内核在很大程度上与颜色无关，然而GPU2上的内核在很大程度上都于颜色有关。这种特异性在每次迭代期间都会发生，并且独立于任何特定的随机权重初始化过程（以GPU的重新编号为模）。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gia20l0no1j30ii07y0yu.jpg" srcset="/img/loading.gif" alt="image-20200831160356963" style="zoom:50%;" /></center><blockquote><p>图3：第一个卷积层从224x224x3的输入图像学习到的11x11x3大小的卷积核。顶部48个卷积核在GPU1上学习，底部48个卷积核在GPU2上学习。第6.1节有详细介绍。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gia30k7o4zj318e0hmnpd.jpg" srcset="/img/loading.gif" alt="image-20200831163831435"></p><blockquote><p>图4：左边八个是ILSVRC-2010测试图像和我们的模型认为最可能的五个标签。正确的标签写在每个图像下，并且分配给正确标签的概率也用红色条显示（如果它恰好位于前5位）。右侧第一栏中的五个ILSVRC-2010测试图像。其余的列显示了六个训练图像，这些图像在最后一个隐藏层中生成特征向量，距离测试图像的特征矢量的欧几里得距离最小。</p></blockquote><p>在图4的左边，我们通过计算8张测试图像的top-5预测来定性评估网络的训练结果。请注意，即使是偏离中心的物体，如左上角的螨虫，也可以被网络识别出来。大多数top-5的标签都显得比较合理。例如，只有其他类型的猫才被认为是豹子的可能标签。在某些情况下（栅栏、樱桃），照片的关注点存在模糊性，不知道到底该关注哪个。</p><p>另一个研究可视化的网络的方法是，考虑由最后一个4096维隐含层中的图像的特征的激活函数输出值。如果两幅图像产生有的欧氏距离，我们可以认为高层次的神经网络认为它们是相似的。图4显示了测试集中的5个图像和训练集的6个图像，这些图像根据这种度量方法来比较它们中的哪一个与其最相似。请注意，在像素层次上，待检测的训练图像通常不会与第一列中的查询图像有较小的L2距离。例如，检索到的狗和大象有各种不同的姿势。我们在补充材料中提供了更多测试图像的结果。</p><p>通过使用欧式距离来计算两个4096维实值向量的相似性，效率不高，但是通过训练自编码器可以将这些向量压缩为较短的二进制码，能够使其更高效。与应用自编码器到原始像素[14]相比，这应该是更好的图像检索方法。它不使用图像标签，因此更倾向于检索具有相似图案边缘的图像，不管它们的图像语义是否相似。</p><h1 id="7-讨论"><a href="#7-讨论" class="headerlink" title="7. 讨论"></a>7. 讨论</h1><p>我们的研究结果表明，一个大的深层卷积神经网络能够在纯粹使用监督学习的情况下，在极具挑战性的数据集上实现破纪录的结果。值得注意的是，如果移除任何一个卷积层，网络的性能就会下降。例如，删除任何中间层的结果会导致网络性能的top-1错误率下降2%。因此网络的深度对于实现我们的结果真的很重要。</p><p>为了简化我们的实验，我们没有使用任何无监督的预训练方法，尽管这样可能会有所帮助，特别是如果我们获得了足够的计算能力来显著地增加网络的大小而不会相应地增加已标记数据的数量。到目前为止，我们的结果已经获得了足够的提升，因为我们已经使网络更大，并且训练了更长时间。但我们仍然有很大的空间去优化网络，使之能够像人类的视觉系统一样感知。最后，我们希望对视频序列使用非常大非常深的卷积神经网路，其中时间结构提供了非常有用的信息，这些信息往往在静态图像中丢失了，或者说不太明显。</p><h1 id="附：原文"><a href="#附：原文" class="headerlink" title="附：原文"></a>附：原文</h1><div class="row">    <embed src="/pdf/AlexNet.pdf" width="100%" height="550" type="application/pdf"></div> ]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>AlexNet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——EfficientDet：可扩展且高效的目标检测</title>
    <link href="/2020/08/28/EfficientDet/"/>
    <url>/2020/08/28/EfficientDet/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在计算机视觉中，模型效率变得越来越重要。在本文中，我们系统地研究了用于目标检测的神经网络体系结构设计选择，并提出了一些关键的优化措施来提高效率。首先，我们提出了一种加权双向特征金字塔网络（BiFPN），该网络可以轻松快速地进行多尺度特征融合。其次，我们提出了一种复合缩放方法，该方法可以同时对所有主干，特征网络和框/类预测网络的分辨率，深度和宽度进行统一缩放。基于这些优化和EfficientNet主干，我们开发了一种称为EfficientDet的新目标检测器系列，在各种资源限制条件下，其效率始终比现有技术好得多。特别是，通过单一模型和单一尺度，我们的EfficientDet-D7可在COCOtest-dev上实现最先进的52.2 AP，使用52M参数和325B FLOPs，与之前的检测器相比，其FLOP减少了13倍– 42倍，参数缩小了4-9倍。代码在：<a href="https://github.com/google/automl/tree/master/efficientdet" target="_blank" rel="noopener">https://github.com/google/automl/tree/master/efficientdet</a></p><blockquote><p>FLOPS：注意全大写，是floating point operations per second的缩写，意指每秒浮点运算次数，理解为计算速度。是一个衡量硬件性能的指标。</p><p>FLOPs：注意s小写，是floating point operations的缩写（s表复数），意指浮点运算数，理解为计算量。可以用来衡量算法/模型的复杂度。</p></blockquote><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>近年来，在更精确的目标检测方面取得了巨大的进步。同时，最新的物体检测器也变得越来越昂贵。例如，最新的基于AmoebaNet的NAS-FPN检测器[42]需要167M参数和3045B的FLOP（比RetinaNet [21]多30倍）才能达到最先进的精度。巨大的模型尺寸和昂贵的计算成本阻碍了它们在许多实际应用中的部署，例如机器人技术和无人驾驶汽车，在这些应用中，模型尺寸和延迟受到很大限制。考虑到这些现实世界中的资源限制，模型效率对于目标检测变得越来越重要。</p><p>以前有许多工作旨在开发更有效的检测器架构，例如one-stage[24,30,31,21]和anchor-free检测器[18,41,37]或者压缩现有模型[25,26]。尽管这些方法往往会达到更高的效率，但它们通常会牺牲准确性。此外，大多数以前的工作仅关注特定的或少量的资源需求，但是从移动设备到数据中心的各种实际应用程序通常需要不同的资源约束。</p><p>一个自然的问题是：是否有可能在广泛的资源约束下构建具有更高准确性和更好效率的可扩展检测架构？（例如，从3B到300B FLOPs）。本文旨在通过系统地研究探测器架构的各种设计选择来解决这个问题。基于one-stage范例，我们检查了骨干，特征融合和类/盒网络的设计选择，并确定了两个主要挑战：</p><p>挑战1：高效的多尺度特征融合——自[20]引入以来，FPN已被广泛用于多尺度的特征融合。最近，PANet [23]，NAS-FPN [8]和其他研究[17,15,39]为跨尺度特征融合开发了更多的网络结构。在融合不同的输入特征时，大多数以前的工作只是简单地将它们加起来而没有区别对待。但是，由于这些不同的输入特征具有不同的分辨率，因此我们观察到它们通常会不均等地影响融合输出特征。为了解决这个问题，我们提出了一个简单而高效的加权双向特征金字塔网络（BiFPN），该网络引入了可学习的权重以了解不同输入特征的重要性，同时反复应用自上而下和自下而上的多尺度特征融合。</p><p>挑战2：模型缩放——虽然先前的工作主要依靠更大的骨干网[21,32,31,8]或更大的输入图像尺寸[11,42]来获得更高的准确性，但我们观察到扩大特征网络和框/类预测网络也很关键同时考虑准确性和效率。受近期工作[36]的启发，我们提出了一种用于目标检测器的复合缩放方法，该方法可联合缩放所有骨干，特征网络，盒子/类预测网络的分辨率/深度/宽度。</p><p>最后，我们还观察到，最近发布的EfficientNets [36]比以前的常用骨干网具有更高的效率。通过将EfficientNet主干网与我们提出的BiFPN和复合缩放相结合，我们开发了一个名为EfficientDet的新目标检测器系列，与以前的对象检测器相比，它始终可以以更少的参数和FLOP获得更高的准确性。图1和图4显示了COCO数据集上的性能比较[22]。在类似的精度约束下，我们的EfficientDet使用的FLOP比YOLOv3 [31]少28倍，比RetinaNet [21]少30倍，比最近的基于ResNet的NAS-FPN [8]少19倍。尤其是，通过单模型和单个测试时标，我们的EfficientDet-D7可以达到具有52M参数和325B FLOP的最新52.2 AP，在1.5 AP的情况下胜过之前的最佳检测器[42]，而其尺寸却缩小了4倍，并且使用FLOP减少13倍。我们的EfficientDet在GPU / CPU上的速度也比以前的检测器快3到8倍。</p><p>通过简单的修改，我们还证明了我们的单模型单尺度EfficientDet在Pascal VOC 2012语义分割上使用18B FLOP达到81.74％mIOU精度，比DeepLabV3 + [4]高出1.7％，FLOP减少了9.8倍。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6e1i7xouj30qq0m042o.jpg" srcset="/img/loading.gif" alt="image-20200828115745842" style="zoom:50%;" /></center><blockquote><p>图1：模型FLOP与COCO准确性–所有数字均为单模型单标度。我们的EfficientDet实现了最新的52.2％COCO AP，其参数和FLOP比以前的检测器少得多。表4和表5中提供了有关不同主干以及FPN / NAS-FPN / BiFPN的更多研究。完整的结果在表2中。</p></blockquote><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h1><p><strong>One-Stage 检测器：</strong>现有的目标检测器主要根据它们是否具有感兴趣区域（ROI）建议步骤（两阶段[9,32,3,11]）（单阶段[33,24,30,21]）进行分类。尽管two-stage检测器趋向于更灵活，更准确。通常认为one-stage检测器更简单，更高效，通过利用预定义的Anchors[14]。近来，one-stage检测器因其高效和简单而受到了广泛的关注[18,39,41]。在本文中，我们主要遵循one-stage的检测器设计，并且我们证明通过优化的网络架构可以同时实现更高的效率和更高的精度。</p><p><strong>多尺度特征表示：</strong>目标检测的主要困难之一是有效地表示和处理多尺度特征。早期的检测器通常基于从骨干网络中分层提取的金字塔特征来直接执行预测[2,24,33]。作为一项开创性工作，特征金字塔网络（FPN）[20]提出了一种自上而下的途径来组合多种尺度的特征。按照这个想法，PANet [23]在FPN的基础上增加了一个自下而上的路径聚合网络。STDL [40]提出了一个尺度转移模块来利用跨尺度特征。M2det [39]提出了一种U形模块来融合多尺度特征，而G-FRNet [1]提出了用于控制信息跨特征流动的门单元。最近，NAS-FPN [8]利用神经体系结构搜索来自动设计特征网络拓扑。尽管NAS-FPN可以实现更好的性能，但在搜索过程中却需要数千个GPU小时，并且由此产生的功能网络是不规则的，因此难以解释。本文旨在以一种更直观，更原则的方式优化多尺度特征融合。</p><p><strong>模型缩放：</strong>为了获得更好的精度，通常通过采用更大的骨干网络来扩大基准检测器。（例如，从移动尺寸模型[35,13]和ResNet [12]到ResNeXt [38]和AmoebaNet [29]），或增加输入图像的尺寸（例如，从512x512 [21]到1536x1536 [42]）。最近的一些工作[8,42]表明，增加通道大小和重复特征网络也可以提高精度。这些缩放方法主要集中在单个或有限的缩放维度上。最近，[36]通过共同扩大网络宽度，深度和分辨率，证明了图像分类的卓越模型效率。我们提出的用于对象检测的复合缩放方法主要受[36]启发。</p><h1 id="3-BiFPN"><a href="#3-BiFPN" class="headerlink" title="3.BiFPN"></a>3.BiFPN</h1><p>在本节中，我们首先阐述多尺度特征融合问题，然后介绍我们提出的BiFPN的主要思想：高效的双向跨尺度连接和加权特征融合。</p><h2 id="3-1问题表述"><a href="#3-1问题表述" class="headerlink" title="3.1问题表述"></a>3.1问题表述</h2><p>多尺度特征融合旨在聚合不同分辨率的特征。例如，给出一个多尺度特征列表<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6kzia3qej30aa01s0sp.jpg" srcset="/img/loading.gif" alt="image-20200828155802845" style="zoom:33%;" />P<sub>li</sub><sup>in</sup>代表在li级别上的特征，我们的目标是寻找一个转换f，可以高效的聚合不同的特征，并且输出一个新特征的列表：<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6l2bfe5sj309m020q2x.jpg" srcset="/img/loading.gif" alt="image-20200828160047115" style="zoom:33%;" />。作为一个具体的例子，图2（a）显示了传统的自上而下的FPN [20]。它具有3-7级输入特征<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6mjz7azwj309i01ojrc.jpg" srcset="/img/loading.gif" alt="image-20200828165220646" style="zoom:33%;" />,其中P<sub>i</sub><sup>in</sup>表示使用输入图片分辨率的（1/2)<sup>i</sup>的特征等级。例如，如果输入分辨率为640x640，则P<sub>3</sub><sup>in</sup>代表分辨率为80x80的特征级别3（640/2<sup>3</sup> = 80），而P<sub>7</sub><sup>in</sup>代表分辨率为5x5的特征级别7。常规的FPN以自顶向下的方式聚合多尺度特征：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6msr32bsj30iy07cq3o.jpg" srcset="/img/loading.gif" alt="image-20200828170046983" style="zoom: 50%;" /></center><p>其中Resize通常是用于分辨率匹配的上采样或下采样操作，而Conv通常是用于特征处理的卷积操作。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6mtwsy2kj31i00iyjzd.jpg" srcset="/img/loading.gif" alt="image-20200828170153509"></p><blockquote><p>图2：特征网络设计——（a）FPN [20]引入了自上而下的途径，以融合从3级到7级（P3-P7）的多尺度特征；（b）PANet [23]在FPN的顶部增加了一条自下而上的途径；（c）NAS-FPN [8]使用神经架构搜索来查找不规则特征网络拓扑，然后重复应用相同的块；（d）是我们的BiFPN，具有更好的准确性和效率的权衡。</p></blockquote><h2 id="3-2跨尺度连接"><a href="#3-2跨尺度连接" class="headerlink" title="3.2跨尺度连接"></a>3.2跨尺度连接</h2><p>常规的自上而下的FPN固有地受到单向信息流的限制。为了解决这个问题，PANet [23]添加了一个额外的自下而上的路径聚合网络，如图2（b）所示。在[17,15,39]中进一步研究了跨尺度连接。最近，NAS-FPN [8]采用神经架构搜索来搜索更好的跨尺度特征网络拓扑，但是在搜索过程中需要数千个GPU小时，并且发现的网络不规则且难以解释或修改，如图2（c）所示。</p><p>通过研究这三个网络的性能和效率（表5），我们发现PANet的精度比FPN和NAS-FPN更好，但需要更多的参数和计算成本。为了提高模型效率，本文针对跨尺度连接提出了几种优化方法：首先，我们删除只有一个输入边的节点。我们的直觉很简单：如果一个节点只有一个输入边且没有特征融合，那么对于致力于融合不同特征的特征网络的贡献将较小。这导致简化的双向网络。其次，如果原始输入与输出节点处于同一级别，则在原始输入和输出节点之间添加一条额外的边，以便在不增加成本的情况下融合更多功能。第三，与PANet [23]仅具有一个自上而下和一个自下而上的路径不同，我们将每个双向（自上而下和自下而上）路径视为一个特征网络层，并重复这一相同层多次以实现更高的水平的特征融合。第4.2节将讨论如何使用复合缩放方法确定不同资源约束的层数。通过这些优化，我们将新特征网络命名为双向特征金字塔型网络（BiFPN），如图2和3所示。</p><h2 id="3-3加权特征融合"><a href="#3-3加权特征融合" class="headerlink" title="3.3加权特征融合"></a>3.3加权特征融合</h2><p>当融合具有不同分辨率的特征时，一种常见的方法是先将它们的大小调整为相同的分辨率，然后对其求和。Pyramid attention network(金字塔注意力网络)[19]引入了全局自注意力上采样以恢复像素局部化，这在[8]中有进一步的研究。所有先前的方法均等地对待所有输入特征，没有区别。但是，我们观察到，由于不同的输入特征的分辨率不同，因此它们通常对输出特征的贡献不同。为了解决这个问题，我们建议为每个输入增加一个额外的权重，并让网络学习每个输入特征的重要性。基于此思想，我们考虑三种加权融合方法：</p><p><strong>无限融合：</strong><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6osp9mwbj308k01qq2v.jpg" srcset="/img/loading.gif" alt="image-20200828180956474" style="zoom:33%;" />其中，wi是一个可学习的权重，可以是标量（每个特征），也可以是向量（每个通道），或者是一个多维的张量（每个像素）。我们发现一个尺度可以达到与其他方法相当的精度与最小的计算成本。但是，由于标量权重是无限的，可能会潜在地造成训练的不稳定性。因此，我们通过权重归一化来限定每个权重的取值范围。</p><p><strong>基于Softmax的融合：</strong><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6ozp2pf5j309o03474b.jpg" srcset="/img/loading.gif" alt="image-20200828181639687" style="zoom:33%;" />直观的想法是将softmax应用于每个权重，以便将所有权重标准化为一个概率范围，值的范围是0到1，代表每个输入的重要性。但是，如我们在第6.3节中的消融研究所示，额外的soft-max会导致GPU硬件显著减慢。为了最小化额外的等待时间成本，我们进一步提出了一种快速融合方法。</p><p><strong>快速归一化融合：</strong><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6p68njl6j30bm02wq2z.jpg" srcset="/img/loading.gif" alt="image-20200828182256481" style="zoom:50%;" />其中wi≥0是通过在每个wi之后应用Relu来确保的，e= 0.0001是一个小数值，以避免数值不稳定。同样，每个归一化权重的值也介于0和1之间，但由于此处没有softmax运算，因此效率更高。我们的消融研究表明，这种快速融合方法具有与基于softmax的融合非常相似的学习行为和准确性，但在GPU上的运行速度最高可提高30％（表6）。</p><p>我们最终的BiFPN集成了双向跨尺度连接和快速归一化融合。作为一个具体的示例，在这里我们描述BiFPN在6级的两个融合特征，如图2（d）所示：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6peeo4tbj30rm07o75d.jpg" srcset="/img/loading.gif" alt="image-20200828183047425" style="zoom:50%;" /></center><p>其中P<sub>6</sub><sup>td</sup>是自顶向下路径上第6级的中间特征，P<sub>6</sub><sup>out</sup>是自底向上路径上第6级的输出特征。所有其他特征均以类似方式构造。值得注意的是，为了进一步提高效率，我们使用深度可分离卷积[5,34]进行特征融合，并在每次卷积后添加批处理归一化和激活。</p><h1 id="4-EfficientDet"><a href="#4-EfficientDet" class="headerlink" title="4.EfficientDet"></a>4.EfficientDet</h1><p>基于我们的BiFPN，我们开发了一个名为EfficientDet的新检测模型系列。在本节中，我们将讨论网络架构和EfficientDet的新的复合缩放方法。</p><h2 id="4-1EfficientDet-结构"><a href="#4-1EfficientDet-结构" class="headerlink" title="4.1EfficientDet 结构"></a>4.1EfficientDet 结构</h2><p>图3显示了EfficientDet的总体架构，该架构很大程度上遵循了one-stage检测器范式[24,30,20,21]。我们采用ImageNet预训练的EfficientNets作为骨干网络。我们提出的BiFPN用作特征网络，它从骨干网络中获取3-7级的特征{P3，P4，P5，P6，P7}，并反复应用自上而下和自下而上的双向特征融合。这些融合的特征被馈送到一个类和Box网络，以分别生成目标类别和边界框预测。类似于[21]，类和框网络权重在所有级别的特征之间共享。</p><h2 id="4-2复合缩放"><a href="#4-2复合缩放" class="headerlink" title="4.2复合缩放"></a>4.2复合缩放</h2><p>为了优化准确性和效率，我们想开发一系列可以满足各种资源限制的模型。这里的主要挑战是如何扩展基准EfficientNet模型。</p><p>以前的工作主要是通过使用更大的主干网（例如ResNeXt[38]或AmobaNet[29]），使用更大的输入图像，或堆叠更多的FPN层[8]，来扩大基线检测器的规模。这些方法通常是无效的，因为它们只关注单个或有限的缩放维度。最近的工作[36]通过联合扩大网络宽度，深度和输入分辨率的所有维度，在图像分类上显示了卓越的性能。在[8,36]的启发下，我们提出了一种新的目标检测复合缩放方法，它使用一个简单的复合系数φ来联合放大主干网、BiFPN网络、类/盒网络和分辨率的所有维度。与[36]不同的是，目标检测器比图像分类模型具有更大的缩放维度，因此所有维度的网格搜索成本高得令人望而却步。因此，我们使用了基于启发式的缩放方法，但仍然遵循联合放大所有维度的主要思想。</p><p><strong>骨干网络</strong>——我们重用EfficientNet-B0到B6的相同宽度/深度缩放系数[36]，这样我们就可以很容易地重用他们的ImageNet预训练检查点。</p><p><strong>BiFPN网络</strong>——我们线性增加BiFPN深度D<sub>bifpn</sub>(层），因为深度需要四舍五入为小整数。对于BiFPN 的宽度W<sub>bifpn</sub>（通道）类似于[36]，呈指数增长。具体地说，我们对一个数值列表{1.2、1.25、1.3、1.35、1.4、1.45}执行网格搜索，并选择最佳值1.35作为BiFPN的宽度缩放因子。</p><p>形式上，BiFPN宽度和深度按以下公式缩放：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6tnubba4j30nu022mxc.jpg" srcset="/img/loading.gif" alt="image-20200828205815859" style="zoom: 50%;" /></center><p><strong>框/类预测网络</strong>——我们将其宽度固定为与BiFPN相同（即W<sub>pred</sub> = W<sub>bifpn</sub>），但使用以下等式线性增加深度（层数）：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6tqs3og0j30la02omx8.jpg" srcset="/img/loading.gif" alt="image-20200828210105033" style="zoom:50%;" /></center><p><strong>输入图像分辨率</strong>——由于在BiFPN中使用了特征级别3-7，输入分辨率能被2<sup>7</sup>=128整除，因此我们使用以下等式线性增加分辨率：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6tuxjkdrj30ji02qaa4.jpg" srcset="/img/loading.gif" alt="image-20200828210504201" style="zoom:50%;" /></center><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6o06xv55j31is0k4dlz.jpg" srcset="/img/loading.gif" alt="image-20200828174231906"></p><blockquote><p>图3：<strong>EfficientDet 结构</strong>——它采用EfficientNet [36]作为骨干网络，使用BiFPN作为特征网络，以及共享的类/盒预测网络。如表1所示，根据不同的资源约束，BiFPN层和类/框网络层都重复了多次。</p></blockquote><p>根据具有不同φ的方程1,2,3，如表1所示，将EfficientDet-D0（φ= 0）发展为D7（φ= 7），其中D7与D6相同，只是具有更高的分辨率。值得注意的是，我们的缩放是基于启发式的，可能不是最佳的，但是我们将证明，这种简单的缩放方法比图6中的其他一维缩放方法可以显著提高效率。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6o256507j30qi0dgjth.jpg" srcset="/img/loading.gif" alt="image-20200828174424853" style="zoom:50%;" /></center><blockquote><p>表1：EfficientDet D0-D6的缩放配置——ø是控制所有其他缩放比例尺寸的复合系数；BiFPN，框/类网络和输入大小分别使用公式1,2,3进行放大。</p></blockquote><h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5.实验"></a>5.实验</h1><h2 id="5-1-EfficientDet用于目标检测"><a href="#5-1-EfficientDet用于目标检测" class="headerlink" title="5.1 EfficientDet用于目标检测"></a>5.1 EfficientDet用于目标检测</h2><p>我们使用118K训练图像在COCO 2017检测数据集[22]上评估EfficientNet。每个模型均使用SGD优化器进行训练，其动量为0.9，权重衰减为4e-5。在第一个训练时期，学习率从0线性增加到0.16，然后使用余弦衰减规则进行退火。每次卷积后添加了同步批处理归一化，批处理范数衰减为0.99，ε为1e-3。与[36]相同，我们使用swish激活[28,6]和衰减为0.9998的指数移动平均值。我们还采用了α= 0.25和γ= 1.5且纵横比{1/2，1，2}的常用焦距[21]。每个模型在32个TPUv3内核上以批量大小128进行训练，每个内核批量大小为4。我们使用RetinaNet [21]进行预处理，并采用训练时间进行多分辨率裁剪/缩放和翻转增强。值得注意的是，我们没有对任何模型使用自动增强[42]。</p><p>表格2将EfficientNet与其他对象检测器进行比较，在没有测试时间增加的单模型单标度设置下。我们报告了test-dev（无真值的20K测试图像）和val（有真值的5K验证图像）的准确性。我们的EfficientDet实现了比以前的探测器更高的效率，在各种精度或资源限制条件下，其缩小了4倍-9倍，并使用了13倍-42倍更少的FLOP。在相对较低的精度范围内，我们的EfficientDet-D0达到了与YOLOv3相似的精度，使用了28x 更少的FLOPs。与RetinaNet [21]和Mask-RCNN [11]相比，我们的EfficientDet-D1达到了相似的精度，参数减少了8倍，FLOP减少了21倍。在高精度体制下，我们的EfficientDet也始终比最近的NAS-FPN好（及其[42]中的增强版本，具有更少的参数和FLOP）。特别是，我们的EfficientDet-D7实现了针对单模型单尺度的最先进的52.2AP（在测试集上）和51.8 AP（在验证集上）。值得注意的是，与需要特殊设置的大型AmoebaNet + NAS-FPN + AutoAugment模型[42]不同，（例如，将锚点从3x3更改为9x9，以模型并行性进行训练，并依靠昂贵的自动增强功能）所有的EfficientDet模型使用相同的3x3 Anchors，并且训练时没有模型并行性。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6ussgbp8j31dl0u0k2o.jpg" srcset="/img/loading.gif" alt="image-20200828213736622"></p><blockquote><p>表2 ：<strong>EfficientDet在COCO上的性能</strong>——结果适用于单模型单尺度。test-dev是COCO测试集，val是验证集。Params和Flops表示参数和相乘的数目。Latency指示批大小为1的推断延迟。AA表示自动增强[42]。如果模型具有相似的准确性，我们将它们分组在一起，并比较每组模型的大小、FLOPs和延迟。</p></blockquote><p>除了参数大小和FLOP，我们还比较了Titan-V GPU和单线程Xeon CPU的实际延迟。我们以批量大小1运行每个模型10次，并报告均值和标准差。图4说明了模型大小，GPU延迟和单线程CPU延迟的比较。为了公平比较，这些数字仅包括在同一台机器上以相同设置测量的结果。与以前的检测器相比，EfficientDet模型在GPU上的运行速度提高了3.2倍，在CPU上的运行速度提高了8.1倍，这表明它们在现实世界的硬件上也非常有效。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6uynhaijj31ko0i2qag.jpg" srcset="/img/loading.gif" alt="image-20200828214315300"></p><p>图4：<strong>模型大小和推理延迟比较</strong>——延迟是在配备Titan V GPU和Xeon CPU的同一台计算机上以批处理大小1进行测量的。AN表示AmoebaNet+NAS-FPN使用自动增强训练[42]。我们的EfficientDet模型比其他检测器小4倍-6.6倍，在GPU上快2.3倍-3.2倍，在CPU上快5.2倍-8.1倍。</p><h2 id="5-2-EfficientDet用于语义分割"><a href="#5-2-EfficientDet用于语义分割" class="headerlink" title="5.2 EfficientDet用于语义分割"></a>5.2 EfficientDet用于语义分割</h2><p>尽管我们的EfficientDet模型主要用于目标检测，但我们也对它们在其他任务（例如语义分割）上的性能感兴趣。继[16]之后，我们修改了EfficientDet模型，以将特征级别{P2，P3，…，P7}保留在BiFPN中，但仅将P2用于最终的每像素分类。为简单起见，这里我们仅评估基于EfficientDet-D4的模型，该模型使用ImageNet预训练的EfficientNet-B4骨干网（大小与ResNet-50相似）。对于BiFPN，我们将通道大小设置为128；对于分类头，我们将通道大小设置为256。BiFPN和分类头均重复3次。</p><p>表3显示了我们的模型和Pascal VOC 2012[7]上的以前的DeepLabV3 + [4] 之间的比较。值得注意的是，我们通过集合，测试时间增加或COCO预训练排除了这些结果。在相同的单模型单标度设置下，我们的模型比DeepLabV3 +的现有技术提高了1.7％的精度，而FLOP减少了9.8倍[4]。这些结果表明，Efficient-Det在语义分割方面也很有前途。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6v51ex2cj30pi078myd.jpg" srcset="/img/loading.gif" alt="image-20200828214923293" style="zoom:50%;" /></center><blockquote><p>表3：<strong>在Pascal VOC上的语义分割性能对比</strong></p></blockquote><h1 id="6-消融研究"><a href="#6-消融研究" class="headerlink" title="6.消融研究"></a>6.消融研究</h1><p>在本节中，我们为提议的EfficientDet消除各种设计选择。为简单起见，此处的所有准确性结果均用于COCO验证集。</p><h2 id="6-1分离主干和BiFPN"><a href="#6-1分离主干和BiFPN" class="headerlink" title="6.1分离主干和BiFPN"></a>6.1分离主干和BiFPN</h2><p>由于EfficientDet同时使用了功能强大的主干网和新的BiFPN，因此我们想了解它们各自对准确性和效率提高的贡献。表4比较了骨干网和BiFPN的影响。从使用ResNet-50 [12]骨干和自上而下的FPN [20]的RetinaNet检测器[21]开始，我们首先用EfficientNet-B3取代了骨干，这通过大约更少的参数和FLOPs将精度提高了约3 AP。通过用我们提出的BiFPN进一步替代FPN，可以用更少的参数和FLOP来实现额外的4 AP增益。这些结果表明，EfficientNet骨干和BiFPN对我们最终模型至关重要。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6vmeevq1j30qm07g758.jpg" srcset="/img/loading.gif" alt="image-20200828220604682" style="zoom:50%;" /></center><blockquote><p>表4：<strong>分离主干和BiFPN</strong> ——从标准RetinaNet（ResNet50 + FPN）开始，我们首先用EfficientNet-B3替换主干，然后将基线FPN替换为我们提出的BiFPN。</p></blockquote><h2 id="6-2-BiFPN跨尺度连接"><a href="#6-2-BiFPN跨尺度连接" class="headerlink" title="6.2 BiFPN跨尺度连接"></a>6.2 BiFPN跨尺度连接</h2><p>表5显示了图2中列出的具有不同跨尺度连接的功能网络的准确性和模型复杂性。值得注意的是，原始FPN [20]和PANet [23]仅具有一个自上而下或自下而上的流程，但为了公平比较起见，我们在此重复多次，并用深度可分离的conv替换所有conv，这与BiFPN相同。我们使用相同的主干网络和类/框预测网络，所有实验均使用相同的训练设置。可以看到，传统的自上而下的FPN固有地受到单向信息流的限制，因此准确性最低。尽管重复的FPN + PANet的精度比NAS-FPN略好[8]，但它还需要更多的参数和FLOP。我们的BiFPN的精度与重复FPN + PANet相似，但使用的参数和FLOP少得多。通过附加的加权特征融合，我们的BiFPN可以以更少的参数和FLOP进一步达到最佳精度。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6vhus4lyj30qc0cm40n.jpg" srcset="/img/loading.gif" alt="image-20200828220141936" style="zoom:50%;" /></center><blockquote><p>表5：<strong>不同特征网络的比较</strong>——我们加权的BiFPN以较少的参数和FLOP达到了最佳精度。</p></blockquote><h2 id="6-3-Softmax-vs-快速归一化融合"><a href="#6-3-Softmax-vs-快速归一化融合" class="headerlink" title="6.3 Softmax vs 快速归一化融合"></a>6.3 Softmax vs 快速归一化融合</h2><p>正如第3.3节所讨论的，我们提出了一种快速的标准化特征融合方法，在保留归一化权重优点的同时，摆脱昂贵的softmax。表6比较了三种不同型号的探测器中的softmax和快速归一化融合方法。结果表明，我们的快速归一化融合方法实现了与基于softmax的融合相似的准确性，但在GPU上的运行速度提高了1.26倍至1.31倍。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6vwqq1vlj30qo094wft.jpg" srcset="/img/loading.gif" alt="image-20200828221601103" style="zoom:50%;" /></center><blockquote><p>表6：<strong>不同特征融合的比较</strong>——我们的快速归一化融合实现了与基于softmax的融合相似的准确性，但运行速度提高了28％-31％。</p></blockquote><p>为了进一步了解基于softmax的快速归一化融合的行为，图5展示了从EfficientDet-D3中的BiFPN层中随机选择的三个特征融合节点的学习权重。值得注意的是，所有输入的归一化权重（例如，基于softmax的融合的ewi / ∑jewj和用于快速归一化融合的wi /（e+ ∑jwj））总和为1。有趣的是，标准化权重在训练过程中变化很快，说明不同的特征对特征融合的贡献是不相等的。尽管变化很快，我们的快速归一化融合方法在所有三个节点上的学习行为都与基于softmax的融合方法非常相似。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6w0nlmgqj31ji0b2n1g.jpg" srcset="/img/loading.gif" alt="image-20200828221946190"></p><blockquote><p>图5：<strong>Softmax与快速归一化特征融合</strong>——（a）-（c）显示了训练中三个代表性节点的归一化权重（即重要性）；每个节点都有两个输入（input1和input2），它们的归一化权重总和为1。</p></blockquote><h2 id="6-4复合缩放"><a href="#6-4复合缩放" class="headerlink" title="6.4复合缩放"></a>6.4复合缩放</h2><p>正如第4.2节所讨论的，我们使用一种复合缩放方法来联合放大主干网、BiFPN和盒/类预测网络的深度/宽度/分辨率的所有维度。图6将我们的复合缩放与其他可选方法进行了比较，这些方法可以放大分辨率/深度/宽度的单一维度。虽然从同一个基线检测器开始，我们的复合缩放方法实现了比其他方法更好的效果。通过更好地平衡差异体系结构维度，表明了联合扩展的好处。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi6wgnwsimj30oa0i076q.jpg" srcset="/img/loading.gif" alt="image-20200828223509603" style="zoom:50%;" /></center><blockquote><p>图6：不同缩放方法的比较——复合缩放可实现更好的准确性和效率。</p></blockquote><h1 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h1><p>在本文中，我们系统地研究了用于有效目标检测的网络体系结构设计选择，并提出了加权双向特征网络和定制的复合缩放方法，以提高准确性和效率。基于这些优化，我们开发了一个名为EfficientDet的新检测器系列，该检测器在广泛的资源限制范围内始终比现有技术具有更高的准确性和效率。尤其是，与以前的目标检测和语义分割模型相比，我们扩展后的EfficientDet以更少的参数和FLOP达到了最高的准确性。</p><h1 id="附：原文"><a href="#附：原文" class="headerlink" title="附：原文"></a>附：原文</h1><div class="row">    <embed src="/pdf/EfficientDet.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>EfficientDet</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>win10安装指南</title>
    <link href="/2020/08/26/win10/"/>
    <url>/2020/08/26/win10/</url>
    
    <content type="html"><![CDATA[<p>最近总是帮别人装系统，每次都需要到处搜索下载所需资料。在此整理分享一下我安装win10的过程，供大家参考。</p><h1 id="1-制作win-pe启动U盘"><a href="#1-制作win-pe启动U盘" class="headerlink" title="1. 制作win-pe启动U盘"></a>1. 制作win-pe启动U盘</h1><p>我之前用过许多PE工具箱，如：老毛桃、大白菜、U深度等，这些pe系统在安装系统的过程中，会植入许多自带的软件（即使使用原版镜像），导致安装完系统需要逐一卸载，十分繁琐。最近发现了一款纯净的WinPE工具箱：<a href="http://www.wepe.com.cn" target="_blank" rel="noopener">微PE工具箱</a>。 经实际测试，该工具箱确实确实十分纯净，没有植入其他软件，没有强制设置主页等流氓行为。并且安装过程简单，无需像传统工具箱那样先安装好软件，再制作启动U盘。wePE只需在安装程序中直接选择安装到u盘，即可一键制作启动U盘。</p><p>下载链接：<a href="/downloads/WePE_32_V2.1.exe">32位</a> 、 <a href="/downloads/WePE_64_V2.1.exe">64位</a></p><h1 id="2-下载win10原版镜像"><a href="#2-下载win10原版镜像" class="headerlink" title="2. 下载win10原版镜像"></a>2. 下载win10原版镜像</h1><p>这里推荐一个一站式镜像下载网站：<a href="https://msdn.itellyou.cn" target="_blank" rel="noopener">MSDN我告诉你</a>。 点击进入主页后，选取左侧”操作系统“一栏，选择所需要的版本，在右侧选择32位或64位，点击’详细信息‘，复制ed2k链接到迅雷下载即可。（这一步下载速度还是比较快的，因为资源比较多，基本可以跑满你的带宽）</p><p>附：win10,v1909(Updated Jan 2020)的ed2k链接：</p><blockquote><pre><code class="hljs gherkin">ed2k://|<span class="hljs-string">file</span>|<span class="hljs-string">cn_windows_10_business_editions_version_1909_updated_jan_2020_x64_dvd_b3e1f3a6.iso</span>|<span class="hljs-string">5311711232</span>|<span class="hljs-string">3527D2A9845FF4105F485CC364655B66</span>|<span class="hljs-string">/</span></code></pre></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gi4mukqv2dj31ug0tenbl.jpg" srcset="/img/loading.gif" alt="image-20200826233122509"></p><h1 id="3-进入pe安装系统"><a href="#3-进入pe安装系统" class="headerlink" title="3. 进入pe安装系统"></a>3. 进入pe安装系统</h1><p>（备份原系统数据！备份原系统数据！备份原系统数据！）</p><p>插入制作好的启动U盘，在BIOS里选择使用U盘启动。</p><p>进入pe后，使用桌面上的安装系统工具，选择刚刚下载好的镜像以及要安装的盘符，点击下一步即可一键安装。</p><p>安装完成后，拔掉U盘，重启系统。</p><p>按照win10的安装向导执行连接网络、设置密码等步骤。</p><p>”海内存知己天涯若比邻“</p><p>”嗨，别来无恙啊！“</p><p>当你看到这两句话时，系统安装已经完成。</p><h1 id="4-激活系统"><a href="#4-激活系统" class="headerlink" title="4. 激活系统"></a>4. 激活系统</h1><p><span style='color:red'>本文提供的激活工具仅供学习交流，严禁用于商业用途,请于24小时内删除！</span></p><p>激活脚本下载地址：<a href="/downloads/Win10数字权利激活.zip">Win10数字权利激活脚本</a></p><p>右键使用管理员身份运行，选择一键激活当前版本即可。</p><p>（请大家支持正版！）</p><h1 id="5-安装部署Office"><a href="#5-安装部署Office" class="headerlink" title="5. 安装部署Office"></a>5. 安装部署Office</h1><p>众所周知，wps广告多，而且经常出现格式错误。所以还是推荐大家使用Microsoft的office系列软件。</p><p>这里用到了一款office部署、激活工具：<a href="https://otp.landian.vip/zh-cn/" target="_blank" rel="noopener">Office Tool Plus</a>，请自行到官网下载安装最新版。</p><p>安装后选择”部署“，选择所需安装的版本（此处需要选批量版，推荐Office 2019 Stantard Volume)，选择所需安装的软件（常用的选择Word、PowerPoint、Excel即可），之后在右侧选择企业长期激活、创建桌面快捷方式。最后点击”部署“即可一键安装。（这一步耗时主要与网速相关，一般需要3-5分钟）</p><p>返回主页，点击”激活“。在证书管理栏选择与刚刚安装的版本对应的版本，如：Office 2019 Stantard Volume，点击安装许可证，等待完成。在密钥管理栏输入密钥，点击安装密钥，等待完成。</p><blockquote><p>————————- Office 2019 GVLK 密钥（KMS 专用）————————-</p><p> Office Pro Plus 2019        NMMKJ-6RK4F-KMJVX-8D9MJ-6MWKP  </p><p>Office Standard 2019      6NWWJ-YQWMR-QKGCB-6TMB3-9D9HK  </p><p>Project Pro 2019              B4NPR-3FKK7-T2MBV-FRQ4W-PKD2B </p><p>Project Std 2019              C4F7P-NCP8C-6CQPT-MQHV9-JXD2M </p><p>Visio Pro 2019                 9BGNQ-K37YR-RQHF2-38RQ3-7VCBB  </p><p>Visio Std 2019                  7TQNQ-K3YQQ-3PFH7-CCPPM-X4VQ2 </p><p>Access 2019                     9N9PT-27V4Y-VJ2PD-YXFMF-YTFQT  </p><p>Office Pro Plus 2016         XQNVK-8JYDB-WJ9W3-YJ8YR-WFG99</p></blockquote><p>输入kms服务器地址，点击设定服务器地址。之后点击下拉箭头，测试kms服务器。</p><p>常用kms服务器地址：</p><blockquote><p>kms.03k.org </p><p>kms.chinancce.com </p><p>kms.lotro.cc </p><p>cy2617.jios.org </p><p>kms.shuax.com </p><p>kms.luody.info </p><p>kms.cangshui.net </p><p>zh.us.to </p><p>122.226.152.230</p><p> kms.digiboy.ir </p><p>kms.library.hk </p><p>kms.bluskai.com </p><p>xykz.f3322.org</p></blockquote><p>最后点击右下角激活按钮，等待激活完成即可。</p><p>若激活失败，一般是因为kms服务器失效，更换一个kms服务器地址即可。</p><p>（注意，激活过程必须按照上述顺序来，选择的证书和密钥版本必须与之前安装的相同，否则会出错。）</p>]]></content>
    
    
    <categories>
      
      <category>教程</category>
      
    </categories>
    
    
    <tags>
      
      <tag>win10</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（8.21)</title>
    <link href="/2020/08/21/8-21/"/>
    <url>/2020/08/21/8-21/</url>
    
    <content type="html"><![CDATA[<p>计算机视觉领域和目标相关的经典任务有三种：分类、检测和分割。其中分类的目标是获取待检测目标“是什么？”，而检测则需要在图片上用方框出目标，知道目标“在哪里？”，目标分割则需要在像素级别上回答“在哪里？”的问题。目标分割又分为语义分割（Semantic segmentation）和实例分割（Instance segmentation），语义分割需要在像素级别上将同类物体分割出来，对于同类物体的不同实例不需单独分割。而实例分割需要在语义分割的基础上按个体分割，找出同一类物体的不同个体。</p><p>此前我阅读学习了YOLO系列和R-CNN系列经典目标检测算法，本次阅读首先学习了将YOLO和Faster RCNN结合起来的SSD算法[1]，完善了对目标检测经典论文的阅读，我认为学习经典的算法有助于后续学习最新的算法。之后阅读了一篇目标分割方向的综述论文[2]，学习了常用的实例分割方法：MASK R-CNN[4],由于MASK R-CNN用到了FCN，所以先学习了FCN[3]。最后阅读了两篇目标跟踪方向的综述性论文[5,6]。对目标分割和目标跟踪方向有了初步的了解。</p><p>阅读文献：</p><p>[1] Liu W, Anguelov D, Erhan D, et al. Ssd: Single shot multibox detector[C]//European conference on computer vision. Springer, Cham, 2016: 21-37.</p><p>[2] Song Y, Yan H. Image Segmentation Techniques Overview[C]//2017 Asia Modelling Symposium (AMS). IEEE, 2017: 103-107.</p><p>[3] Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 3431-3440.</p><p>[4] He K, Gkioxari G, Dollár P, et al. Mask r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2017: 2961-2969.</p><p>[5]王鑫瑞.目标跟踪算法研究综述[J].信息通信,2020(04):42-43+46.</p><p>[6]王海涛,王荣耀,王文皞,王海龙,刘强.目标跟踪综述[J].计算机测量与控制,2020,28(04):1-6+21.</p><h1 id="1-SSD"><a href="#1-SSD" class="headerlink" title="1. SSD"></a>1. SSD</h1><p>R-CNN系列算法是基于“Proposal+Classification”的Two Stage目标检测算法，这一类算法先预先回归一次边框，然后进行骨干网络训练，精度较高但速度较慢。由此诞生了YOLO系列One Stage算法。YOLO只做了一次边框回归和打分，速度较快，能达到实时效果。但由于只做了一次边框回归和打分，导致对小目标训练不充分，对目标尺度敏感，对尺度变化较大的物体泛化能力较差。</p><p>针对YOLO和Faster R-CNN各自的优缺点，WeiLiu等人提出了SSD算法（Single Shot MultiBox Detector），采用了One Stage的理念，提高检测速度。同时融入Faster R-CNN中的Anchors思想，进行特征分层提取并依次计算边框回归和分类操作，可适应多种尺度的目标训练和检测任务，同时精度高，实时性好。</p><p>SSD网络的设计思想是：分层特征提取，依次进行边框回归和分类。因为不同层次的特征图代表不同层次的语义信息：低层次的特征图代表低层语义信息（含有更多细节），适合小尺度目标学习，能提高语义分割质量；高层次的特征图代表高层语义信息，适合对大尺度目标进行深入学习，能平滑分割结果。</p><p>SSD网络的结构被分为6个stage，每个stage学到一幅特征图，然后进行边框回归和分类，每个Stage包含多个卷积层操作。SSD采用VGG16的前五层卷积网络作为第一个stage，将VGG16中fc6和fc7两个全连接层转换为两个卷积层conv6和conv7作为第2、3个stage。接着增加了Conv8、Conv9、Conv10、Conv11四层网络，用来提取高层次语义信息。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghyfg5bl52j31da0qs0zc.jpg" srcset="/img/loading.gif" alt="img"></p><p>设计要点：</p><ul><li>Default Box生成：作者充分吸取Faster R-CNN中的Anchor机制，在每个stage中根据特征图的大小，按照固定的尺寸和长宽比生成Default Box。例如conv9这一层的特征图大小为5x5，每个点默认生成6个box，在这一层上共生成5x5x6=150个box。</li><li>特征向量生成：在每张特征图上生成许多Default Box之后，还需要生成相应的特征向量，用来进行边框回归和分类。边框回归需要4维向量，分别是x、y方向的缩放和平移。假设数据集类别数为c，加上背景，总类别数为c+1类。故SSD网络采用c+1维向量来代表某个Default Box在每个类别上得到的分数。假设使用VOC数据集，该数据集有20个类别。每个Default Box将生成一个20+1+4=25维的特征向量。</li><li>新增四层卷积网络。如前文所述，SSD共增加了Conv8、Conv9、Conv10、Conv11四个卷积层，新增的这些卷积层均通过一些小的卷积核操作。对于具有p个通道（num_output）的大小为m×n的特征层，使用3×3×p卷积核卷积操作，其中Padding和stride都为1，保证卷积后的特征图大小不变。</li><li>损失函数设计：总体目标损失函数是位置损失（loc）和置信损失（conf）的加权和。</li></ul><p>SSD算法的基本步骤是：输入图片经过卷积神经网络提取特征，生成特征图；抽取其中六层特征图，并在其每个点上生成default box；将生成的所有default box执行非极大值抑制，输出筛选后的结果。</p><p><strong>SSD算法总结：</strong></p><p>创新点：</p><ul><li>采用不同尺度的特征图进行检测，大尺度的特征图可用来检测小物体，小尺度的特征图检测大物体。</li><li>采用不同尺度和长宽比的先验框。</li><li>实现高精度的端到端训练，即使输入相对低分辨率的图像依然可以在速度和精度上得到权衡。</li></ul><p>优点：运行速度快，可与YOLO媲美；检测精度高，可与Faster R-CNN媲美。</p><p>缺点：需人工设置prior box的基础参数，prior box的基础大小和形状不能直接通过学习获得。</p><h1 id="2-Image-Segmentation-Techniques-Overview"><a href="#2-Image-Segmentation-Techniques-Overview" class="headerlink" title="2. Image Segmentation Techniques Overview"></a>2. Image Segmentation Techniques Overview</h1><p>目前的图像分割技术包括：基于区域的分割、边缘检测分割、基于聚类的分割、基于弱监督学习的CNN分割。</p><ul><li><p><strong>阈值分割方法。</strong>阈值分割是基于区域的分割算法中最常用的分割技术之一。本质是根据特定标准自动确定最佳阈值，并根据灰度使用这些像素以实现聚类。</p><ul><li>阈值分割可以分为局部阈值法和全局阈值法。全局阈值方法通过单个阈值将图像分为目标和背景两个区域。局部阈值方法需要选择多个分割阈值，并通过多个阈值将图像分为多个目标区域和背景。</li><li>最常用的阈值分割算法是最大的类间差异方法（Otsu），除此之外还有基于熵的阈值分割方法，最小误差方法，共现矩阵方法，矩保持方法，简单统计方法，概率松弛方法，模糊集方法。</li><li>优点：计算简单，运算速度快。特别是当目标和背景具有高对比度时，可以获得良好的分割效果</li><li>缺点：对于在图像中没有明显的灰度差异或灰度值有较大重叠的图像分割问题，很难获得准确的结果。由于它仅考虑图像的灰度信息而不考虑图像的空间信息，因此对噪声和灰度不均匀敏感，因此经常将其与其他方法结合使用。</li></ul></li><li><p><strong>区域增长分割法。</strong>基本思想是将具有相似属性的像素合并以形成区域，对于每个要划分的区域，首先找到一个种子像素作为生长点，然后用该区域中具有相似属性的像素点合并周围邻域。</p><ul><li>优点：可以将具有相同特征的连接区域分开，并提供良好的边界信息和分割结果。</li><li>缺点：计算量大，噪声和灰度不均匀会导致空隙和过度分割。阴影往往对图像造成不好的影响。</li></ul></li><li><p><strong>边缘检测分割法。</strong>利用像素的不同区域边缘的的像素灰度或颜色不连续性检测区域来实现图像分割。</p><ul><li>对象的边缘呈图像的不连续局部特征的形式，即图像的最重要部分局部亮度发生变化，例如灰度值的突变，颜色的突变，纹理的变化等。使用不连续性是为了检测边缘，从而达到图像分割的目的。</li><li>可以使用导数运算来检测两个相邻区域之间灰度值不连续的情况。</li><li>广泛的一阶微分算子是Prewitt算子，Roberts算子和Sobel算子。</li><li>二阶微分算子具有非线性算子，例如Laplacian，Kirsch算子和Wallis算子。</li></ul></li><li><strong>基于聚类的分割。</strong>以事物之间的相似度为分类准则，即根据样本集的内部结构将其划分为多个子类，以使同类样本尽可能相似，不同的样本尽可能不相似。<ul><li>利用特征空间聚类方法对图像空间中的像素点进行分割，得到相应的特征空间点。根据它们在特征空间中的聚集，对特征空间进行分割，然后将它们映射回原始图像空间，得到分割结果。</li><li>K-Means是最常用的聚类算法之一。K-means的基本思想是根据距离将样本集合到不同的簇中。两点越接近，越能得到紧凑独立的簇作为聚类目标。</li><li>K-Means优点：快速、简单、高效、可扩展，适用于大数据集。</li><li>K-Means缺点：算法每次迭代都要遍历所有样本，时间复杂度高。且聚类数K没有明确的选择标准。</li></ul></li><li><strong>基于弱监督学习的CNN分割方法。</strong>涉及为图像中的每个像素分配语义标签的问题，由三个部分组成。1）提供包含哪些对象的图像。2）给出对象的边界。3）图像中的目标区域标记有部分像素。</li></ul><h1 id="3-FCN"><a href="#3-FCN" class="headerlink" title="3. FCN"></a>3. FCN</h1><p>对于一般的分类CNN网络，如VGG和Resnet，网络的最后都是全连接层，经过softmax后可以获得类别的概率信息。但这个概率信息是一维的，适用于整个图片的分类，不能用于表示每个像素点的类别，所以全连接方法不适用于图像分割。FCN即把最后的全连接层都转换为卷积层，这样就可以获得一张二维特征图，之后经过softmax获得每个像素点的分类信息，从而解决分割问题。</p><p>上采样有3种方法：双线性插值、反卷积和反池化。反卷积是一种特殊的正向卷积，先按照一定的比例补0来扩大输入图像尺寸，然后旋转卷积核进行正向卷积。上采样的意义在于将小尺寸的高维度feature map恢复回去，以便做逐像素的预测，获得每个点的分类信息。反卷积操作输出的图像实际是很粗糙的，丢失了很多细节。因此需要找到一种方式填补丢失的细节数据，所以就有了跳跃结构。</p><p>FCN网络的原理（跳跃结构）如图所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghyfg68dw5j30wx0mf765.jpg" srcset="/img/loading.gif" alt="img"></p><p>1） FCN-32s：直接对pool5特征层进行32倍上采样获得32x upsampled feature，再对其每个点做softmax，获得32x upsampled feature 分割图。</p><p>2） FCN-16s:对pool5进行2倍上采样，将其与pool4逐点相加，最后对相加的特征图进行16倍上采样，并做softmax，获得16x upsampled feature prediction.</p><p>3） FCN-8s:将FCN-16s中相加得到的特征图再与pool3逐点相加，得到更多层次的特征融合。</p><p>作者在文中给出了三种网络的结果对比，显然多层特征融合有助于提高分割准确性。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghyfg6ts8bj30rm0bqwj0.jpg" srcset="/img/loading.gif" alt="img" style="zoom:33%;" /></center><p><strong>FCN总结：</strong></p><p>优点：</p><ul><li>FCN将传统CNN中的全连接层转换为一个个卷积层。因为没有全连接层，可适应任意尺寸输入，不用要求所有训练图像和测试图像具有同样的尺寸。</li><li>FCN通过反卷积对低分辨率图像进行上采样，仅使用最后一层池化后的特征图进行上采样，得到的结果很粗糙，作者使用了一个跳跃结构，将不同池化层的结果进行特征融合，补充细节，优化输出结果。</li></ul><p>缺点：</p><ul><li>没有考虑到像素间的关系，忽略了在通常的基于像素分类的分割方法中使用的空间规整步骤，缺乏空间一致性。</li><li>分割结果不够精细，图像过于模糊或平滑，没有分割出目标图像的具体细节。</li></ul><h1 id="4-MASK-R-CNN"><a href="#4-MASK-R-CNN" class="headerlink" title="4. MASK R-CNN"></a>4. MASK R-CNN</h1><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghyfg7aeicj30u80fck1u.jpg" srcset="/img/loading.gif" alt="img" style="zoom: 33%;" /></center><p>  Mask R-CNN是一个非常灵活的框架，可以增加不同的分支完成不同的任务，例如:实例分割、语义分割、目标检测、目标检测、目标分类、人体姿势识别等多种任务。为同时实现高速和高准确率检测，作者联合了经典的目标检测算法Faster R-CNN和经典的语义分割算法FCN。同时发现了ROI Pooling中存在的像素偏差问题，提出了ROI Align策略进行修正，进一步提高了准确率。</p><p>  Mask R-CNN可被分解为3个模块：Faster R-CNN、ROIAlign和FCN，这里重点介绍ROIAlign。传统的ROI Pooling方法在进行池化操作时，会对非整数结果进行量化操作（即取整）。为了得到固定大小的feature map，需要经过两次量化过程：1）图像坐标-&gt;feature map坐标2)feature map坐标-&gt;ROI feature坐标。这就引入了两次误差，导致将feature空间的ROI对应到原图上时，会出现很大的偏差。为解决此问题，作者提出了ROIAlign方法，ROIAlign方法没有使用量化操作，即保留池化运算结果的小数点，不取整。但是像素点的位置都是整数，为小数的像素点的值应该是多少呢？作者使用了“双线性插值”算法，利用图像中虚拟点四周真实存在的四个像素值来估计出该虚拟点的值。这样以来，没有使用量化操作，也就没有引入误差，即feature map中的像素和原图中的像素完全对齐，没有偏差，这将有利于进行实例分割。</p><p>  MASK R-CNN在损失函数中加入了Mask分支，定义了Lmask允许网络为每一类生成一个mask，计算loss的时候，并不是每个类别的sigmoid输出都计算二值交叉熵损失，而是该像素属于哪个类，哪个类的sigmoid输出才要计算损失。不用和其它类进行竞争，将分类和mask生成分解开来，不同的mask之间不存在竞争关系，可以提高实例分割的效果。</p><p><strong>总结：</strong></p><ul><li>Mask R-CNN使用ResNet-FPN作为backbone，有利于多尺度物体的检测和分割。</li><li>提出了RoI Align，解决了ROI Pooling的像素偏差问题，大幅提高分割的准确率。</li><li>新增Mask分支, 在每个ROI上像素级别地预测分割masks，该分支与原有的classification、bounding box regression并行进行，将mask分割预测和分类预测彻底解耦。</li><li>独立地为每个类别预测一个二进制掩码，类别之间没有竞争，并且依靠网络的ROI分类分支来预测类别。</li></ul><h1 id="5-目标跟踪算法研究综述"><a href="#5-目标跟踪算法研究综述" class="headerlink" title="5. 目标跟踪算法研究综述"></a>5. 目标跟踪算法研究综述</h1><p>跟踪算法的精度和鲁棒性取决于对运动目标的表达和相似性度量的定义。</p><p>跟踪算法的实时性取决于匹配搜索策略和滤波预测算法。</p><p>目标跟踪算法的分类：</p><p>  按跟踪对象划分：基于模型的跟踪、基于区域的跟踪、基于轮廓的跟踪、基于特征的跟踪</p><ul><li><strong>基于模型的目标跟踪</strong></li></ul><p>根据目标的几何模型进行跟踪，在学习阶段构建模型，跟踪阶段对区域和目标模型进行匹配。</p><p>鲁棒性强，跟踪精度高，抗干扰能力强，但计算复杂。</p><p>模型构建有三种方式：</p><p>1） 线图模型（Stick Figures Model）根据人的骨骼运动使用不同的直线组合来表示人体。</p><p>2） 2D轮廓模型，利用人体投影建立模型。</p><p>3） 3D模型，利用三维立体模型来描述人体。</p><ul><li><strong>基于区域的目标跟踪</strong></li></ul><p>根据先前目标检测的结果或手动输入的数据，提前确定好目标模板，在相邻图像上与目标模板进行匹配，根据匹配程度对目标进行跟踪。由于提取了完整的目标模板，相对于其他跟踪算法获取的信息更多。近期的研究集中在处理模板变化的情况。缺点：逐帧匹配，运算量大。</p><ul><li><strong>基于轮廓的目标跟踪</strong></li></ul><p>使用封闭的轮廓描述目标，先描述出初始目标，然后不断的更新轮廓，实现连续跟踪。</p><ul><li><strong>基于特征的目标跟踪</strong></li></ul><p>常用的局部特征：轮廓、颜色、卷积特征、纹理。</p><p>相关滤波器用于描述数字信号之间的相似性，即信号之间的联系。</p><p><strong>主流的相关滤波算法：</strong></p><ul><li><strong>最小输出平方误差和滤波器：</strong>在图像初始帧确定目标位置，跟踪时与前一帧模型进行比较。在根据PSR值检测到目标被遮挡时可以暂停跟踪，在目标重新出现后继续跟踪。适用于简单、小尺度变化的场景，算法鲁棒性好但只使用灰度作为特征，在目标尺度发生改变时难以适应。</li><li><strong>基于颜色的粒子滤波器跟踪算法：</strong>将颜色分布集成到粒子滤波中，与基于边缘的图像特征结合使用。选用颜色分布是因为颜色对部分遮挡具有鲁棒性，旋转和缩放图像颜色不变。但由于颜色会随着光照、时间、视角、相机参数等因素发生变化，因此在跟踪图像时会动态调整目标模型。基于颜色的粒子滤波器常用于非线性和非高斯估计的问题，但容易受到光照影响，实时性差。</li><li><strong>判别相关滤波器</strong>(Discriminative Correlation Filters)<strong>：</strong>利用训练样本的周期假设，有效的学习场景中所有补丁的分类器得到邻域。周期假设引入了不必要的边界效应，降低了跟踪模型的质量等级。可以应用于复杂场景，但难以适应目标出现大幅度的形变。</li><li><strong>空间正则化判别滤波器</strong>（Spatially Regularized Discriminative Correlation Filters）<strong>：</strong>在学习阶段引入空间规则化分量，根据相关滤波器系数的空间分布阳离子来乘法了相关滤波器系数。也可用于复杂场景，但跟踪后图像分辨率降低。</li></ul><h1 id="6-目标跟踪综述"><a href="#6-目标跟踪综述" class="headerlink" title="6. 目标跟踪综述"></a>6. 目标跟踪综述</h1><p>根据跟踪目标数目的不同，目标跟踪可分为单目标跟踪和多目标跟踪。目前主流的目标跟踪算法主要由外观模型和目标搜索两个部分组成。目标搜索是对比视频中每一帧图像的信息来帮助目标跟踪，外观模型主要是充分利用目标特征，使跟踪目标和背景区分出来。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghyfg5rue0j30ou05y3zh.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>  主流的目标跟踪系统结构如图1所示，分为初始化、特征提取、外观模型、目标搜索4个部分，其中外观模型和目标搜索是整个系统的核心。跟踪算法的效果很大程度上取决于目标外观模型的建立，建立合适的外观模型能有效提高算法的跟踪性能。目标搜索用于对目标下一帧的运动状态进行预测，减少目标搜索范围。</p><p>  <strong>外观特征</strong>有：边缘特征、灰度特征、颜色特征、纹理特征、梯度特征。</p><ul><li>边缘检测寻找图像中亮度发生剧烈变化的点的集合即为目标轮廓。常用且效果好的算法：Canny算子。</li><li>灰度特征转化简单、容易计算，其中灰度值和区域灰度变化特征(Haar特征)这两种灰度特征表征形式最为常用。使用灰度值作为外观模型的表达方式最为简单直接，但其对外观表征能力较弱，易受环境变量干扰。Haar特征反映了图像区域的灰度变化情况。</li><li>颜色特征难以准确描述目标姿态变化，外观表征能力较弱，常与其他特征共同使用来表征目标外观。颜色直方图能通过计算图像中不同色彩所占的比例来大致表述其外观，是一种全局特征。</li><li>纹理特征描述目标或目标周围的表面变化，也是一种全局特征。纹理特征只能表述目标的表面性质，具有局限性。因此仅用纹理特征无法描述目标的本质特征。常用LBP（Local Binary Pattern）来描述目标的纹理特征，通过对比目标像素与其邻域像素得到目标像素的LBP值，该值反映了该像素周围区域的纹理信息。</li><li>梯度特征通过梯度分布来描述目标。最原始的描述算子为SIFT算子，SIFT算子的计算过程较为复杂且耗时，对实时性有影响。因此在SIFT的基础上增加SURF算子来减少计算时间，提高实时性。此外，在目标跟踪中使用更为广泛的是HOG算子，该算子对目标局部区域的梯度方向进行统计，能够良好的表征目标局部像素之间的关联。梯度特征的缺点是无法准确描述目标姿态、外观等信息。</li></ul><p>以上每种特征都有其优缺点，采用单一特征的方式往往不能很好的表达目标的外观变化，通常需要将多种特征进行融合，以提高目标外观模型与背景的区分能力。</p><p><strong>外观模型：</strong></p><p>  外观模型的发展经历了两个阶段：第一阶段是在环境条件提前设定且目标运动稳定的条件下提出。第二阶段针对周围环境和运动状态发生改变而设计自适应的外观模型。构建外观模型有两种方法：生成式模型和判别式模型。</p><ul><li>生成式模型：首先对跟踪目标进行学习，得到目标的外观模型，之后选择和目标最为接近的区域作为目标区域，需要不断的更新外观模型来保证跟踪的准确性。<ul><li>基于模板的学习模型：固定特征模板、自适应特征模板。固定特征模板的缺点：无法提前改变参数来适应目标外观变化，固定的特征参数难以完全描述所有的外观变化情况。自适应高斯混合模型的特征分量可根据目标外观变化，调整分量的数量以达到最好的逼近目标外观变化的效果。</li><li>基于子空间的学习模型：将目标映射到低维的子空间中，该子空间包含目标的外观模型数据，通过对子空间中的外观模型进行训练并不断更新，实现对目标的自适应跟踪。<ul><li>基于向量的子空间模型：将目标通过向量的形式映射到子空间。</li><li>基于非线性子空间：LLE算法采用非线性降维将目标映射到非线性子空间中，使跟踪算法能够适应复杂跟踪环境且对目标突然外观变化有良好的跟踪效果。</li><li>基于稀疏子空间：稀疏表示归根结底是求约束条件的L1范数最小化问题，该方法计算量大但对目标被遮挡和目标姿态变化的情况具有良好的跟踪效果。</li></ul></li></ul></li><li>判别式外观模型：借鉴机器学习的方法将跟踪目标和背景分成两类，将目标跟踪转换为分类问题。优点是对跟踪目标外观有良好的区分能力。<ul><li>基于SVM的外观模型：将跟踪目标设定为正样本，背景设定为负样本，通过SVM方法对正负样本进行训练得到训练好的模型，并结合其他自适应的学习方法来实时更新目标外观模型。SVM方法只是对单一目标进行训练，需要准确选择训练目标，一旦训练目标选择错误将会影响跟踪效果。</li><li>基于Boosting的外观模型：将一堆弱分类器提升为一个强分类器，通过不断迭代的方法优化分类器的分类效果。</li></ul></li></ul><p><strong>目标搜索算法：</strong></p><p>  为减少计算量，提高算法的实时性，需要通过目标搜索算法预测运动目标在下一帧图像中可能出现的位置。</p><ul><li>卡尔曼滤波</li></ul><p>对跟踪目标的运动轨迹进行最小方差估计，从而对运动目标的下一帧位置进行预测。</p><p><strong>优点</strong>：计算量小，实时性较高。<strong>局限性：</strong>仅适用于符合高斯分布的运动系统。</p><ul><li>粒子滤波</li></ul><p>通过在跟踪目标上一帧的位置附近进行粒子采样，得到每个粒子位置图像与目标的相似性分布，再对粒子进行重要性采样，形成对目标的实时跟踪。</p><p><strong>优点：</strong>适用于非线性、非高斯系统，计算量小，实时性好。</p><ul><li>均值漂移算法（Meanshift算法）</li></ul><p>通过计算目标上一帧模板和当前帧目标位置的候选模板的相似性，选择相似程度较高的模板得到目标的Meanshift向量（由目标上一帧位置指向当前帧位置）。通过不断迭代计算新的Meanshift向量，最终收敛到当前帧目标的最终位置，实现目标跟踪。</p><p>  文章介绍了目标跟踪算法在深度学习背景下的发展，指出深度学习在目标检测领域的应用，主要瓶颈是训练样本的缺失，因为深度学习的运用是在大量样本训练基础上实现的，而在目标跟踪中只能提供第一针的图像区域作为训练样本，很难针对当前目标训练深度模型。</p><p>  <strong>基于深度学习的跟踪算法：</strong></p><ul><li>FCNT：直接使用ImageNet训练CNN网络来提取目标区域的特征，再通过相应的观测模型进行分类，实现目标跟踪。</li><li>DLT和SO-DLT：通过对大量的非跟踪辅助数据进行预训练（该辅助数据需与跟踪目标有相同特征），得到对待跟踪目标的通用模型，在具体跟踪时根据目标样本对预训练模型进行微调即可得到对当前目标有很强分类能力的模型。</li></ul><p>文章最后介绍了目标跟踪的应用领域：智能安防监控、人机交互、异常行为识别分析、行人追踪、套牌车识别、无人机应用、无人驾驶及智能交通控制。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SSD</tag>
      
      <tag>FCN</tag>
      
      <tag>MASK R-CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——SSD:Single Shot MultiBox Detector</title>
    <link href="/2020/08/19/SSD/"/>
    <url>/2020/08/19/SSD/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们提出了一种使用单个深层神经网络检测图像中的目标的方法。我们的方法，名为SSD。将边界框的输出空间离散化为一组默认框，该默认框在每个特征图位置有不同的宽高比和尺寸。在预测期间，网络针对每个默认框中的每个存在对象类别生成分数（即给其打分），并且对框进行调整以更好地匹配对象形状。另外，网络结合不同分辨率的多个特征图的预测来自然处理各种尺寸的对象。SSD相对于需要region proposal的方法是简单的，因为它完全消除了生成候选框及之后的像素或特征的重采样阶段，并将所有计算封装在单网络中。这使得SSD容易训练和直接集成到需要检测组件的系统。PASCALVOC，MS COCO和ILSVRC数据集的实验结果证实，SSD与使用额外的region proposal（区域候选框，具体可参看R-CNN系列论文，表示在图像中目标可能的区域提取n个候选框然后分类训练）的方法具有可比较的准确性，并且速度更快，同时为训练和推理提供统一的框架。对VOC2007，在300×300输入，SSD在Nvidia Titan X上59FPS时达到74.3％的mAP，512×512输入SSD达到76.9％的mAP，优于可与之相比的最新的Faster R-CNN模型。代码链接：<a href="https://github.com/weiliu89/caffe/tree/ssd" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd</a></p><p>关键词：试试目标检测；卷积神经网络</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>当前，现有对象检测系统是以下方法的变体：假设边框，对每个框的像素或特征重新取样，再应用高质量分类器（边框选择——边框归一化——CNN训练提取特征——分类器分类）。自从选择性搜索[1]提出以来，特别是基于Faster R-CNN[2]和[3]等这类更深特征的方法在PASCAL VOC，MSCOCO和ILSVRC检测取得领先结果后，这种检测流程成为检测领域的基准。尽管准确，但这些方法对于嵌入式系统来说计算量过大，即使对于高端硬件，对于实时或接近实时的应用来说也太慢。 这些方法的检测速度通常以每秒帧数为单位进行测量，即使最快的高精度检测器( Faster R-CNN）仅以每秒7帧（FPS）运行。目前，已经有许多尝试通过研究检测流程的每个阶段来建立更快的检测器（参见第4节中的相关工作），但是迄今为止，显着增加的速度仅仅是以显着降低的检测精度为代价。</p><p> 本文提出了第一个基于深层网络的目标检测器，它不会对假设边框中的像素或特征进行重新取样，但和那种做法一样准确（即有效避免了Faster-R-CNN中的重复采样）。这使高精度检测的速度有了显着提高（在VOC2007测试中, 58 FPS下 72.1％mAP，对Faster R-CNN 7 FPS 下mAP 73.2％，YOLO 45 FPS 下mAP 63.4％）。速度的根本改进来自消除候选边框（bounding box proposals）和随后的像素或特征重采样阶段。我们不是首次这么做（cf [4,5]），但是通过增加一系列改进，我们设法提高了以前尝试的准确性。我们的改进包括使用小型卷积滤波器来预测对象类别和边界框位置中的偏移量，使用单独的预测器（滤波器）来检测不同宽高比，并将这些滤波器应用于网络后期的多个特征映射中，以便在多个尺度上执行检测。通过这些修改，我们可以使用相对低分辨率的输入实现高精度检测，进一步提高处理速度。 虽然这些贡献独立看可能觉得贡献很小，但我们注意到，所得的系统提高了PASCAL VOC的实时高速检测的准确性，从YOLO的63.4％mAP到我们提出的网络的72.1％mAP。相比于最近在残差网络[3]方面的工作，这是相对于目标检测精度的相对提高。  此外，显着提高高质量检测的速度可以拓宽计算机视觉有效使用范围。</p><p>总结我们的贡献如下：</p><ul><li><p>我们介绍了SSD，一个单次检测器，用于多个类别，比先前最先进的单次检测器（YOLO）速度更快，并且更准确很多，实际上和明确使用区域候选region proposal、pooling的更慢的技术一样准确（包括Faster RCNN）</p></li><li><p>SSD的核心是使用应用于特征映射的小卷积滤波器来预测固定的一组默认边界框的类别分数和框偏移量。</p></li><li>为了实现高检测精度，我们从不同尺度的特征图产生不同尺度的预测，并且通过宽高比来明确地分离预测。</li><li>这些设计特性得到了简单的端到端训练和高精度，进一步提高速度和精度的权衡，即使输入相对低分辨率图像。</li><li>实验包括在PASCAL VOC，MS COCO和ILSVRC上评估不同输入大小下模型耗时和精度分析，并与一系列最新的先进方法进行比较。</li></ul><h1 id="2-单次检测器（SSD）"><a href="#2-单次检测器（SSD）" class="headerlink" title="2.单次检测器（SSD）"></a>2.单次检测器（SSD）</h1><p>本节介绍我们提出的SSD检测架构（第2.1节）和相关的训练方法（第2.2节）。之后， 第3节呈现特定数据集的模型细节和实验结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghwg9pb9pkj310q0eiqdp.jpg" srcset="/img/loading.gif" alt="image-20200819213913182"></p><p><strong>图1：SSD架构。</strong> （a）SSD在仅需要一张输入图像和训练期间每个目标的真实标签框。以卷积的方式，我们评估在具有不同尺度（例如图（b）和（c）中的8×8和4×4）的若干特征地图中的每个位置处的不同长宽比的默认框的小集合（例如4个）。 对于每个默认框，我们预测对所有对象类别（（c 1，c2，…，cp））的形状偏移和置信度。在训练时，我们首先将这些默认框匹配到真实标签框。 例如，两个默认框匹配到猫和狗，这些框为正，其余视为负。 模型损失是定位损失（例如Smooth L1 [6]）和置信损失（例如Softmax）之间的加权和。</p><h2 id="2-1-模型"><a href="#2-1-模型" class="headerlink" title="2.1 模型"></a>2.1 模型</h2><p>SSD方法基于前馈卷积网络，该网络会生成固定大小的边界框集合，并对这些框中存在的对象类别实例进行评分，然后执行非最大抑制步骤以进行最终检测。早期的网络层基于用于高质量图像分类的标准体系结构（在任何分类层之前均被截断），我们将其称为基础网络。然后，我们向网络添加辅助结构，产生了具有以下主要特征的检测：</p><p><strong>多尺度特征图检测：</strong>我们将卷积特征层添加到截断的基础网络（CNN）的末尾。这些层尺寸（基础网络层的卷积核导致尺寸越来越小的特征图）逐渐减小，得到多个尺度检测的预测值。预测检测的卷积模型对于每个特征层是不同的（参见在单个尺度特征图上操作的Overfeat [4]和YOLO[5]（两者皆是以一种固定大小的边框去移动截取特征框））。</p><p><strong>检测的卷积预测器：</strong>每个添加的特征层（或可选的基础网络中已存在的特征层）可以使用一组卷积滤波器产生固定的预测集合。这些在图2中SSD网络架构顶部已指出。对于具有p个通道（num_output）的大小为m×n的特征层，使用3×3×p卷积核卷积操作，这导致每一个类别产生一个分数，或者产生相对于默认框的坐标偏移。在每个应用卷积核运算的m×n位置处，产生一个输出值。边界框偏移输出值是相对于与每个特征图位置相对的默认框位置测量的（参见YOLO [5]的架构，中间使用全连接层而不是用于该步骤的卷积滤波器）。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghwzus25dwj30zo0j6tc4.jpg" srcset="/img/loading.gif" alt="image-20200820085653680"></p><p><strong>图2：两个单次检测模型之间的比较：SSD和YOLO [5]。</strong> 我们的SSD模型在基础网络的末尾添加了几个特征层，这些层预测了不同尺度和宽高比对默认框的偏移及其相关置信度。 300×300输入尺寸的SSD在VOC2007测试中的精度显着优于448×448输入的YOLO的精度，同时还提高了运行速度，尽管YOLO网络比VGG16快。</p><p><strong>默认框与宽高比：</strong>我们将一组默认边界框与每个特征映射单元相关联，以用于网络顶部的多个特征图（以提取到的边框做CNN训练的输入数据）。默认框以卷积的方式平铺特征映射，以便每个框相对于其对应单元的位置是固定的。在每个特征映射单元中，我们预测相对于单元格中的默认框形状的偏移，以及指出这些框中每个框存在的详细类别的每类评分。具体来说，对于在给定位置的k个框中每个框，我们计算c类分数和相对于原始默认框的4个偏移量。这使得在特征图中的每个位置需要总共（c+4）k个滤波器，对于m×n特征图产生（c+4）kmn个输出。有关默认框的说明，请参见图1。我们的默认框类似于Faster R-CNN [2]中使用的anchor boxes（固定框），但我们将其应用于不同分辨率的特征图中。在多个特征图中使用不同的默认框形状，可以有效地离散可能的输出框形状空间。</p><h2 id="2-2训练"><a href="#2-2训练" class="headerlink" title="2.2训练"></a>2.2训练</h2><p>训练SSD和训练使用region proposal的典型分类器的关键区别在于，真实标签信息需要被指定到固定的检测器输出集合中的某一特定输出。Faster R-CNN [2]和MultiBox [7]的regionproposal阶段、YOLO [5]的训练阶段也需要类似这样的标签。一旦确定了该指定，则端对端地应用损失函数和反向传播。训练还涉及选择默认框和检测尺度的集合，以及hard negative mining（硬负挖掘）和数据增强策略。</p><p><strong>匹配策略：</strong>在训练时，我们需要在训练期间，我们需要确定哪些默认框对应于真实标签检测并相应地训练网络（建立真实标签和默认框之间的对应关系）。请注意，对于每个真实标签框，我们从默认框中进行选择，这些默认框随位置、纵横比和尺度而变化。起始时，我们匹配每个真实标签框与默认框最好的jaccard重叠（这是原始MultiBox [7]使用的匹配方法，它确保每个真实标签框有一个匹配的默认框）。与MultiBox不同，我们将默认框与jaccard重叠高于阈值（0.5）的任何真实标签框相匹配。这简化了学习问题：它使得有多个重叠默认框时网络预测获得高置信度，而不是要求它选择具有最大重叠的那个。</p><p><strong>训练</strong>：SSD训练来自MultiBox[7,8]，但扩展到处理多个对象类别。 以<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx070qawij306i01qjrb.jpg" srcset="/img/loading.gif" alt="image-20200820090839899" style="zoom:33%;" />表示第<em>i</em>个默认框与类别<em>p</em>的第<em>j</em>个真实标签框相匹配（匹配正确与否用1或0表示）。根据上述匹配策略，我们有<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0890w2pj306401mdfr.jpg" srcset="/img/loading.gif" alt="image-20200820090950851" style="zoom:33%;" />（意味着可以有多于一个与第<em>j</em>个真实标签框相匹配的默认框。）总体目标损失函数是位置损失（loc）和置信损失（conf）的加权和：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0bcy741j30nk02st8x.jpg" srcset="/img/loading.gif" alt="image-20200820091249686" style="zoom:50%;" /></center><p>其中N是匹配的默认框的数量，如果N = 0，则将损失设置为0。位置损失是预测框（l）和真实标签值框（g）参数之间的平滑L1损失（L1范数）[6]。 类似于Faster R-CNN [2]，我们回归到默认边框（d）的中心（cx,cy）和其宽度（w）高度（h）的偏移。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0dq9b7hj30zq0acabo.jpg" srcset="/img/loading.gif" alt="image-20200820091506243" style="zoom:50%;" /></center><p>置信损失是softmax损失对多类别置信（c）和权重项α设置为1的交叉验证。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0f3f2tkj314u044aaw.jpg" srcset="/img/loading.gif" alt="image-20200820091625086"></p><p><strong>选择默认框的尺度和横宽比</strong>：【大多数卷积网络通过加深层数减小特征图的大小。这不仅减少计算和存储消耗（小卷积核减少了参数所以节省了空间），而且还提供一定程度的平移和尺寸不变性。】为了处理不同的目标尺寸，一些方法[4,9]建议将图像转换为不同的尺寸，然后单独处理每个尺寸，然后组合结果。然而，通过用单个网络中的若干不同层的特征图来进行预测，我们可以得到相同的效果，同时还在所有目标尺度上共享参数。之前的研究[10,11]已经表明使用来自较低层的特征图可以提高语义分割质量，因为较低层捕获到输入对象的更精细的细节。类似地，[12]表明，添加从特征图下采样的全局背景可以帮助平滑分割结果。受这些方法的启发，我们使用低层和高层的特征图进行检测预测。图1示出了在框架中使用的两个示例特征图（8×8和4×4），当然在实践中，在实践中，我们可以使用更多的小计算量的特征图（即较小的卷积核）。</p><p>已知网络中不同层次的特征图具有不同的（经验的）感受野大小[13]。幸运的是，在SSD框架内，默认框不需要对应每层的实际感受野。我们可以设计默认框平铺，以便特定的特征图学习响应目标的特定尺度。假设我们要使用m个特征图做预测。每个特征图的默认框的比例计算如下：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0olj2n3j30my02mgls.jpg" srcset="/img/loading.gif" alt="image-20200820092533563" style="zoom:67%;" /></center><p>其中smin是0.2，smax是0.95，意味着最低层具有0.2的尺度，最高层具有0.95的尺度（scale类似与BN层配合的scale），并且其间的所有层是规则间隔的。我们对默认框施以不同的宽高比，表示为ar∈{1,2,3,1/2,1/3}。我们可以计算每个默认框的宽度<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0pgewkfj306m01iwef.jpg" srcset="/img/loading.gif" alt="image-20200820092622978" style="zoom:50%;" />和高度<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0r1z4ivj307u01gweg.jpg" srcset="/img/loading.gif" alt="image-20200820092755308" style="zoom: 50%;" />对于宽高比为1，我们还添加了一个缩放为<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0rqtf4tj307c01kglj.jpg" srcset="/img/loading.gif" alt="image-20200820092835125" style="zoom: 50%;" />的默认框，结果使每个特征图位置有6个默认框。设定每个默认框中心为<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0skwu04j306w024weg.jpg" srcset="/img/loading.gif" alt="image-20200820092922922" style="zoom:50%;" />，其中<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0t2p91tj302401ot8k.jpg" srcset="/img/loading.gif" alt="image-20200820092951594" style="zoom:50%;" />是第k个正方形特征图的大小，<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx0u560lej306y01gt8n.jpg" srcset="/img/loading.gif" alt="image-20200820093052846" style="zoom: 50%;" />，随后截取默认框坐标使其始终在[0，1]内。在实践中，也可以用最合适的特定数据集设计默认框的分布。如何设计默认框的最佳平铺也是一个悬而未决的问题。</p><p>通过组合许多特征图在所有位置的不同尺寸和宽高比的所有默认框的预测，我们具有多样化的预测集合，覆盖各种输入对象尺寸和形状。例如图 1中，狗被匹配到4×4特征图中的默认框，但不匹配到8×8特征图中的任何默认框。这是因为那些框具有不同的尺度且不匹配狗的真实框，因此在训练期间被认为是负样本。</p><p><strong>Hard negative mining（硬负挖掘）</strong> ：在匹配步骤之后，大多数默认框都是负样本，特别是当可能的默认框数量很大时。这导致了训练期间正负样本的严重不平衡（正负样本的不均衡会导致发散或者精度经常保持为1）。我们不是使用所有负样本，而是使用每个默认框的最高置信度来使负样本排序，然后挑选较高置信度的负样本，以便负样本和正样本之间的比率至多为3：1。我们发现，这导致更快的优化和更稳定的训练。</p><p><strong>数据增强</strong>：为了使模型对于各种输入对象大小和形状更加鲁棒，每个训练图像通过以下选项之一随机采样：</p><p>-  使用整个原始输入图像</p><p>-  采样一个片段，使对象最小的jaccard重叠为0.1,0.3,0.5,0.7或0.9。</p><p>-  随机采样一个片段</p><p>每个采样片段的大小为原始图像大小的[0.1，1]，横宽比在1/2和2之间。如果真实标签框中心在采样片段内，则保留重叠部分。在上述采样步骤之后，除了应用类似于[14]中所述的一些光度失真之外，将每个采样片段调整为固定大小并以0.5的概率水平翻转。</p><h1 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3.实验结果"></a>3.实验结果</h1><p><strong>基础网络</strong>：我们的实验基于VGG16 [14]网络，在ILSVRC CLS-LOC数据集[15]预训练。类似于DeepLab-LargeFOV [16]，我们将fc6和fc7转换为卷积层，以从fc6和fc7采样参数，将pool5从2×2-s2更改为3×3-s1，并使用atrous算法填“洞”（指权重初始化方法）。我们删除了所有的dropout层和fc8层，使用SGD对这个模型进行fine-tune，初始学习率 0.001，0.9 momentum, 0.0005 weight decay, batch大小32。每个数据集的学习速率衰减策略略有不同，稍后我们将描述详细信息。所有训练和测试代码在caffe框架编写，开源地址：<a href="https://github.com/weiliu89/caffe/tree/ssd。" target="_blank" rel="noopener">https://github.com/weiliu89/caffe/tree/ssd。</a></p><h2 id="3-1-PASCAL-VOC2007"><a href="#3-1-PASCAL-VOC2007" class="headerlink" title="3.1 PASCAL VOC2007"></a>3.1 PASCAL VOC2007</h2><p>在这个数据集上，我们比较了Fast R-CNN [6]和Faster R-CNN [2]。所有方法使用相同的训练数据和预训练的VGG16网络。特别地，我们在VOC2007train val和VOC2012 train val（16551images）上训练，在VOC2007（4952图像）测试。</p><p> 图2显示了SSD300模型的架构细节。我们使用conv4_3，conv7（fc7），conv8_2，conv9_2，conv10_2和pool11来预测位置和置信度（对SSD500模型，额外增加了conv11_2用于预测），我们在conv4_3^3上设置了比例为0.1的默认框。用“xavier”方法初始化所有新添加的卷积层的参数[18]。由于conv4_3的大小较大（38×38），因此我们只在其上放置3个默认框：一个0.1比例的框和另外纵横比为1/2和2的框。（对于conv4_3，conv10_2和conv11_2，我们只在每个特征图位置关联4个默认框， - 省略1/3和3的纵横比。）对于所有其他层，我们设置6个默认框，如第 2.2节。如[12]中所指出的，由于conv4_3与其他层相比具有不同的特征尺度，我们使用[12]中引入的L2（2范数）正则化技术，将特征图中每个位置处的特征范数（特征标准）缩放为20，并在反向传播期间学习这种缩放（尺度）。我们使用学习速率0.001进行40k次迭代，然后将其衰减到0.0001，并继续训练另外20k次迭代（并使用学习率0.0001和0.00001分别继续训练10K次迭代）。当我们在VOC2007训练集上训练时，表1显示，我们的SSD300模型已经比Fast R-CNN更准确。当以更大的500×500输入图像训练SSD，结果更准确，甚至惊人的超过了Faster R-CNN 1.9% mAP（mAP表示平均精度的均值）（当在更大的512x512输入图像上训练SSD时，结果甚至更加准确，惊人的超过了Faster R-CNN有1.7%的mAP）。如果使用更多的数据（例如VOC2007加上VOC2012）训练SSD，我们看到SSD300已经可以优于Faster R-CNN约1.1%，而SSD512更是达到3.6%。如果使用这个模型去训练如3.4节描述的COCO trainval135k和在07+12数据集使用SSD512微调，我们获得了最好的结果：81.6%mAP。</p><p> 为了更详细地了解我们的两个SSD模型的性能，我们使用了来自[19]的检测分析工具。图3显示SSD可以高质量检测（大、白色区域）各种对象类别。它的大部分置信度高的检测是正确的。召回率在85-90％左右，并且比“弱”（0.1 jaccard重叠（jaccard相似度可以看成两个集合A,B的交集占并集的比例：Jaccard Sim = (A∩B) / (A∪B)，其实就是计算A与B产生重叠的程度。））标准高得多。与R-CNN [20]相比，SSD具有较少的定位误差，表明SSD可以更好地定位目标，因为它直接学习回归对象形状并对对象类别进行分类，而不是使用两个分离的步骤。然而，SSD对相似对象类别（尤其是动物）有更多的混淆，部分是因为多个类别共享了位置。 图4显示SSD对边界框尺寸非常敏感。换句话说，它对较小的对象比对较大的对象具有更差的性能。这毫不意外，因为小对象在最顶层可能没有任何信息保留下来。增加输入尺寸（例如从300×300到500×500）可以帮助改善检测小对象，但是仍然有很大改进空间。积极的一面是，我们可以清楚地看到SSD在大对象上表现很好。并且对于不同的对象宽高比非常鲁棒，因为我们对每个特征图位置使用各种长宽比的默认框。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1a7rpcuj315m0ai431.jpg" srcset="/img/loading.gif" alt="image-20200820094619896"></p><p><strong>表1 ： PASCAL VOC2007测试集检测结果。</strong>Fast和Faster R-CNN输入图像最小尺寸为600，两个SSD模型除了输入图像尺寸（300*300和500*500），其他设置与其相同。很明显，较大的输入尺寸得到更好的结果,并且使用更多的数据是有帮助的。</p><h2 id="3-2-模型分析"><a href="#3-2-模型分析" class="headerlink" title="3.2 模型分析"></a>3.2 模型分析</h2><p>为了更好地理解SSD，我们还进行了几组对照实验，以检查每个组件如何影响最终性能。对于所有实验，我们使用完全相同的设置和输入大小（300×300），除了变动的组件。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1dy8uquj30qu08i3zq.jpg" srcset="/img/loading.gif" alt="image-20200820094954856" style="zoom:50%;" /></center><center>表2： 不同选择和组件对SSD表现的影响</center><p><strong>关键的数据增强。</strong>Fast和Faster R-CNN使用原始图像和水平翻转（0.5概率）图像训练。我们使用更广泛的采样策略，类似于YOLO [5]，但它使用了我们没有使用的光度失真。表2显示，我们可以用这个抽样策略提高6.7％（8.8%）的mAP。我们不知道我们的采样策略将对Fast和Faster R-CNN提升多少，但可能效果不大，他们在分类过程中使用了一个特征池化（feature pooling）步骤，这对设计中的对象翻转来说相对鲁棒。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1hqynf6j31400qetdv.jpg" srcset="/img/loading.gif" alt="image-20200820095334557"></p><p><strong>图3：VOC2007测试集上SSD 500（512）对动物、车辆和家具性能的可视化。</strong> 第一行显示由于定位不佳（Loc）或者与相似类别（Sim）、其他类别（Oth）或背景（BG）混淆的正确（Cor）或假阳性检测的累积分数。 红色实线反映了随着检测次数的增加，“强”标准（0.5 jaccard重叠）的召回率变化。 红色虚线使用“弱”标准（0.1 jaccard重叠）。底行显示排名靠前的假阳性类型的分布。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1jk3cbaj320u0qk7f5.jpg" srcset="/img/loading.gif" alt="image-20200820095518623"></p><p><strong>图4：不同目标特性对VOC2007测试集的灵敏度和影响[21]。</strong>每个红点显示标准错误栏的标准化的AP（平均准确率）。 黑色虚线表示整体归一化的AP。左边的图显示了BBox（边框bounding box）面积对每个类别的影响，右边的图显示了纵横比的影响。Key: BBox Area: XS=extra-small; S=small; M=medium; L=large; XL =extra-large. Aspect Ratio: XT=extra-tall/narrow; T=tall; M=medium; W=wide; XW =extra-wide.</p><p><strong>更多特征图的提升</strong>  受许多语义分割工作启发[10,11,12]，我们也使用底层特征图来预测边界框输出。我们比较使用conv4_3预测的模型和没有它的模型。从表2，我们可以看出，通过添加conv4_3进行预测，它有明显更好的结果（72.1％ vs 68.1％）。这也符合我们的直觉，conv4_3可以捕获对象更好的细粒度，特别是细小的细节。</p><p><strong>更多的默认框形状效果更好</strong> 如第2.2节所述，默认情况下，每个位置使用6个默认框。如果我们删除具有1/3和3宽高比的框，性能下降0.9％（0.6%）。通过进一步移除1/2和2纵横比的框，性能再下降2％。使用多种默认框形状似乎使网络预测任务更容易。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1l51yiej316o0e80vd.jpg" srcset="/img/loading.gif" alt="image-20200820095649761" style="zoom: 33%;" /></center><p><strong>多个不同分辨率的输出图层更好。SSD的主要贡献是在不同的输出层使用不同尺度的默认框。</strong>为了衡量所获得的优势，我们逐步删除图层并比较结果。为了公平比较，每次我们删除一个图层，我们调整默认框平铺，以保持类似于原始默认框的总数（8732）（删减层后调节每一层的框的个数以保持框的总数不变）。这是通过在剩余层上堆叠更多尺度的框并根据需要调整框的尺度来完成的。我们没有详尽地优化每个设置的平铺。 表3显示随着层数减少精度随之下降，从74.3单调地下降到62.4。当我们在一个图层上堆叠多个尺度的框时，很多框都会在图像边界上，并且需要小心处理。我们尝试了Faster R-CNN [2]中使用的策略，忽略了边界上的方框。 我们观察一些有趣的趋势。例如，如果我们使用非常粗糙的特征映射（如conv11_2(1x1)或者conv10_2(3x3)），会大大地损害性能。原因可能是裁剪后我们没有足够大的框来覆盖大的目标。当我们主要使用更高分辨率的图时，性能开始再次上升，因为即使裁剪之后仍然剩余足够数量的大框。如果我们只使用conv7进行预测，那么性能是最糟糕的，这就强化了在不同层上传播不同尺度的框的至关重要的信息。此外，由于我们的预测不像[6]那样依赖于ROI池化，所以我们在低分辨率特征映射中没有<em>collapsing bins</em>的问题[23]。SSD架构将来自各种分辨率的特征图的预测结合起来，以达到与Faster R-CNN相当的精确度，同时使用较低分辨率的输入图像。</p><h2 id="3-3-PASCAL-VOC2012"><a href="#3-3-PASCAL-VOC2012" class="headerlink" title="3.3 PASCAL VOC2012"></a>3.3 PASCAL VOC2012</h2><p>我们采用和VOC2007实验一样的设置，这次，用VOC2012  trainval、VOC2007 trainval和test（21503张图像）三个数据集做训练，在VOC2012 test（10991张图像）测试。由于有了更多的训练数据，模型训练时以0.001学习率进行60K次迭代，再减小到0.0001继续迭代20K次。表3<strong>（表4）</strong>显示了SSD300和SSD500<strong>（SSD512^4）</strong>模型的结果。我们看到与我们在VOC2007测试中观察到的相同的性能趋势。我们的SSD300已经优于Fast R-CNN，并且非常接近Faster R-CNN（只有0.1％的差异）（我们的SSD300提高了精度比Fast/Faster R-CNN）。通过将训练和测试图像大小增加到500×500<strong>（512x512）</strong>，我们的精度比Faster R-CNN高2.7％<strong>（4.5%）</strong>。与YOLO相比，SSD显著精确，可能是由于使用来自多特征图的卷积默认框和训练期间的匹配策略。当我们在使用COCO模型微调训练时，我们的SSD512实现了80.0%的mAP，这比Faster R-CNN高了4.1%。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1nx5crnj31ao094jvc.jpg" srcset="/img/loading.gif" alt="image-20200820095929845"></p><p><strong>表4：PASCAL VOC2012 test 检测结果。</strong>Fast R-CNN和Faster R-CNN使用最小尺寸为600的图像，而YOLO的图像尺寸是448x448。数据：“07++12”表示“VOC2007 trainval、VOC2007 test和VOC2012 trainval三个数据集的组合”；“07++12+COCO”表示“首先使用COCO trainval35K训练模型然后利用该模型微调训练07++12”。</p><h2 id="3-4-COCO"><a href="#3-4-COCO" class="headerlink" title="3.4 COCO"></a>3.4 COCO</h2><p>为了进一步验证SSD架构，我们在MS COCO<strong>（COCO）</strong>数据集上训练了我们的SSD300和SSD500<strong>（SSD512）</strong>架构。由于COCO中的对象往往比PASCAL VOC数据集更小，因此我们对所有层使用较小的默认框。我们遵循第2.2节中提到的策略方法，但是现在我们最小的默认框具有0.1而不是0.2的缩放比例，并且conv4_3上默认框的缩放比例是0.07（但现在我们最小的默认框的尺度是0.15而不是0.2，conv4 3的默认框的尺度是0.07）（例如，对应于300×300图像的21个像素）^5。</p><p>​    我们使用trainval35k [21]来训练我们的模型。由于COCO有更多的对象类别，开始时的梯度不稳定。我们首先用8× 的学习率迭代4K次训练模型，接着以 学习率进行140K次迭代，再以 学习率迭代60K次， 学习率迭代40K次。表4显示了test-dev2015上的结果。与我们在PASCAL VOC数据集上观察到的类似，SSD300在mAP@0.5和mAP@[0.5：0.95]中优于Fast R-CNN，在 mAP@[0.5：0.95]<strong>（mAP@0.75）</strong>与<strong>ION[24]和</strong>Faster R-CNN近似然而，但在mAP@0.5上更糟。我们推测，这是因为图像尺寸太小，这阻止了模型精确定位许多小对象。通过将图像大小增加到500×500<strong>（512x512）</strong>，我们的SSD500<strong>（SSD512）</strong>在两个标准中都优于Faster R-CNN。有趣的是，我们观察到SSD512的mAP@0.75高了5.3%，但mAP@0.5仅仅高了1.2%。我们也观察到它的AP（平均准确率）和AR（平均召回率）分别提高了4.8%和4.6%对于大目标，但对于相对应的小目标只有更小的AP（1.3%）和AR（2.0%）。此外，我们的SSD500模型也比ION[21]更好，它是一个多尺寸版本的Fast R-CNN，使用循环网络显式模拟上下文。在图5中，我们展示了使用SSD500<strong>（SSD512）</strong>模型在MSCOCO test-dev的一些检测示例。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1phllctj31am0f6jw6.jpg" srcset="/img/loading.gif" alt="image-20200820100100798"></p><blockquote><p>这里博主对此表中一些含义做些自己的理解：1、第三列Avg.Precision.IoU可以理解为感兴趣区域，即判断正负样本。默认框与标准框做匹配，0.5表示重合超过50%的判断为正样本，否则为负样本，0.75也可以做此理解。至于0.5:0.95需要联系SSD代码，我的理解有两种，一种可能是每次匹配都会在50%至95%之间产生一个随机数去判断正负；第二种可能是重合度在50%至95%之间的为正样本，超过95%的排除，不足50%为负样本。对此我更倾向于第一种解释方法。2、第四列Avg.Precision，Area为目标图片的区域面积的大小，相关的评判标准论文并没有说明，S即small代表小的、M即middle代表中间的范围、L即large代表大的。3、第五列Avg.Recall,#Dets为平均召回率，1、10及100可能的意义为评判的基数。4、第六列Avg.Recall,Area类似于第四列，只是为平均召回率。</p></blockquote><h2 id="3-5-ILSVRC初步结果"><a href="#3-5-ILSVRC初步结果" class="headerlink" title="3.5 ILSVRC初步结果"></a>3.5 <strong>ILSVRC初步结果</strong></h2><p> 我们将我们用于COCO的相同的网络架构应用于ILSVRC DET数据集[15]<strong>[16]</strong>。我们使用ILSVRC2014 DET train和val1来训练SSD300模型，如[20]<strong>[22]</strong>中所使用。我们首先以8×0.001 的学习率迭代4K次训练模型，再用0.001学习率进行320k次迭代训练该模型，然后用0.0001进行100k次迭代和 0.00001继续训练60k次迭代（我们首先使用0.001的学习率训练迭代320K次，然后用0.0001的学习率继续训练迭代80K次，最后用0.00001的学习率再迭代40K次）。我们可以在val2集上实现41.1mAP[20]<strong>[22]</strong>。再一次的，它验证SSD是高质量实时检测的一般框架。</p><h2 id="3-6-用于小对象精度的数据增强"><a href="#3-6-用于小对象精度的数据增强" class="headerlink" title="3.6 用于小对象精度的数据增强"></a>3.6 用于小对象精度的数据增强</h2><p>没有如Faster R-CNN中的后续的特征重采样步骤，小目标的分类任务对于SSD来说相对困难，正如我们的分析（见图4）所示。第2.2节中描述的数据增强策略有助于显着提高性能，特别是在PASCAL VOC等小数据集上。策略产生的随机产物可以被认为是“放大（zoom in）”操作，并且可以产生许多更大的训练样例。为了实现可以创建更多小型训练示例的“缩小（zoom out）”操作，我们首先将图像随机放置在16x 的铺面上。 在我们做任何随机产物操作之前，原始图像大小的平均值被填充。因为通过引入这个新的“扩展”数据增强策略，我们有更多的训练图像，所以我们必须加倍训练迭代。如表6所示，我们看到多个数据集的mAP持续上升2％-3％。具体来说，图6显示新的增强策略显着提高了小对象的性能。这个结果强调了数据增强策略对最终模型精度的重要性。</p><p>   另一种改进SSD的方法是对默认框设计一个更好的平铺，使得每个位置和比例尺度能够与特征图的每个位置的感受野相匹配对齐。 我们将这个留给未来的工作。 </p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1rjorcmj30u00xphdx.jpg" srcset="/img/loading.gif" alt="image-20200820100257993"></p><p><strong>图5：使用SSD512模型对COCO test-dev的检测样例。</strong>我们显示了评分高于0.6的检测框，每种颜色对应一种目标类别。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1s65vvbj319e0b0q5q.jpg" srcset="/img/loading.gif" alt="image-20200820100334498"></p><p><strong>表6：当我们添加图片扩展数据增强策略时在多数据集上的结果。</strong>SSD300*和SSD512*是使用新的增强的数据训练出的模型。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1sqn9o6j31b00gsq9e.jpg" srcset="/img/loading.gif" alt="image-20200820100408405"></p><p><strong>图6：新的数据增强的目标尺寸对VOC2007 test的敏感和影响。</strong>顶行显示了对于原始SSD300和SSD512的每类的BBOX Area（边框面积）的影响，而底行是对应的使用数据增强策略训练的SSD300<em>和SSD512</em>模型。明显可以观察到新的数据增强策略可以显著帮助检测小目标。</p><h2 id="3-7-推论"><a href="#3-7-推论" class="headerlink" title="3.7 推论"></a>3.7 推论</h2><p>考虑到从我们的方法普遍生成大量的框，有必要在推理期间有效地执行非最大抑制（nms）。通过使用0.01的置信度阈值，我们可以过滤掉大多数框。然后，我们使用Thrust CUDA库进行排序，使用GPU计算所有剩余框之间的重叠，对jaccard重叠为0.45的每个类应用nms，并保留每个图像的前200个检测。对于SSD300和20个VOC类别，每个图像该步骤花费大约2.2<strong>（1.7）</strong>毫秒，这接近在所有新添加的层上花费的总时间<strong>（2.4毫秒）</strong>。我们使用Titan X和cuDNN v4以Intel Xeon E5-2667v3@3.20GHz测量批量大小为8的速度。</p><p>   表显示了SSD、Faster R-CNN[2]和YOLO [5]之间的比较。Faster R-CNN对region proposal（候选区域）使用额外的预测层，并且需要特征下采样。相比之下，我们的SSD500<strong>（SSD300和SSD512）</strong>方法在速度和精度上均优于Faster R-CNN。众所周知的是，SSD300是首创的实时实现70％以上mAP的方法。虽然Fast YOLO[5]可以运行在155帧每秒，但精度只有差不多20％<strong>（22%）</strong>的mAP。请注意，大约80％的前向时间花费在基础网络上（本例中为VGG16）。因此，使用更快的基站网络可以进一步提高速度，这也可能使SSD512模型成为实时的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghx1tx6ngvj31580caq60.jpg" srcset="/img/loading.gif" alt="image-20200820100516136"></p><h1 id="4-相关工作"><a href="#4-相关工作" class="headerlink" title="4. 相关工作"></a>4. 相关工作</h1><p>目前有两种已建立的用于图像中对象检测的方法，一种基于滑动窗口，另一种基于region proposal（候选区域）分类。在卷积神经网络出现之前，用于检测的两种方法DeformablePart Model（DPM）[22]和选择性搜索[1]性能接近（在卷积神经网络出现之前，检测方面最先进的两种方式——DeformablePart Model（DPM）[22]和选择性搜索[1]——有相似的性能）。然而，在R-CNN[20]带来的显着改进之后，其结合了选择性搜索region proposal和基于后分类的卷机网络，region proposal目标检测方法变得普遍。</p><p>原始的R-CNN方法已经以各种方式进行了改进。第一组方法提高了后分类的质量和速度，因为它需要对成千上万的图像作物进行分类，这是昂贵和耗时的。SPPnet[9]对原始的R-CNN方法大大提速。它引入了空间金字塔池化层，其对区域大小和比例尺度更加鲁棒，并且允许分类层重新使用在若干图像分辨率下生成的特征图特征。Fast R-CNN[6]扩展了SPPnet，使得它可以通过最小化置信度和边界框回归的损失来对所有层进行端到端微调，这在MultiBox[7]中首次引入用于学习对象。</p><p>第二组方法使用深层神经网络提高proposal（候选框）生成的质量。在最近的工作中，例如MultiBox[7,8]，基于低水平图像特征的选择性搜索region proposal被直接从单独的深层神经网络生成的proposal所替代。这进一步提高了检测精度，但是导致了一些复杂的设置，需要训练两个相互依赖的神经网络。Faster R-CNN[2]通过从region proposal网络（RPN）中学习的方案替换了选择性搜索proposal，并且引入了通过微调共享两个网络的卷积层和预测层之间来交替集成RPN与Fast R-CNN的方法。用这种region proposa方法池化中等水平的特征图，最终分类步骤更简便。我们的SSD与Faster R-CNN中的region proposal网络（RPN）非常相似，因为我们还使用固定的（默认）框来进行预测，类似于RPN中的achor框。但是，不是使用这些来池化特征和评估另一个分类器，我们同时在每个框中为每个对象类别同时产生一个分数。因此，我们的方法避免了将RPN与Fast R-CNN合并的复杂性，并且更容易训练，更快速，更易于集成到其他任务中。</p><p>另一组方法与我们的方法直接相关，干脆跳过proposal步骤，直接预测多个类别的边界框和置信度。OverFeat[4]是滑动窗口方法的深度版本，在知道基础对象类别的置信度之后直接从最顶层特征图的每个位置预测边界框。YOLO [5]使用整个最高层特征图来预测多个类别和边框（这些类别共享）的置信度。我们的SSD方法属于此类别，因为我们没有proposal步骤，但使用默认框。然而，我们的方法比现有方法更灵活，因为我们可以在不同尺度的多个特征图中的每个特征位置上使用不同宽高比的默认框。如果顶层特征图每个位置只使用一个默认框，我们的SSD将具有与OverFeat[4]类似的架构;如果我们使用整个顶层特征图并且添加一个全连接层用于预测而不是我们的卷积预测器，并且没有明确考虑多个宽高比，我们可以近似地再现YOLO[5]。</p><h1 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h1><p>本文介绍了SSD，一种用于多个类别的快速单次目标检测器。我们的模型的一个关键特点是使用附属于网络顶部的多特征图的多尺度卷积边框输出。这种表示允许我们有效地模拟可能的框形状空间。我们实验验证，给定适当的训练策略，更大量的仔细选择的默认边框得到了性能的提高。我们建立的SSD模型比现有的方法至少要多一个数量级的框预测采样位置，比例和纵横比[2,5,7]<strong>[5,7]</strong>。我们证明，给定相同的VGG-16基础架构，SSD在精度和速度方面胜过最先进的对象检测器。我们的SSD500<strong>（SSD512）</strong>型号在PASCAL VOC和MS COCO的精度方面明显优于最先进的Faster R-CNN [2]，速度快了3倍。 我们的实时SSD300模型运行在58 FPS，这比当前的实时YOLO[5]更快，同时产生了明显更优越的检测精度。</p><p>除了它的独立的效用，我们相信，我们的完整和相对简单的SSD模型为使用目标检测组件的大型系统提供了一个有用的组成块。一个有希望的未来方向，是探索其作为使用循环神经网络的系统一部分，用以检测和跟踪视频中对象。</p><h1 id="6-致谢"><a href="#6-致谢" class="headerlink" title="6. 致谢"></a>6. 致谢</h1><p>这个项目是在谷歌开始的实习项目，并在UNC继续。 我们要感谢亚历克斯·托舍夫有用的讨论，并感谢谷歌的Image Understanding和DistBelief团队。 我们也感谢菲利普·阿米拉托和帕特里克·波尔森有益的意见。我们感谢NVIDIA提供K40 GPU并感谢NSF 1452851，1446631，1526367，133771的支持。</p><blockquote><p>本文转自：<a href="https://blog.csdn.net/wfei101/article/details/79888594" target="_blank" rel="noopener">https://blog.csdn.net/wfei101/article/details/79888594</a></p><p>原作者：BigCowPeking</p></blockquote><h1 id="附：原文"><a href="#附：原文" class="headerlink" title="附：原文"></a>附：原文</h1><div class="row">    <embed src="/pdf/SSD.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SSD</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——Image Segmentation Techniques Overview</title>
    <link href="/2020/08/13/ImageSegmentationTechniquesOverview/"/>
    <url>/2020/08/13/ImageSegmentationTechniquesOverview/</url>
    
    <content type="html"><![CDATA[<p>Song Y, Yan H. Image Segmentation Techniques Overview[C]//2017 Asia Modelling Symposium (AMS). IEEE, 2017: 103-107.</p><h1 id="图像分割技术综述"><a href="#图像分割技术综述" class="headerlink" title="图像分割技术综述"></a>图像分割技术综述</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>图像分割技术被广泛应用于医学图像处理、人脸识别、行人检测等领域。目前的图像分割技术包括基于区域的分割，边缘检测分割，基于聚类的分割，基于弱监督学习的CNN分割，等等。本文分析并总结了这些图像分割算法，并比较了不同算法的优缺点。最后，结合这些算法对图像分割的发展趋势进行了预测。</p><p>关键词：图像分割；基于区域；边缘检测；聚类；弱监督学习；CNN</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>图像是一种传递信息的方式，并且图像包含许多有用的信息。理解图像并从图像中提取信息以完成某些工作是数字图像技术中的重要应用领域，理解图像的第一步是图像分割。实际上，通常不是对图像的所有部分都感兴趣，而仅对某些具有相同特征的区域感兴趣。图像分割是图像处理和计算机视觉的热点之一。这也是图像识别的重要基础。它基于某些标准将输入图像划分为一些相同性质的类别，以提取人们感兴趣的区域。这是图像分析以及理解图像特征提取和识别的基础。</p><p>有许多常用的图像分割算法。本文主要介绍以下五种算法，以简化分析。第一种是<strong>阈值分割方法</strong>。阈值分割是基于区域的分割算法中最常用的分割技术之一[1]。其本质是根据特定标准自动确定最佳阈值，并根据灰度使用这些像素以实现聚类。第二种是<strong>区域增长分割法</strong>。区域增长算法的基本思想是将具有相似属性的像素合并以形成区域，也就是说，对于每个要划分的区域，首先找到一个种子像素作为生长点，然后用该区域中具有相似属性的像素点合并周围邻域。第三种是<strong>边缘检测分割方法</strong>。边缘检测分割算法是指利用像素的不同区域边缘的的像素灰度或颜色不连续性检测区域来实现图像分割[2]。第四种是<strong>基于聚类的分割</strong>。基于聚类的算法以事物之间的相似度为分类准则，即根据样本集的内部结构将其划分为多个子类，以使同类样本尽可能相似，不同的样本尽可能不相似[3]。第五种是<strong>基于弱监督学习的CNN分割方法</strong>。它涉及为图像中的每个像素分配语义标签的问题，由三个部分组成。1）提供包含哪些对象的图像。2）给出对象的边界。3）图像中的目标区域标记有部分像素[4]。</p><p>目前，从国际上的图像分割方法来看，分割方法过程的具体操作是非常多样和复杂的，还没有公认的统一标准。本文讨论并比较了上述四种方法，并从缺点中学习以分析更好的解决方案并做出未来的预测。</p><h1 id="2-分析"><a href="#2-分析" class="headerlink" title="2.分析"></a>2.分析</h1><h2 id="2-1-基于区域的分割方法"><a href="#2-1-基于区域的分割方法" class="headerlink" title="2.1 基于区域的分割方法"></a>2.1 基于区域的分割方法</h2><h3 id="2-1-1-阈值分割"><a href="#2-1-1-阈值分割" class="headerlink" title="2.1.1 阈值分割"></a>2.1.1 阈值分割</h3><p>阈值分割是最简单的图像分割方法，也是最常见的并行分割方法之一。它是一种常见的分割算法，它根据不同目标的灰度值直接划分图像灰度信息处理。阈值分割可以分为局部阈值法和全局阈值法。全局阈值方法通过单个阈值将图像分为目标和背景两个区域[5]。局部阈值方法需要选择多个分割阈值，并通过多个阈值将图像分为多个目标区域和背景。</p><p>最常用的阈值分割算法是最大的类间差异方法（Otsu），该方法通过最大化类之间的差异来选择全局最佳阈值。除此之外，还有基于熵的阈值分割方法，最小误差方法，共现矩阵方法，矩保持方法，简单统计方法，概率松弛方法，模糊集方法和阈值方法与其他方法相结合[6]。</p><p>阈值法的优点是计算简单，运算速度更快。特别地，当目标和背景具有高对比度时，可以获得好的分割效果。缺点是，对于在图像中没有明显的灰度差异或灰度值有较大重叠的图像分割问题，很难获得准确的结果。由于它仅考虑图像的灰度信息而不考虑图像的空间信息，因此对噪声和灰度不均匀敏感，因此经常将其与其他方法结合使用[7]。</p><h3 id="2-1-2-区域增长分割"><a href="#2-1-2-区域增长分割" class="headerlink" title="2.1.2 区域增长分割"></a>2.1.2 区域增长分割</h3><p>区域增长方法是一种典型的串行区域分割算法，其基本思想是将像素的相似属性共同形成一个区域[8]。该方法需要首先选择种子像素，然后将种子像素周围的相似像素合并到种子像素所在的区域。</p><p>图1示出了用于区域生长的已知种子点的示例。图1（a）显示了分割图像的需求。已知两个种子像素（标记为灰色方块）已准备好进行区域生长。这里使用的标准是，如果认为像素和种子像素之间的灰度值差的绝对值小于某个阈值T，则该像素被包括在种子像素所在的区域中。图1（b）显示了T = 3时的区域增长结果，整个区域被很好地分为两个区域。图1（c）显示了T = 6时区域增长的结果，整个图都在一个区域中。因此，阈值的选择非常重要[9]。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghpjxbz83qj30no09qq4b.jpg" srcset="/img/loading.gif" alt="image-20200813222646201" style="zoom:50%;" /></center><p>区域增长的优点是通常可以将具有相同特征的连接区域分开，并提供良好的边界信息和分割结果。区域增长的想法很简单，只需几个种子点即可完成。并且可以自由指定生长过程中的生长标准。最后，它可以同时选择多个标准。缺点是计算量大[10]。噪声和灰度不均匀也会导致空隙和过度分割。最后是阴影对图像的影响往往不是很好[11]。</p><h2 id="2-2-边缘检测分割"><a href="#2-2-边缘检测分割" class="headerlink" title="2.2 边缘检测分割"></a>2.2 边缘检测分割</h2><p>对象的边缘呈图像的不连续局部特征的形式，即图像的最重要部分局部亮度发生变化，例如灰度值的突变，颜色的突变，纹理的变化等[12]。使用不连续性是为了检测边缘，从而达到图像分割的目的。</p><p>在图像中具有不同灰度值的两个相邻区域之间总是存在灰度边缘，并且存在灰度值不连续的情况。通常可以使用导数运算来检测这种不连续性，并且可以使用微分运算符来计算导数[13]。并行边缘检测通常通过空间域差分算子完成，以通过对模板和图像进行卷积来执行图像分割。平行边缘检测通常被用作图像预处理的方法。广泛的一阶微分算子是Prewitt算子，Roberts算子和Sobel算子[14]。二阶微分算子具有非线性算子，例如Laplacian，Kirsch算子和Wallis算子。</p><h3 id="2-2-1-Sobel算子"><a href="#2-2-1-Sobel算子" class="headerlink" title="2.2.1 Sobel算子"></a>2.2.1 Sobel算子</h3><p>Sobel运算符主要用于边缘检测，从技术上讲，它是离散微分运算符，用于计算图像亮度函数的梯度近似值。Sobel算子是基于一阶导数的典型边缘检测算子。由于Sobel算子采用了相似局部平均操作，<strong>因此噪声具有平滑的效果，并可以有效消除噪声的影响。</strong>Sobel算子对像素位置的影响是加权的，这比Prewitt算子和Roberts算子好。Sobel算子由两组3x3矩阵组成，分别是横向和纵向模板，并分别用像平面绘制，以获取水平和纵向差异之间的差异。在实际使用中，以下两个模板用于检测图像的边缘。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqfl7nw9qj30lk09smxl.jpg" srcset="/img/loading.gif" alt="image-20200814164217587" style="zoom:33%;" /></center><p>等式（1）是检测水平边缘（横向模板），等式（2）是检测垂直边缘（纵向模板）。</p><p>可以使用以下公式来组合图像每个像素的水平和垂直梯度近似值，以计算梯度的大小：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqfnonf0hj30js03maa4.jpg" srcset="/img/loading.gif" alt="image-20200814164442799" style="zoom: 50%;" /></center><p>可以使用以下公式计算梯度：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqfona923j30jo03ejrh.jpg" srcset="/img/loading.gif" alt="image-20200814164538315" style="zoom: 50%;" /></center><p>在上述示例中，如果上述角度等于0，即图像具有纵向边缘，且左侧比右侧更暗。</p><h3 id="2-2-2-Laplacian-算子"><a href="#2-2-2-Laplacian-算子" class="headerlink" title="2.2.2 Laplacian 算子"></a>2.2.2 Laplacian 算子</h3><p>拉普拉斯算子是各向同性算子，二阶微分算子。当只考虑边缘的位置而不考虑其周围的像素灰度差异时[15]，这种方法更为合适。<strong>拉普拉斯算子对孤立像素的响应强于边缘或线条，因此仅适用于无噪点图像。</strong>在存在噪声的情况下，拉普拉斯算子需要在检测边缘之前执行低通滤波。因此，通常的分割算法将拉普拉斯算子与平滑算子结合在一起以生成新模板。</p><p>拉普拉斯算子也是具有旋转不变性的最简单的各向同性微分算子。二维图像函数的拉普拉斯变换是各向同性的二阶导数，它更适合于数字图像处理，并且拉算子表示为离散形式：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqgndjr5aj30kk04i74i.jpg" srcset="/img/loading.gif" alt="image-20200814171900904" style="zoom:50%;" /></center><p>另外，拉普拉斯算子也可以以模板的形式表示。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqgotjzvqj30jc09i3yv.jpg" srcset="/img/loading.gif" alt="image-20200814172024355" style="zoom:50%;" /></center><p>公式（6）是离散的拉普拉斯大蒜模板，公式（7）是扩展的模板</p><p>由于Laplacian算子符合下降模型，因此可用于由于模糊效果而改善模糊效果。在成像过程中经常发生扩散效应。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqh0kf3fwj31za0jy7wh.jpg" srcset="/img/loading.gif" alt="image-20200814173141177"></p><h2 id="2-3-基于聚类的分割"><a href="#2-3-基于聚类的分割" class="headerlink" title="2.3 基于聚类的分割"></a>2.3 基于聚类的分割</h2><p>没有图像分割的一般理论。然而，随着各种学科的许多新理论和方法的引入，已经出现了许多结合了一些特定理论和方法的图像分割方法。所谓的类，是指相似元素的集合。聚类是按照一定的要求和规律对事物进行分类的过程。利用特征空间聚类方法对图像空间中的像素点进行分割，得到相应的特征空间点。根据它们在特征空间中的聚集，对特征空间进行分割，然后将它们映射回原始图像空间，得到分割结果。</p><p>K-means是最常用的聚类算法之一。K-means的基本思想是根据距离将样本集合到不同的簇中。两点越接近，越能得到紧凑独立的簇作为聚类目标[16]。K-means的实现过程表示为:</p><ol><li>随机选择K个初始聚类中心;</li><li>计算每个样本到每个聚类中心的距离，将每个样本返回到最近的聚类中心;</li><li>对于每个聚类，以所有样本的均值作为新聚类中心;</li><li>重复步骤2和3，直到聚类中心不再变化或达到设定的迭代次数[17]。</li></ol><p>K-Means聚类算法的优点是算法快速、简单、高效、可扩展，适用于大数据集。其时间复杂度接近线性，适合挖掘大规模数据集。K-means的缺点是其聚类数K没有明确的选择标准，且难以估计[18]。其次，从K-means算法框架可以看出，算法的每次迭代都要遍历所有的样本，所以算法的时间非常昂贵。最后，K-means算法是一种基于距离的划分方法[19]。它只适用于凸数据集，不适合聚类非凸类.</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqhu1zmyxj30u00v3kib.jpg" srcset="/img/loading.gif" alt="image-20200814180001744"></p><h2 id="2-4-基于弱监督学习的CNN分割方法"><a href="#2-4-基于弱监督学习的CNN分割方法" class="headerlink" title="2.4 基于弱监督学习的CNN分割方法"></a>2.4 基于弱监督学习的CNN分割方法</h2><p>近年来，深度学习已经在图像分类、检测、分割、高分辨率图像生成等许多领域取得了突破性的成果[20]。在图像分割方面，提出了一种在该领域更为有效的算法，即基于DCNN的弱监督半监督学习的图像语义分割算法。谷歌的George Papandreou和UCLA的Liang-Chieh Chen在DeepLab的基础上研究了使用边界框和图像级标签作为标记训练数据，并使用<strong>期望最大化算法(EM)</strong>估计未标记的像素类和CNN参数。DeepLab方法分为两个步骤，第一步仍然使用FCN获取粗略得分图并插值到原始图像大小，然后第二步开始从FCN借用完全连接的CRF以获得细节细分优化[21]。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqilitmi1j30qk0guqb5.jpg" srcset="/img/loading.gif" alt="image-20200814182625369" style="zoom: 50%;" /></center><p>对于图像级标记数据，我们可以观察到图像的像素值x和图像级的标记z，但不知道每个像素的标签y，因此y被视为隐藏变量。使用以下概率图模式：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqin7zjegj30ri04ygm3.jpg" srcset="/img/loading.gif" alt="image-20200814182803334" style="zoom:50%;" /></center><p>使用期望最大化算法估算ø和y。E步是固定的ø，期望值y，M步是固定的y，使用SGD计算ø[22]。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghqirg7oilj30ok0h0qc3.jpg" srcset="/img/loading.gif" alt="image-20200814183207655" style="zoom:50%;" /></center><p>对于具有边界框标记的训练图像，该方法使用CRF自动分割训练图像，然后在分割的基础上进行完全监督。实验表明，单纯使用图像级标记得到的分割效果较差，但使用边界框训练数据[23]可以得到较好的分割结果。</p><h1 id="3-结论"><a href="#3-结论" class="headerlink" title="3.结论"></a>3.结论</h1><p>从本文中可以看出，很难找到一种适合所有图像的分割方法。目前，图像分割理论的研究还不完善，在应用研究中还存在许多实际问题。通过比较各种图像分割算法的优缺点，图像分割技术的发展可能呈现如下趋势：1)多种分割方法的结合。由于图像的多样性和不确定性，有必要在多特征融合的基础上，结合多种分割方法，充分利用不同算法的优点，以达到更好的分割效果。2)在参数选择上使用机器学习算法进行分析，以提高分割效果。如阈值分割中的阈值选择以及K-means算法中K值的选择。3）使用CNN模型对感兴趣区域（ROI）进行框架化，然后通过非机器学习分割方法进行分割以提高分割效果。相信在未来的研究和探索中，将会有更多的图像分割方法得到进一步发展和更广泛的应用。</p><h1 id="附：原文"><a href="#附：原文" class="headerlink" title="附：原文"></a>附：原文</h1><div class="row">    <embed src="./1.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>ImageSegmentation</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>老君山航拍</title>
    <link href="/2020/08/12/ljs/"/>
    <url>/2020/08/12/ljs/</url>
    
    <content type="html"><![CDATA[<p>航拍是我的爱好之一，美好的景色从天空中俯瞰，总是会有不一样的感觉。8月6日与好友相约登顶位于洛阳市栾川县的老君山，其为八百里伏牛山脉的主峰，海拔2217米。位于山顶的金顶道观群在云雾中若隐若现，仙境十足！这也是我第一次驾驶无人机进行穿云拍摄，以下为最终的航拍作品，请您欣赏！</p><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=839241049&bvid=BV1P54y1U7Wq&cid=221714752&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" sandbox="allow-top-navigation allow-same-origin allow-forms allow-scripts"> </iframe></div>]]></content>
    
    
    <categories>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>R-CNN系列总结（8.7 阅读笔记）</title>
    <link href="/2020/08/07/8-7/"/>
    <url>/2020/08/07/8-7/</url>
    
    <content type="html"><![CDATA[<p>上一次的总结主要针对One-Stage的YOLO系列目标检测算法，本次主要学习了Two-Stage的R-CNN系列基于候选窗口的目标检测算法，包括：R-CNN[2]、Fast R-CNN[4]、Faster R-CNN[5]。</p><p>R-CNN系列算法由以下三个独立的步骤组成：产生候选窗口、特征提取、SVM分类及窗口回归。由于R-CNN和Fast R-CNN使用Selective Search方法来提取候选窗口，遂找来Selective Search的论文进行学习[1]。此外，Fast R-CNN提出了一个ROI池化层，其实质上是SPP-Net的一个特殊情况，故扩展阅读了SPP-Net的论文[3]。</p><p>代码方面：下载了YOLO的权重进行了运行测试，开始学习PyTorch。</p><p>阅读文献：</p><p>[1] Uijlings J R R, Van De Sande K E A, Gevers T, et al. Selective search for object recognition[J]. International journal of computer vision, 2013, 104(2): 154-171.</p><p>[2] Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587.</p><p>[3] He K, Zhang X, Ren S, et al. Spatial pyramid pooling in deep convolutional networks for visual recognition[J]. IEEE transactions on pattern analysis and machine intelligence, 2015, 37(9): 1904-1916.</p><p>[4] Girshick R. Fast r-cnn[C]//Proceedings of the IEEE international conference on computer vision. 2015: 1440-1448.</p><p>[5] Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.</p><h1 id="1-Selective-Search"><a href="#1-Selective-Search" class="headerlink" title="1.Selective Search"></a>1.Selective Search</h1><p>  传统的目标检测算法大多数以图像识别为基础。在图片上使用穷举法或者滑动窗口选出所有物体可能出现的区域框，对这些区域框提取特征并进行使用图像识别分类方法，得到所有分类成功的区域后，通过非极大值抑制输出结果。</p><p>穷举法或滑动窗口法就是在原始图片上使用不同尺度不同大小的滑窗，获取每个可能的位置。缺点是复杂度太高，产生了很多的冗余候选区域，而且由于不可能每个尺度都兼顾到，因此得到的目标位置也不准。选择性搜索有效地去除冗余候选区域，使得计算量大大的减小。</p><p>selective search首先通过基于图的图像分割方法初始化原始区域，将图像分割成很多的小块。然后使用贪心策略，计算每两个相邻的区域的相似度，然后每次合并最相似的两块，直到最终只剩下一块完整的图片。将其中每次产生的图像块包括合并的图像块都保存下来，这样就得到图像的分层表示了。</p><p><strong>Selective Search算法框架：</strong></p><ul><li>输入：彩色图片。</li><li>输出：物体可能的位置，实际上是很多的矩形坐标。</li><li>首先，将图片初始化为很多小区域R。初始化一个相似集合为空集：S</li><li>计算所有相邻区域之间的相似度（考虑了颜色、纹理、尺寸和空间交叠4个参数），放入集合 S 中，集合 S 保存的是一个区域对以及它们之间的相似度。</li><li>找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空。</li><li>从 R 中找出所有区域的 bounding box（即包围该区域的最小矩形框），这些 box 就是物体可能的区域。</li></ul><h1 id="2-R-CNN"><a href="#2-R-CNN" class="headerlink" title="2.R-CNN"></a>2.R-CNN</h1><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bpccrmj30v20ba0vf.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>R-CNN总体思路：</strong>  <strong>通过Selective Search检测出Region proposals ——&gt; 使用CNN提取Region特征——&gt;使用SVM对提取到的特征进行分类 ——&gt;执行非极大值抑制得到Bbox及其类别</strong></p><p>  由于卷积神经网络的输入为227*227大小的正方形图像，而Selective Search算法搜索到的是2000个大小不一的矩形候选框。因此需要对搜索到的矩形选框进行缩放处理，文章试验了两种不同的处理方法：</p><p>(1) <strong>各向异性缩放</strong>：即忽略原始图像的长宽比，直接将其扭曲缩放至227*227</p><p>(2) <strong>各向同性缩放：</strong>由于扭曲的图片会对后续CNN的训练精度产生影响，作者测试了两种各向同性缩放方案。</p><p>a)  直接在原始图片中将Bounding Box的边界延展为正方形，如果已经延伸到原始图片的边界，则使用Bounding Box中的颜色均值填充。</p><p>b)  先将Bounding Box的裁剪出来，使用其颜色均值将背景填充为正方形图片。</p><p>当缺乏大量的标注数据时，作者提出了一个比较好的可行手段，即进行神经网络的迁移学习。先在其他大型数据集预训练神经网络，然后在小规模特定的数据集中进行 fine-tune 微调。在使用Selective Search提取候选区域并进行Resize后，接下来使用CNN为每个候选区域提取特征，本文训练CNN的方法主要包括两步：</p><p>(1) <strong>预训练阶段。</strong>由于物体标签训练数据少，如果要直接采用随机初始化CNN参数的方法是不足以从零开始训练出一个好的CNN模型。基于此，本文采用的是有监督的预训练，使用一个大的数据集（ImageNet ILSVC 2012）来训练AlexNet，得到一个分类的预训练模型（需要预测1000个类别）。注意：在ImageNet上训练的是模型识别物体类型的能力，而不是预测bbox位置的的能力。</p><p>(2) <strong>Fine-tuning阶段</strong>：在PASCAL VOC上对预训练模型进行fine-tuning。将预训练阶段的CNN模型最后一层全连接层的1000个神经元，替换成21个神经元的分类层（20类物体+背景）（这一层采用随机初始化），然后计算每个候选区域和ground truth 的IoU，对于IoU&gt;0.5的候选区域被视为正样本，否则为负样本（即背景）。在每次迭代的过程中，batch size大小选择128，包括32个正样本和96个负样本（正负比：1：3）。使用0.001的学习率和随机梯度下降（SGD）来进行训练。</p><p>本文为每一个类训练一个SVM分类器，故共有21个分类器，每个SVM分类器的输入包括两个部分：</p><p>(1) CNN Feature：CNN网络为每个候选区域提取的特征，大小为2000*4096</p><p>(2) Ground Truth Labels:在训练时为每个候选区域附上一个label。</p><p>在SVM分类过程中，当IoU&lt;0.3时为负样本，其余为正样本。</p><p><strong>总结：</strong></p><ul><li>R-CNN采用AlexNet网络架构</li><li>R-CNN采用Selective Search生成候选区域</li><li>R-CNN先在ImageNet上进行预训练，然后利用成熟的权重参数在PASCAL VOC上进行fine-tune.</li><li>R-CNN使用CNN抽取特征，然后使用一系列的SVM做类别预测</li><li>R-CNN在Pascal VOC 2012 的数据集上，能够将目标检测的验证指标 mAP 提升到 53.3%,这相对于之前最好的结果提升了 30%。</li></ul><p><strong>R-CNN的缺点：</strong></p><ul><li>训练时间长：分阶段多次训练，而且对每个候选区域都要单独计算一次Feature map。</li><li>占用空间大：每个候选区域的Feature map都要写入磁盘保存。</li><li>步骤繁多：整个模型包括多个模块，且每个模块是相互独立的。</li><li>重复计算：多数候选区域是互相重叠的，重叠部分会被多次提取Feature。</li></ul><h1 id="3-SPP-Net"><a href="#3-SPP-Net" class="headerlink" title="3.SPP-Net"></a>3.SPP-Net</h1><p>CNN一般都含有卷积层和全连接层，其中卷积层无需固定尺寸的图像，而全连接层则需要固定大小的输入。</p><p>在R-CNN网络结构模型中，由于卷积神经网络的全连接层对于输入的图像尺寸有限制，所以所有候选区域的图像都必须经过变形转换后才能交由卷积神经网络模型进行特征提取，但是无论采用剪切(crop)还是采用变形(warp)的方式，都无法完整保留原始图像信息，何凯明等人提出的空间金字塔池化层(Spatial Pyramid Pooling Layer)有效地解决了传统卷积神经网络对输入图像的尺寸的限制。</p><p>SPP-Net在最后一个卷积层后，加入了一个空间金字塔池化层，用来保证传到下一层全连接层的输入固定。在普通的CNN机构中，输入图像的尺寸往往是固定的（比如224*224像素），输出则是一个固定维数的向量。SPP Net在普通的CNN结构中加入了ROI池化层（ROI Pooling），使得网络的输入图像可以是任意尺寸的，输出则不变，同样是一个固定维数的向量。ROI池化层一般跟在卷积层后面，此时网络的输入可以是任意尺度的，在SPP layer中每一个pooling的filter会根据输入调整大小，而SPP的输出则是固定维数的向量，然后给到全连接层。</p><p>此外，SPP-net也解决了R-CNN重复计算的问题。SPP-net的思路是由于原图与经过卷积层后得到的特征图在空间位置上存在一定的对应关系，所以<strong>只需对整张图像进行一次卷积层特征提取</strong>，<strong>然后将候选区域在原图的位置映射到卷积层的特征图上得到该候选区域的特征</strong>，最后将得到每个候选区域的卷积层特征输入到全连接层进行后续操作。R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。</p><p>SPP-net网络结构如下图所示，输入一副任意尺度的待测图像，用CNN可以提取得到<strong>卷积层特征</strong>(例如VGG16最后的卷积层为Conv5_3，得到256幅特征图)。然后<strong>将不同大小候选区域的坐标投影到特征图上得到对应的窗口</strong>(window)，将每个window均匀划分为4<em>4, 2</em>2, 1<em>1的块，然后对每个块使用Max-Pooling下采样，这样无论window大小如何，经过SPP层之后都得到了一个固定长度为(4</em>4+2<em>2+1)</em>256维的特征向量，将这个特征向量作为全连接层的输入进行后续操作。<strong>这样就能保证只对图像提取一次卷积层特征，同时全连接层的输入维度固定。</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3boaucnj30g70dftat.jpg" srcset="/img/loading.gif" alt="img"></p><p>SPP-Net整体流程：</p><p>(1) 输入一幅待检测图像</p><p>(2) 提取候选区域：利用Selective Search算法从输入图像中提取约2000个候选框。</p><p>(3) 候选区域尺度缩放：以候选区域长宽中的较短边进行统一。</p><p>(4) 特征提取：利用SPP-net网络结构提取特征。</p><p>(5) 分类与回归：根据提取的特征利用SVM进行分类，用边框回归器微调候选框的位置。</p><p><strong>总结：</strong></p><ul><li>SPP-Net使得任意大小的特征图都能够转换成固定大小的特征向量。</li><li>R-CNN要对每个候选区域计算卷积，而SPP-Net只需要对原图计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。</li><li>SPP-Net解决了R-CNN重复提取候选区域特征的问题，同时允许各种尺寸图像作为输入，解决了图像畸变的问题，但R-CNN的其它问题，如训练步骤繁琐、磁盘空间开销大等依然有待解决。</li></ul><h1 id="4-Fast-R-CNN"><a href="#4-Fast-R-CNN" class="headerlink" title="4. Fast R-CNN"></a>4. Fast R-CNN</h1><p>针对R-CNN存在的问题，Fast R-CNN逐一进行了解决：</p><p>(1) 训练、测试速度慢：R-CNN中用CNN对候选区域提取特征，而一张图片的2000个候选区域有大量重叠部分，重叠部分会被多次<strong>重复提取特征</strong>，造成大量的计算开销。Fast R-CNN将整个图像归一化后直接送入CNN网络，卷积层不进行候选区域的特征提取，而是在最后一个池化层中加入候选区域坐标信息，进行特征提取的计算。</p><p>(2) 训练所需空间大：R-CNN中目标分类与候选框回归是两个独立的操作，并且需要大量特征作为训练样本。Fast R-CNN将目标分类与候选框回归统一到CNN网络中，不需要将每个候选区域的Feature map写入磁盘保存。</p><p>Fast R-CNN的网络结构如下图所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bn2cavj30zm0ea4gc.jpg" srcset="/img/loading.gif" alt="img"></p><p>Fast R-CNN的输入由两部分组成：一是待处理的整张图像；二是候选区域(region proposal)。Fast R-CNN处理的第一步是对图像进行多次卷积核池化处理来获取<strong>卷积特征图</strong>。由于存在多个候选区域，系统会有一个甄别，判断出感兴趣区域，也就是Region of Interest, RoI。<strong>RoI池化层是SSP(Spatial Pyramid Pooling)层的特殊情况</strong>，它可以从特征图中提取一个固定长度的特征向量。每个特征向量，都会被输送到全连接层序列中，这个全连接层分支成两个同级输出层。<strong>其中一层的功能是进行分类</strong>，对目标关于K个对象类(包括全部“背景background”类)输出每一个RoI的概率分布，也就是产生softmax概率估计；<strong>另一层是为了输出</strong>K个对象中每一个类的四个实数值(bbox regression)。每4个值编码K个类中的<strong>每个类的精确边界框(bounding-box)位置</strong>。整个结构是使用多任务损失的端到端训练 (除去Region Proposal提取阶段)。</p><p>与R-CNN框架图对比，可以发现主要有两处不同：一是最后一个卷积层后加了一个<strong>ROI池化层</strong>，二是损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练</p><p>(1) ROI池化层实际上是SPP-NET的一个精简版，可以看成是SPP层的一个特殊情况。SPP-Net对每个候选区域使用了不同大小的金字塔映射，而ROI池化层只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有候选区域对应了一个7<em>7</em>512维度的特征向量作为全连接层的输入。ROI池化层与SPP-Net的效果一样，可以将不同大小的输入映射到一个固定尺度的特征向量。卷积、池化、relu等操作都不要求固定的size，在原始图片上进行这些操作后，因输入图片的size不同，就会导致得到的feature map的尺寸也不相同，不能直接连接到全连接层进行分类。因此加入ROI池化层后，对每个候选区域都提取出一个固定维度的特征表示，再通过softmax进行分类。</p><p>(2) Fast R-CNN使用softmax替代R-CNN的SVM进行分类，同时利用多任务损失函数将边框回归直接加入到CNN网络中训练，这样整个训练过程只包含提取候选区域和CNN训练两个阶段。整个训练过程是端到端的（除去提取候选区域阶段）。此前R-CNN的训练过程分为四步：提取候选区域、使用CNN提取特征、使用SVM分类器、回归Bbox。而在Fast R-CNN中，作者将最后一步回归Bbox放进了CNN内部，与Region分类合并为一个multi-task模型。实验也证明，这两个任务能共享卷积特征，并且可以相互促进。</p><p><strong>主要创新点：</strong></p><ul><li>借助多任务损失函数，将物体识别和位置修正合并到一个网络中，不再进行分步训练。</li><li>共享卷积层，不再像R-CNN一样每个候选框都使用CNN提取特征，而是输入一张完整的图片，在第五个卷积层再得到每个候选框的特征。</li><li>不需要在硬盘中存储训练过程中的特征数据。</li><li>用ROI层代替SPP层，可以使用BP算法更高效的训练更新整个网络。</li><li>相较于R-CNN和SPP-Net，检测结果的精确度更高</li><li>对全连接层提出了SVD奇异值分解的优化方法，降低了全连接层需要学习的参数数目，节省计算时间。</li></ul><p><strong>缺点：</strong></p><ul><li>仍然使用Selective Search方法提取候选框，耗费时间大。</li></ul><h1 id="5-Faster-R-CNN"><a href="#5-Faster-R-CNN" class="headerlink" title="5. Faster R-CNN"></a>5. Faster R-CNN</h1><p>针对Fast R-CNN依然使用选择搜索方法提取候选框耗时较大的问题，Faster R-CNN加入了一个提取边缘的神经网络，即将搜索候选框的工作也交由神经网络。具体来说，Faster R-CNN引入了Region Proposal Network（RPN）替代Selective Search，同时引入anchor box以应对目标形状变化问题。（Anchor box就是位置和大小固定的box，可理解为实现设定好的固定的proposal）Faster R-CNN将RPN放在最后一个卷积层后面，RPN直接训练得到候选区域。Faster RCNN抛弃了传统的滑动窗口和SS方法，直接使用RPN生成检测框，这也是Faster R-CNN的巨大优势，能极大提升检测框的生成速度。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bmijefj30cj0cm0tf.jpg" srcset="/img/loading.gif" alt="img"></p><p>各层功能：</p><p>(1) Conv layers卷积层提取特征图：Faster R-CNN首先使用一组基础的conv+relu+pooling层提取输入图像的特征图，该特征图会用于后续的RPN层和全连接层。</p><p>(2) RPN(Region Proposal Networks)层:RPN网络主要用于生成候选区域，首先生成一堆Anchor Box，对其进行裁剪过滤后使用Softmax来判断该Anchor Box是属于前景或者背景（即是物体或不是物体），是一个二分类问题。同时另一分支的边界框回归来修正Anchor Box，形成更精确的proposal.</p><p>(3) ROI池化层：该层利用RPN生成的Proposals和VGG16最后一层得到的Feature map得到固定大小的Proposal Feature map,以供后续利用全连接操作进行目标识别和定位。</p><p>(4) Classifier：将Roi Pooling层形成固定大小的feature map进行全连接操作，利用Softmax进行具体类别的分类，同时，利用L1 Loss完成bounding box regression回归操作获得物体的精确位置。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bnwi0qj31820lkthq.jpg" srcset="/img/loading.gif" alt="img"></p><p>上图中RPN网络实际分为2条线，上面的网络分支通过softmax分类anchors获得前景和背景（实际应用过程中，我们将目标默认为前景）；下面的网络分支用于计算对于anchors的边界框回归的偏移量，以获得较精确的目标候选区。（注：这里的较精确是相对于后面全连接层的再一次box regression而言）。之后的Proposal层综合前景锚点和边界框回归偏移量获取目标的候选区，同时剔除太小和超出边界的目标区域。RPN实际就是实现了目标定位功能。</p><p>RPN网络：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bpp09gj30w00l40vq.jpg" srcset="/img/loading.gif" alt="img"></p><p>(1) RPN网络的作用就是将候选区域的提取也整合到了神经网络中，而且RPN网络和Fast R-CNN共用卷积网络，几乎没有增加计算量。</p><p>(2) RPN提取候选框的方法是对conv feature map上的每个点为中心取9种不同比例大小的anchors（3种尺度，3种比例），再按照比例映射到原图中即为提取的region proposals。具体的实现方法如上图，先用一个256维的3<em>3卷积核以步长为1进行特征提取，这样就可以得到一个256</em>1的特征向量，将这个256长度的特征向量分别输入两个全连接层，其中一个输出长度为2<em>9=18表示9个anchors是物体和不是物体的概率，另一个输出长度为4</em>9=36表示每个proposals的四个坐标。</p><p>(3) 在得到了概率和bbox的坐标之后映射到原图中得到region proposals，再进行一次非极大值抑制(NMS)得到最终输入ROI池化层的proposals。</p><p>创新点：</p><ul><li>设计了RPN网络，利用CNN卷积后的特征图生成候选区域，取代了SS和EB等方法，极大地提升了生成候选区域的速度。</li><li>训练RPN与Fast R-CNN共享部分卷积层，极大地提升了检测速度。</li></ul><h1 id="6-总结："><a href="#6-总结：" class="headerlink" title="6.总结："></a>6.总结：</h1><ul><li><p><strong>R-CNN（Selective Search + CNN + SVM）</strong></p><ul><li>使用Selective Search提取候选区域。</li><li>使用CNN抽取特征，然后使用一系列的SVM做类别预测。</li><li>创新点：提出了Region Proposal并与CNN、SVM结合，实现目标检测。</li><li>缺点：训练时间长、占用空间大、步骤繁多、重复计算。</li></ul></li><li><p><strong>SPP-net（ROI Pooling）</strong></p><ul><li>使得任意大小的特征图都能够转换成固定大小的特征向量。</li><li>R-CNN要对每个候选区域计算卷积，而SPP-Net只需要对原图计算一次卷积，节省了大量的计算时间。</li><li>SPP-Net解决了R-CNN重复提取候选区域特征的问题，同时允许各种尺寸图像作为输入。</li><li>解决了图像畸变的问题。</li></ul></li><li><p><strong>Fast R-CNN（Selective Search + CNN + ROI）</strong></p><ul><li>借助多任务损失函数，将物体识别和位置修正合并到一个网络中，不再进行分步训练。</li><li>共享卷积层</li><li>无需在硬盘中存储训练过程中的特征数据。</li><li>用ROI层代替SPP层，可以使用BP算法更高效的训练更新整个网络。</li><li>相较于R-CNN和SPP-Net，检测结果的精确度更高</li><li>对全连接层提出了SVD奇异值分解的优化方法，降低了全连接层需要学习的参数数目，节省计算时间。</li><li>缺点：仍然使用Selective Search方法提取候选框，耗费时间大。</li></ul></li><li><p><strong>Faster R-CNN（RPN + CNN + ROI）</strong></p><ul><li>引入RPN替代Selective Search，极大提升了生成候选区域的速度。</li><li>将候选区域的提取也整合到了神经网络中。引入anchor box以应对目标形状变化问题。</li></ul></li></ul><p>基于候选区域的R-CNN系列目标检测方法是当前目标检测领域最主要的一个分支，从R-CNN, SPP-NET, Fast R-CNN, 到Faster R-CNN，算法流程变得越来越精简，精度越来越高，速度也越来越快。</p><p><strong>附：YOLO算法运行结果</strong></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3bnd51oj30nl0fqdjh.jpg" srcset="/img/loading.gif" alt="img"><br><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghi3boqp4hj30nk0fpn38.jpg" srcset="/img/loading.gif" alt="img"></p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>R-CNN</tag>
      
      <tag>Fast-RCNN</tag>
      
      <tag>Faster-RCNN</tag>
      
      <tag>SPPnet</tag>
      
      <tag>SelectiveSearch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Pytorch 学习代码</title>
    <link href="/2020/08/01/pytorch-code/"/>
    <url>/2020/08/01/pytorch-code/</url>
    
    <content type="html"><![CDATA[<details>      <summary>基础操作</summary>  <pre><code class="hljs python"><span class="hljs-keyword">import</span> torch</code></pre><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)x</code></pre>    tensor([[-0.7203,  0.2899, -1.0918],            [ 1.0827, -0.2606,  1.2002],            [-1.4074, -0.6593,  0.5724],            [-2.3323, -0.3888,  1.1932],            [-0.9810,  0.3074,  1.8057]])<pre><code class="hljs python">torch.empty(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)</code></pre>    tensor([[ 0.0000e+00, -3.6893e+19,  0.0000e+00],            [-3.6893e+19,  1.1033e-15,  1.4013e-45],            [-3.7719e-01,  4.5911e-41, -3.7701e-01],            [ 4.5911e-41,  0.0000e+00,  0.0000e+00],            [-3.7711e-01,  4.5911e-41,  0.0000e+00]])<pre><code class="hljs python">x = torch.rand(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)x</code></pre>    tensor([[0.9503, 0.3975, 0.5659],            [0.7228, 0.2224, 0.8126],            [0.8509, 0.7857, 0.6712],            [0.4691, 0.0178, 0.1114],            [0.3368, 0.6302, 0.7395]])<pre><code class="hljs python">x = torch.zeros(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)x.dtype</code></pre>    torch.float32<pre><code class="hljs python">x = torch.zeros(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,dtype=torch.long)xx.dtype</code></pre>    torch.int64<pre><code class="hljs python">x = torch.tensor([<span class="hljs-number">4.4</span>,<span class="hljs-number">3</span>])x</code></pre>从一个已有的tensor构建一个tensor,重用原来tensor的特征，例如数据类型，除非提供新数据<pre><code class="hljs python">x = x.new_ones(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>,dtype=torch.double)print(x);x.dtype</code></pre>    tensor([[1., 1., 1.],            [1., 1., 1.],            [1., 1., 1.],            [1., 1., 1.],            [1., 1., 1.]], dtype=torch.float64)    torch.float64随机产生一个与x形状相同的tensor<pre><code class="hljs python">x = torch.randn_like(x, dtype=torch.float)xx.shape</code></pre>    torch.Size([5, 3])加法运算<pre><code class="hljs python">y = torch.rand(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)y</code></pre>    tensor([[0.3626, 0.0456, 0.2931],            [0.6978, 0.5613, 0.7820],            [0.4712, 0.1948, 0.7791],            [0.4992, 0.7192, 0.9465],            [0.9000, 0.2099, 0.3872]])<pre><code class="hljs python">x+y</code></pre>    tensor([[-1.5833,  1.2630, -1.2127],            [ 0.9719,  0.7344, -0.0690],            [ 1.1334, -0.1762, -0.5193],            [ 0.2159, -0.3773,  0.6665],            [ 1.7880,  0.4180, -0.9072]])<pre><code class="hljs python">torch.add(x,y)</code></pre>    tensor([[-1.5833,  1.2630, -1.2127],            [ 0.9719,  0.7344, -0.0690],            [ 1.1334, -0.1762, -0.5193],            [ 0.2159, -0.3773,  0.6665],            [ 1.7880,  0.4180, -0.9072]])<pre><code class="hljs python">result = torch.empty(<span class="hljs-number">5</span>,<span class="hljs-number">3</span>)torch.add(x,y,out=result)result</code></pre>    tensor([[-1.5833,  1.2630, -1.2127],            [ 0.9719,  0.7344, -0.0690],            [ 1.1334, -0.1762, -0.5193],            [ 0.2159, -0.3773,  0.6665],            [ 1.7880,  0.4180, -0.9072]])任何以_结尾的操作都会替换原变量<pre><code class="hljs python">y.add_(x)y</code></pre>    tensor([[-1.5833,  1.2630, -1.2127],            [ 0.9719,  0.7344, -0.0690],            [ 1.1334, -0.1762, -0.5193],            [ 0.2159, -0.3773,  0.6665],            [ 1.7880,  0.4180, -0.9072]])<pre><code class="hljs python">y[<span class="hljs-number">2</span>:,]</code></pre>    tensor([[ 1.1334, -0.1762, -0.5193],            [ 0.2159, -0.3773,  0.6665],            [ 1.7880,  0.4180, -0.9072]])<pre><code class="hljs python">y[<span class="hljs-number">2</span>:<span class="hljs-number">4</span>,<span class="hljs-number">1</span>:]</code></pre>    tensor([[-0.1762, -0.5193],            [-0.3773,  0.6665]])<pre><code class="hljs python">x.shapez=x.view(<span class="hljs-number">-1</span>,<span class="hljs-number">5</span>)z</code></pre>    tensor([[-1.9458,  1.2174, -1.5058,  0.2741,  0.1731],            [-0.8509,  0.6623, -0.3710, -1.2984, -0.2833],            [-1.0965, -0.2801,  0.8880,  0.2081, -1.2944]])只有一个元素的tensor，可用.item()方法把里面的value变成python数值<pre><code class="hljs python">x = torch.randn(<span class="hljs-number">1</span>)x.item()</code></pre>    0.704008936882019<pre><code class="hljs python">z.shape</code></pre>    torch.Size([3, 5])转置：<pre><code class="hljs python">z</code></pre>    tensor([[-1.9458,  1.2174, -1.5058,  0.2741,  0.1731],            [-0.8509,  0.6623, -0.3710, -1.2984, -0.2833],            [-1.0965, -0.2801,  0.8880,  0.2081, -1.2944]])<pre><code class="hljs python">t = z.transpose(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>)t.shape</code></pre>    torch.Size([5, 3])把Torch Tensor转换成Numpy Array<pre><code class="hljs python">a = torch.ones(<span class="hljs-number">5</span>)a</code></pre>    tensor([1., 1., 1., 1., 1.])<pre><code class="hljs python">b = a.numpy()b</code></pre>    array([1., 1., 1., 1., 1.], dtype=float32)两者共享内存空间<pre><code class="hljs python">b[<span class="hljs-number">2</span>]=<span class="hljs-number">2</span>b</code></pre>    array([1., 1., 2., 1., 1.], dtype=float32)<pre><code class="hljs python">a</code></pre>    tensor([1., 1., 2., 1., 1.])把numpy ndarray转换成Torch Tensor<pre><code class="hljs python"><span class="hljs-keyword">import</span> numapy <span class="hljs-keyword">as</span> npa = np.ones(<span class="hljs-number">5</span>)a</code></pre>    array([1., 1., 1., 1., 1.])<pre><code class="hljs python">b = torch.from_numpy(a)b</code></pre>    tensor([1., 1., 1., 1., 1.], dtype=torch.float64)<pre><code class="hljs python">np.add(a,<span class="hljs-number">1</span>,out=a)print(a,b)<span class="hljs-comment">#a和b共享内存</span></code></pre>    [3. 3. 3. 3. 3.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)<pre><code class="hljs python">a = a + <span class="hljs-number">1</span>  <span class="hljs-comment">#此时为a重新分配一个新的空间</span>print(a,b)</code></pre>    [4. 4. 4. 4. 4.] tensor([3., 3., 3., 3., 3.], dtype=torch.float64)# CUDA Tensors使用.to方法，Tensor可以被移动到别的device上<pre><code class="hljs python"><span class="hljs-keyword">if</span> torch.cuda.is_available():    device = torch.device(<span class="hljs-string">"cuda"</span>)    y = torch.ones_like(x, device=device)    x = x.to(device)    z=x+y    z.to(<span class="hljs-string">"cpu"</span>, torch.double)</code></pre>    False<pre><code class="hljs python">z.to(<span class="hljs-string">"cpu"</span>).data.numpy()z.cpu().data.numpy() <span class="hljs-comment">#numpy都是在CPU上操作，需要先将GPU中的变量转换到CPU</span></code></pre><pre><code class="hljs python">model = model.cuda()</code></pre></details>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——R-CNN</title>
    <link href="/2020/07/30/R-CNN/"/>
    <url>/2020/07/30/R-CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="丰富的特征层次结构，用于精确的对象检测和语义分割"><a href="#丰富的特征层次结构，用于精确的对象检测和语义分割" class="headerlink" title="丰富的特征层次结构，用于精确的对象检测和语义分割"></a>丰富的特征层次结构，用于精确的对象检测和语义分割</h1><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>在标准的PASCAL VOC 数据集上测量的对象检测性能在过去几年已经稳定。最佳性能的方法通常是一个复杂的混合系统，它通常将多个低级图像特征与高级上下文组合起来。在本文中，我们提出了一个简单和可扩展的检测算法，相对于以前在VOC2012上的最佳结果(实现mAP 53.3%)，在平均精度上(mAP)提高了30%以上。我们的方法结合了两个关键的要素：(1) 将高容量的卷积神经网络(CNN)应用到自下而上的候选区域方法中，来定位和分割对象；(2)当标记的训练数据稀缺时，对于一个辅助任务来说，使用一个有监督的预训练好的模型，加上在特定域上进行微调（domain-specific <strong>fine-tuning</strong>）后，会产生一个显著的性能提高。因为我们将候选区域(region proposal)和卷积神经网络(CNN)结合在一起，我们称我们的方法叫R-CNN：带有CNN特征的区域(Region with CNN features)，我们也将R-CNN与OverFeat进行了比较，OverFeat是一个最近提出的基于类似CNN架构的滑动窗口检测器。我们发现R-CNN在200类的ILSVRC2013检测数据集上的结果比OverFeat好很多。完整系统的源代码可以从下面获得：<a href="http://www.cs.berkeley.edu/~rbg/rcnn" target="_blank" rel="noopener">地址</a></p><h1 id="1-引言"><a href="#1-引言" class="headerlink" title="1.引言"></a>1.引言</h1><p>特征很重要。在过去10年的时间里关于各种视觉识别任务的进展大量基于SIFT和HOG的使用。但是如果我们观察在经典的视觉识别任务(PASCAL VOC目标检测)上的性能表现，通常认为在2010-2012年期间进展缓慢，在这期间只是通过建造一些集成系统和在一些成功方法上做一些小改动，收效甚微。</p><p>SIFT和HOG是逐块定向直方图，它是一种表示方法，我们大致可以与V1中的复杂细胞，灵长类动物视觉途径中的第一皮质区域相关联。但我们也知道识别发生在几个阶段的下游，这表明可能有分层的，多阶段的过程用于计算特征，这些特征甚至具有更多的视觉识别信息。</p><p>Fukushima的“neocognitron”是一种生物学启发的用于模式识别分层和转移不变的模型，是早期的对于这样一个过程的尝试。然而”neocognitron“缺少一个监督训练算法。Rumelhart et al. [33], LeCun et al. [26]的研究表明通过反向传播的随机梯度下降方法对于训练CNNs是非常有效的，它也是一种”neocognitron“模型的扩展。</p><p>CNNs在20世纪90年代被大量使用，但随着SVM的兴起，CNNs又不流行了。在2012年，Krizhevsky等人通过在ILSVRC上显示出了更高的图像分类精度，使得人们重新燃起了对CNN的兴趣。他们的成功源于在120万个标记图像上训练的大的CNN，并结合了一些在LeCun的CNN中使用的一些小技巧(例如，max(x,0)修正的非线性单元和”droupout”正则化)。</p><p>在ILSVRC 2012研讨会期间，ImageNet结果的意义被激烈的讨论。中心问题可以归纳如下：在ImageNet上的CNN分类结果在多大程度上可以被推广到PASCAL VOC挑战的目标检测结果上？</p><p>我们通过弥合图像分类和对象检测之间的差距来回答这个问题。本文首次表明，与基于更简单的HOG类似的特征系统相比，CNN可以在PASCAL VOC上得到更加显著的目标检测表现。</p><p>与图像分类不同，检测需要在图像中定位（可能有许多）目标。一种方法是将定位看作回归问题。然而，Szegedy等人的工作，与我们自己的工作同时表明这种策略在实践中可能不好（在VOC2007中，他们提交的mAP是30.5%，而我们的方法实现的是58.5%）。一种替代方案是构建一个滑动窗口检测器。CNN已经以这种方式使用了至少二十年，通常是在限定的对象类别上，例如人脸和行人。为了保持高的空间分辨率，这些CNN通常仅具有两个卷积和池化层。我们也考虑采用一个滑动窗口的方法。然而，在我们的网络中具有5个卷积层的单元在输入图像上具有非常大的感受野（195*195像素）和步幅（32*32像素）。这使得在滑动窗口范例内的精确定位成为一个公开的技术挑战。</p><p>相反，我们通过在”使用区域识别“范式中进行计算来解决CNN定位问题，其已经成功应用于目标检测和语义分割。在测试时，我们的方法为输入图像生成大约2000个类别无关的候选区域，使用CNN从每个候选区提取固定长度的特征向量，然后使用特定类别的线性SVM对每个进行区域进行分类。我们使用一种简单技术（仿射图像扭曲-affine image warping）来对每一个候选区域计算一个固定大小的CNN的输入，而不虑区域的形状。图1给出了我们方法的概述，并高亮了我们的一些结果。由于我们的系统将候选区域和CNN结合在一起，我们将方法缩写成R-CNN：带有CNN特征的区域（Region with CNN features）。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh8zvk8xa3j30v20badmk.jpg" srcset="/img/loading.gif" alt="image-20200730144403455" style="zoom:50%;" /></center><p><strong>图1：目标检测系统概述 </strong>  我们的系统（1）接收一个输入图像，（2）抽取大约2000个从下到上的候选区域，（3）对于每一个大的候选区域使用一个大的CNN计算特征，然后（4）使用特定类的线性SVM对每一个区域进行分类。R-CNN在PASCAL VOC 2010 上实现了一个平均精度（mAP）53.7%。为了比较，[39]使用相同的候选区域方法实现了35.1%的mAP，但是使用了空间金字塔和视觉词袋方法。</p><p>在本文的这个更新版本中，我们通过在200类的ILSVRC2013检测数据集上运行R-CNN，提供了一种对R-CNN和最近提出的OverFeat检测系统的一种直接比较。OverFeat使用滑动窗口CNN进行检测，直到现在也是ILSVRC2013检测中性能最佳的方法。我们显示R-CNN显著优于OverFeat，R-CNN的mAP为31.4%，而OverFeat的mAP是24.3%。</p><p>检测中面临的第二个挑战是标记的数据很少，并且当前可用的量不足以训练大的CNN。该问题的常规解决方案是使用无监督进行预训练，随后是进行有监督的微调。本文的第二个主要贡献是先在大型辅助数据集（ILSVRC）上进行有监督的预训练，然后是在小数据集（PASCAL）上进行特定领域的微调，这种方法在数据不足时是学习大容量CNN的有效范例。在我们的实验中，用于检测任务的微调将mAP性能提高了8个百分点。微调之后，我们的系统在VOC2010上实现了mAP到54%，而高度调整后的，基于HOG的DPM的mAP为33%。我们也给读者指出了由 Donahue等人做的同时期的工作，他表明Krizhevsky的CNN可以当成一个黑盒使用（不用微调）来进行特征提取，在几个识别任务中产生优秀的表现，包括场景分类，细粒度子分类，和域适配。</p><p>我们的系统也很高效。唯一的特定类的计算也是合理的小的矩阵向量积和贪婪的非极大值抑制。该计算性质来自于所有类别中共享的性质，并且比先前使用的区域特征低两个数量级（参见[39]）。</p><p>理解我们方法的失败模式也是改善它的关键，所以我们报告了从Hoiem等人[23]的检测分析工具得到的结果。作为这种分析的直接结果，我们证明一个简单的边界框回归方法显著地减少了误定位，这是主要的错误模式。<br>　　在讲述技术细节之前，我们注意到，因为R-CNN在区域上操作，所以很自然地将其扩展到了语义分割任务上。通过微小的修改，我们还在PASCAL VOC分割任务上实现了竞争性的结果，在VOC2011测试集上的平均分割精度47.9%。</p><h1 id="2-目标检测"><a href="#2-目标检测" class="headerlink" title="2.目标检测"></a>2.目标检测</h1><p>我们的目标检测系统包括三个模块。第一个生成类无关的候选区域。这些候选区域定义了可用于我们的检测器的候选检测的集合。第二个模块是一个大的卷积神经网络，它从每个区域中抽取固定长度的特征向量。第三个模块是一些特定类别的线性SVM。在本节中，我们介绍每个模块的设计决策，描述其测试时间使用情况，详细了解其参数的学习方法，并在PASCAL VOC 2010-12和ILSVRC-2013上显示了检测结果。</p><h2 id="2-1-模块设计"><a href="#2-1-模块设计" class="headerlink" title="2.1 模块设计"></a>2.1 模块设计</h2><p><strong>候选区域。</strong>各种最近的论文提供了用于生成类无关候选区域的方法。例子包括：objectness[1]，选择性搜索[39]，类独立的候选对象[14]，CPMC，多尺度组合分组，和Ciresan等人通过将CNN应用于规则间隔的方形剪裁来检测有丝分裂细胞，这是候选区域的一个特殊情况。R-CNN对于特定的候选区域方法是不可知的，我们使用选择性搜索来实现与先前的检测工作的控制比较。</p><p><strong>特征提取。</strong>我们使用Krizhevsky等人描述的CNN的Caffe实现从每个候选区域中提取4096维特征向量。通过将减去平均值的227*227的RGB图像通过5个卷积层和2个全连接层的前向传播来计算特征。我们引导读者参考[24,25]来获得更多的网络架构的细节。</p><p>为了计算候选区域的特征，我们必须首先将该区域中的图像转换成与CNN兼容的形式（其体系结构需要固定的227*227像素尺寸的输入）。在我们任意形状区域的许多可能的变换中，我们选择最简单的。忽略了候选区域的大小或宽高比，我们会将其周围的紧密边界框中的所有像素扭曲到所需要的大小。在变形之前，我们扩大紧密边界框，使得在变形大小处，在原始框周围正好有p个像素的扭曲图像上下文（我们使用p=16）。图2展示了扭曲训练区域的随机采样。扭曲的替代方法会在附录A中讨论。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh92lv3m89j30va0a2dxn.jpg" srcset="/img/loading.gif" alt="image-20200730161834161" style="zoom:50%;" /></center><h2 id="2-2-测试时间检测"><a href="#2-2-测试时间检测" class="headerlink" title="2.2 测试时间检测"></a>2.2 测试时间检测</h2><p>在测试时，我们在测试图像上运行选择性搜索抽取大约2000个候选区域（在所有的实验中，我们都使用选择性搜索的“快速模型”）。我们扭曲每一个候选图像，并且将它前向传播通过CNN为了去计算特征。然后，对于每个类，我们使用为该类训练的SVM对每个提取的特征向量求得分。给定图像中所有得分区域，我们对每个类独立地应用贪心的非最大值抑制，如果其具有与大于学习阈值较高得分的选择区域的IoU重叠，那么它就会拒绝一个区域。</p><p><strong>运行时间分析。</strong>两个属性使检测有效。首先，所有类别共享所有CNN参数。第二，当与其它常见方法（例如具有视觉词袋编码的空间金字塔）相比时，由CNN计算的特征向量是低维的。例如，在UVA检测系统中使用的特征比我们的特征大两个数量级（360K对4K维）。</p><p>这种共享的结果就是计算候选区域和特征所花费的时间（在一个GPU上是13s/图像，在CPU上是53s/图像）在所有类上平摊。唯一的特定类的计算是特征和SVM权重和非最大值抑制之间的点积。在实际中，图像的所有点积被分批成单个矩阵-矩阵的乘积。特征矩阵通常为2000*4096，SVM权重矩阵为4096*N，其中N是类的数量。</p><p>这个分析表明，R-CNN可以扩展到数千个对象类，而不用诉诸于近似技术，如哈希。即使有100K个类，在现代的多核CPU上产生的矩阵乘法也只需要10s。这种效率不仅仅是使用候选区域和共享特征的结果。由于其高维特征，UVA系统将减慢两个数量级，需要134GB的存储器仅仅存储100k的线性预测器，相比之下，我们的低维特征只有1.5GB。</p><p>将R-CNN与Dean等人近来使用的DPM和哈希的可扩展的检测方法的工作进行对比也是非常有趣的。他们在VOC 2007上报告了大约16%的mAP，当引入了10k个牵引类时， 每个图像的运行时间为5分钟。用我们的方法，10k个牵引类在一个CPU上大约运行1分钟，因为没有进行近似，mAP将保持在59%（第3.2节）。</p><h2 id="2-3-训练"><a href="#2-3-训练" class="headerlink" title="2.3 训练"></a>2.3 训练</h2><p><strong>监督预训练。</strong>我们有区别的在仅使用图像级注释的大型辅助数据集（ILSVRC212分类）上预训练CNN（边界框标签不可用于该数据）。预训练是用开源的Caffe CNN库来执行的。简而言之，我们的CNN几乎匹配了Krizhevsky等人的性能，获得了top-1错误率在ILSVRC2012分类验证集上2.2个百分点的增加。这种差异是由于训练过程的简化。</p><p><strong>特征领域的微调。</strong>为了使我们的CNN适应新任务（检测）和新领域（扭曲的候选窗口），我们仅使用扭曲的候选区域继续CNN参数的随机梯度下降（SGD）的训练。除了用随机初始化的（N+1）类分类层（其中N是对象类的数目，加上背景的1）来替换CNN的ImageNet特定的1000类分类层之外，CNN架构没有改变。对于VOC来说，N = 20，并且对于ILSVRC2013来说，N = 200。我们将所有候选区域与真实框重叠大于等于0.5的作为该框类的正例，其余的作为负例。我们以0.01的学习率（初始预训练率的1/10）开始SGD，这允许微调进行而不破坏初始化。在每一次的SGD迭代中，我们均匀地采样32个正窗口（在所有类上），和96个背景窗口以构造大小为128的小批量。我们将采样偏向正窗口，因为它们与背景相比非常少。</p><p><strong>对象类别分类器。</strong>考虑训练一个二分类器去检测小汽车。很清楚，紧紧包围一个汽车的图像应该是一个正例。同样，明显的，与汽车无关的背景区域应该是一个负例。不清楚的是，怎样标注一个部分覆盖一辆汽车的区域。我们使用IoU重叠阈来重新解决这个问题，在这之下的区域就定义为负例。通过在验证集上的{0，0.1，…，0.5}的网络搜索来选择一个重叠阈0.3。我们发现小心地选择这个阈值是非常重要的。将它设置为0.5，正如[39]中一样，能够将mAP减少5个百分点。相似地，将它设置成0的话，能够将mAP减少4个百分点。正例仅仅被定义为每个类的真实边界框。</p><p>一旦特征被提取出来，训练标签被应用上，我们就对每一个类优化一个线性SVM。由于训练数据太大，不适合内存，我们采用标准的hard negative mining方法。Hard negative mining快速收敛，并且在实际中，所有的图像仅通过一次，mAP就停止增加了。</p><p>在附录B中，我们讨论了为什么在微调和SVM训练中正例和负例的定义不同了。我们还讨论了训练检测SVM的权衡，而不是简单地使用来自微调CNN的最终SoftMax层的输出。</p><h2 id="2-4-在PASCAL-VOC-2020-12上的结果"><a href="#2-4-在PASCAL-VOC-2020-12上的结果" class="headerlink" title="2.4 在PASCAL VOC 2020-12上的结果"></a>2.4 在PASCAL VOC 2020-12上的结果</h2><p>遵循PASCAL VOC 最佳实践[15]，我们验证了VOC 2007数据集（第3.2节）中所有的设计决策和超参数。对于在VOC 2010-12数据集上的最后的结果，我们微调了VOC 2012训练上的CNN并且在VOC 2012 训练验证集上优化了我们的检测SVM。我们对于两个主要算法变量（有和没有边界回归）中的每一个都只向评估服务器提交测试结果一次。</p><p>表1展示了在VOC 2010上的完整结果。我们将我们的方法与4个强基线进行了对比，包括SegDPM，其将DPM检测器与语义分割系统的输出结合在一起，并且使用了额外的内部上下文检测器和图像分类器重新评分。</p><p>最杰出的比较是来自Uijlings等人的UVA系统，因为我们的系统也使用了相同的候选区域算法。为了对区域进行分类，他们的方法构建了一个4个级别的空间金字塔，并且用密集采样的SIFT，扩展OpponentSIFT，和RGBSIFT描述符来填充他们，每个矢量都用4000个字的编码本来进行量化。使用一个直方图相交核的SVM进行分类。与它们的多特征，非线性核SVM方法相比，我们在mAP上实现了一个大的改进，从35.1%到 53.7% mAP，同时也快得多（第2.2节）。我们的方法在VOC 2011/12测试中达到了相似的性能（53.3%的mAP）。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha4vvkvfgj31qq0aydk4.jpg" srcset="/img/loading.gif" alt="image-20200731142256287"></p><blockquote><p>表1：在2010测试集上检测平均精度（%），R-CNN与UVA和Regionlet最直接对比，因为所有的方法都使用选择性搜索候选区域。边界框回归会在C节描述。在提交的时候，SegDPM是在PASCAL VOC排行榜上表现最出色的。DPM和SegDPM使用了一种其它方法没有使用的上下文重新评估的方法。</p></blockquote><h2 id="2-5-在ILSVRC2013检测上的结果"><a href="#2-5-在ILSVRC2013检测上的结果" class="headerlink" title="2.5 在ILSVRC2013检测上的结果"></a>2.5 在ILSVRC2013检测上的结果</h2><p>我们在200类ILSVRC2013检测数据集上运行R-CNN，使用与PASCAL VOC 上相同的系统超参数。我们遵循相同的规则，就是将测试结果提交给ILSVRC2013评估服务器仅仅俩次，一次带有边界框回归，另一次不带。</p><p>图3比较了R-CNN与ILSVRC2013竞赛中的条目以及竞赛后的OverFeat结果。R-CNN实现了31.4%的mAP，显著超过了OverFeat的24.3%的次优结果。为了给出对类别分布的一个感觉，呈现了一个盒图，并且在论文的结尾处的<strong>表8</strong>里出现了类别AP的表格。大多数的竞赛提交（OverFeat, NEC-MU, UvAEuvision, Toronto A, and UIUC- IFP）都使用了卷积神经网络，指示在CNN如何应用于对象检测方面存在显著差异，导致了极大变化的结果。</p><p>在第4节，我们概述了ILSVRC2013检测数据集，并提供了在运行R-CNN时我们做出的选择的详细信息。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha55q0w7zj31qa0nsn43.jpg" srcset="/img/loading.gif" alt="image-20200731143221826"></p><blockquote><p>图3：<strong>（左）在ILSVRC2013检测测试数据集上的mAP。</strong>带*的方法都是使用了外部训练数据（所有情况下都是从ILSVRC分类数据集中的图像和标签）。(右）每种方法的200个平均精度值的箱线图。未显示竞争后的OverFeat结果的框图，因为每类的APs尚不可用（R-CNN的每类AP在表8中，也包含在上传到arXiv.org的技术报告源中；见R-CNN- ILSVRC2013-APs.txt）。红线标示中值AP，箱底和箱顶是25和75百分位数。虚线延伸到每种方法的最小和最大的AP值。每个AP被绘制成虚线上的一个绿点（最好用数字缩放）。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha5drt2oxj30u00z2wso.jpg" srcset="/img/loading.gif" alt="image-20200731144008537"></p><h1 id="3-可视化，消融，和误差模式"><a href="#3-可视化，消融，和误差模式" class="headerlink" title="3. 可视化，消融，和误差模式"></a>3. 可视化，消融，和误差模式</h1><h2 id="3-1-可视化学习的特征"><a href="#3-1-可视化学习的特征" class="headerlink" title="3.1 可视化学习的特征"></a>3.1 可视化学习的特征</h2><p>第一层的过滤器可以直接被可视化并且很容易理解。它们捕获有方向的边和相对的颜色。理解下面的层次是更具有挑战性的。Zeiler and Fergus在[42]中提出了一种视觉上有吸引力的反卷积方法。我们提出了一个简单（和互补的）非参数的方法，直接显示网络学习的内容。</p><p>这个想法就是在网络中单独出一个特定的单元（特征），并且使用它就好像它本身就是一个对象检测器。也就是说，我们在大量提出的候选区域（大约1千万）计算单元的activations，将候选区域从高的activation到低的activation进行排序，执行非最大值抑制，然后显示最高评分区域。我们的方法让选择的单元“说出自己”通过精确显示它所触发的输入。我们避免平均，以便看到不同的视觉模式，并且可以获得由单元计算的不变性的洞察。</p><p>我们从第5个池化层观察单元，其是网络第5和最后卷积层的最大池化的输出。第5个池化层的特征映射是6∗6∗255=9216维。忽略边界的影响，每一个第5层的池化单元在原始的227 <em> 227个像素输入上都有一个195 </em> 195像素的感受野。一个中心的pool5的单元具有几乎全局的视野，而一个靠近边缘的单元具有更小的，剪裁的支持。</p><p>图4中的每一行显示了我们在VOC 2007 trainval上微调的CNN的一个pool5单元的前16个activation。256个功能独特的单元中的六个被可视化（附录D包括更多）。选择这些单元来显示网络学习的代表性样本。在第二行中，我们看到在狗脸和点阵上触发的单元。对应于第三行的单元是一个红色斑点检测器。还有用于人脸的检测器和更多抽象的图案，例如文本和带有窗口的三角形结构。网络似乎学习将少量的调整的类特征与形状，纹理，和材料属性的分布式表式组合在一起的表示。随后的全连接层fc6具有对这些丰富特征的大量组合进行建模的能力。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha5vfdkv2j31jo0mchdu.jpg" srcset="/img/loading.gif" alt="image-20200731145706550"></p><blockquote><p>图4：6个pool5单元的靠前的区域。感受野和活性值以白色画出。某些单元与概念对齐，例如人（第1行）或文本（4）。其他单元捕获纹理和材料属性，例如点组（2）和特殊的反射（6）。</p></blockquote><h2 id="3-2消融研究"><a href="#3-2消融研究" class="headerlink" title="3.2消融研究"></a>3.2消融研究</h2><p><strong>性能逐层，无需微调。</strong>为了理解哪一层对于检测的性能是重要的，我们对CNN的最后三层的每一个都在VOC 2007数据集上分析了结果。pool5层在3.1节中被简单地描述。最后两层总结如下。</p><p>fc6对于pool5是全连接的。为了计算特征，它将4096×9216权重矩阵乘以pool5特征映射（重新形成一个9216维的向量），然后添加上偏移向量。这个中间的向量是被逐元素half-wave修正的（x←max(0,x)&gt;）。</p><p>fc7是网络的最后一层。它通过将由fc6计算的特征乘以4096 × 4096的权重矩阵，并且类似地加上一个偏移向量并且应用half-wave修正来实现的。</p><p>我们首先从不在PASCAL上进行微调的CNN上查看结果，即所有的CNN的参数仅在ILSVRC2012上进行预训练。逐层分析性能（表2第1-3行）提示了fc7的特征比fc6的特征更差。这意味着29%，或者大约1680万个CNN参数可以被去除而不降mAP。更令人惊讶的是，删除fc6和fc7产生相当好的结果，即使计算pool5特征仅使用了CNN参数的6%。大多数CNN的表示能力来自于其卷积层，而不是来自于大得多的密集连接层。该发现表明在通过仅使用CNN的卷积层计算任意大小的图像的HOG意义上的密集特征图中的潜在效用。这种表示将允许在pool5特征的顶部上使用滑动窗检测器（包括DPM）进行实验。</p><p><strong>带有微调的逐层的性能。</strong>在VOC2007 trainval上调整了我们的CNN的参数之后，我们现在再来看一下结果。改善是显著的（表2中的4-6行）：微调将mAP提高了8.0个百分点至54.2%。对于fc6和fc7&lt;，来自微调的提升远大于pool5，这表明从ImageNet中学习的pool5的特征是一般性的，并且大部分的改进是从学习特定领域的非线性分类器获得的。<br>　　<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha65zv8lxj31ja0f4n2t.jpg" srcset="/img/loading.gif" alt="image-20200731150713483"></p><blockquote><p>表2：<strong>在VOC 2007测试集上的检测的平均精度（%）。</strong>1-3行展示的是没有经过微调的R-CNN的性能。4-6行展示了在ILSVRC2012上预训练过，并且在VOC2007trainval上微调过的CNN的结果。7行包括一个简单的边界框回归（BB）阶段，其减少了定位错误（C节）。8-10行展示了DPM方法作为一个强基准线。第一个仅使用了HOG，然而接下来的2个使用了不同的特征学习方法来增强或替换HOG。</p></blockquote><p><strong>对比于最近的特征学习方法。</strong>最近有相对较少的特征学习方法在PASCAL VOC检测上进行尝试。我们看看基于可变形部件模型的两个最近的方法。作为参考，我们也包括了标准的基于HOG的DPM的结果。</p><p>第一种DPM特征学习方法DPM ST[28]用“草图标记”概率的直方图来增强HOG特征。直观来说，一个草图标记就是穿过图像块中心的轮廓的紧密分布。草图标记概率在每个像素处由随机森林计算，该森林被训练成将35*35像素块分类成150个草图标志或背景中的一个。第二种方法，DPM HSC[31]，用稀疏编码（HSC）的直方图代替HOG。为了计算HSC，使用100个7*7像素（灰度）原子的学习字典来解决每个像素处的稀疏码激活。所得激活以三种方式（全半波和两半波）来进行修正，空间池化，单元l2正则化，然后是功率变换（x←sign(x)|x|α）。</p><p>所有的R-CNN变体都大大超过三个DPM基线（表2中的8-10行），包括使用特征学习的两个。与仅使用HOG特征的最新版的DPM相比，我们的mAP高出了20个百分点：54.2% 对 33.7%-相对改善了61%。HOG草图标记的结合超过了HOG自己2.5个mAP，而HSC改进超过HOG4个mAP（当内部与其私有DPM基线相比时-都使用非公开的实现的DPM，其比开源版本的性能要差一点）。这些方法各自实现mAP为29.1%和34.3%。</p><h2 id="3-3-网络结构"><a href="#3-3-网络结构" class="headerlink" title="3.3 网络结构"></a>3.3 网络结构</h2><p>这篇文章的大部分结果都使用Krizhevsky等人的网络结构。然而，我们发现，结构的选择对于R-CNN检测性能有一个很大的影响。在表3中我们展示了使用Simonyan and Zisserman最近提出的16层深度网络的VOC 2007测试的结果。这个网络是近期ILSVRC 2014分类挑战中表现最佳的网络之一。网络具有由13个3*3卷积核组成的均匀结构，中间分散了5个最大池化层，最后跟着3个全连接层。对于OxfordNet，我们将此网络称为“O-Net”，对于TorontoNet的基线称为“T-Net”。</p><p>为了在R-CNN中使用O-Net，为了在R-CNN中使用O-Net，我们从Caffe Model Zoo上下载了VGG_ILSVRC_16_layers模型的公开可用的预训练网络权重。然后，我们使用与T-Net相同的协议对网络进行了微调。唯一的不同就是根据需要使用较小的minibatches（24个示例）以适应GPU的内存。表3中的结果显示具有O-Net的R-CNN基本上优于具有T-Net的R-CNN，将mAP从58.5%增加到66.0%。然而，在计算时间方面存在相当大的缺点，其中O-Net的正向传播比T-Net长大约7倍。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6hsjs2tj31je06u76w.jpg" srcset="/img/loading.gif" alt="image-20200731151836443"></p><blockquote><p><strong>表3：两个不同的CNN结构在VOC2007上测试集上目标检测的平均精度（%）。</strong>前两行是来自于表2，使用了Krizhevsky等人的结构（T-Net）。3，4行使用了最近由Simonyan<br>and Zisserman（O-Net）提出的16层的结构。</p></blockquote><h2 id="3-4-检测错误分析"><a href="#3-4-检测错误分析" class="headerlink" title="3.4 检测错误分析"></a>3.4 检测错误分析</h2><p>我们应用了Hoiem等人的优秀的检测分析工具，为了揭示我们方法的错误模式，了解微调如何改变他们，以及看看我们的错误模式与DPM的比较。分析工具的完整总结超出了本文的范围，我们鼓励读者咨询[23]了解一些更精细的细节（例如“归一化AP”）。由于分析最好在相关图的上下文中才会更受吸引，所以我们在图5和图6的标题内给出了讨论。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6k3gd9bj30qw0kqq7b.jpg" srcset="/img/loading.gif" alt="image-20200731152048924" style="zoom:50%;" /></p><blockquote><p>图5：排名最高的FP类型的分布。每幅图展示了FP类型的演变分布，因为更多的FP按照分数递减的顺序被考虑。每个FP分类成4种类型中的1种：Loc-poor定位（和正确类别的IoU重叠在0.1和0.5，或重复）；Sim-与相似的类别混淆；Oth-与不同对象类别的混淆；BG-在背景上触发的FP。与DPM相比（参见[23]），我们的错误更多的是来自于差的定位，而不是与背景或其他对象类的混淆，表明CNN特征比HOG更有区别性。差的定位可能是由于我们使用自下而上的候选区域和从预训练CNN进行整体图像分类学习的位置不变性。第三列显示了我们的简单的边界框回归方法如何修复很多定位错误。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6lhl4gkj31ka0bqtcj.jpg" srcset="/img/loading.gif" alt="image-20200731152208985"></p><blockquote><p><strong>图6：对对象特征的敏感性。</strong>每个图展示出了六个不同对象特征（遮挡，截断，边界框区域，宽高比，视角，部分可见性）内的最高和最低的表现子集的平均（所有类）归一化AP。我们在图上展示了我们的方法（R-CNN），带有和不带有fine-tuning(FT)和边界框回归（BB），以及DPM 的voc-release5。总的来说，微调并不会降低敏感性（最大值和最小值之间的差值），但是对于几乎所有的特性，基本上改善了最高和最低性能子集。这表明微调不仅仅改善了长宽比和边界框区域的最代性能子集，因为人们可能基于我们如何扭曲网络输入来进行推测。相反，微调改变了所有类别的鲁棒性，包括遮挡，截断，视角，部分可见。</p></blockquote><h2 id="3-5-边界框回归"><a href="#3-5-边界框回归" class="headerlink" title="3.5 边界框回归"></a>3.5 边界框回归</h2><p>基于错误分析，我们实现了一个简单的用于减少定位错误的方法。受在DPM中使用的边界框回归的启发，在给定了一个选择性搜索候选区域的pool5特征的情况下，我们训练了一个线性回归模型用来预测一个新的检测窗口。完整细节在附录C中给出。表1，表2和表5的结果表明这种简单的方法改善了大量错误定位的检测，将mAP提高了3至4个点。</p><h2 id="3-6-定性结果"><a href="#3-6-定性结果" class="headerlink" title="3.6 定性结果"></a>3.6 定性结果</h2><p>在文章最后的图8和图9上展示了ILSVRC2013上的定性的检测结果。每一个图像都是从val2数据集上随机抽取的，并且在所有检测器上精度大于0.5的检测都展示出来了。注意到，这些不是策划的，并且在行动上给了检测器一个现实印象。更加定性的结果展示在图10和图11中，并且这些被策划过。我们选择的每一张图片，都是因为它包含了有趣的，令人惊讶的，或令人欣喜的结果。在这里，同样的，所有精度大于0.5的检测都展示出来的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6vobapwj30u010tqv6.jpg" srcset="/img/loading.gif" alt="image-20200731153155986"></p><blockquote><p>图8：对val2集合的示例检测，使用了从在val2上实现了31.0%mAP的配置。每个图像都是被随机抽取的（都没有进行过策划）。所有精度大于0.5的检测都展示出来了。每一个检测都标注有预测的类和从检测的precision-recall曲线上的精确值。建议放大来看。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6xjbpfhj30u00zh1l0.jpg" srcset="/img/loading.gif" alt="image-20200731153343706"></p><blockquote><p><strong>图9：更加随机的选择的样本。</strong>看图8的标题以获得更详细的描述。建议放大来看。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6zbtoavj30u010c4qr.jpg" srcset="/img/loading.gif" alt="image-20200731153527292"></p><blockquote><p>图10：策划过的样例。每一张选择的图片，都是因为我们发现他们印象深刻，令人惊叹，非常有趣，或者使人开心。建议放大来看。</p></blockquote><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha6zzqx2mj30u013lhdv.jpg" srcset="/img/loading.gif" alt="image-20200731153605032"></p><blockquote><p>图11：更多策划过的样例。详情请看图10的标题。建议放大来看。</p></blockquote><h1 id="4-ILSVRC2013检测数据集"><a href="#4-ILSVRC2013检测数据集" class="headerlink" title="4. ILSVRC2013检测数据集"></a>4. ILSVRC2013检测数据集</h1><p>在第2节中，我们展示了在ILSVRC2013检测数据集上的结果。这个数据集和PASCAL VOC数据集有很少是一样的，需要选择怎样去用它。因为这些决定是不容易的，我们在本节中讨论。</p><h2 id="4-1-数据集概述"><a href="#4-1-数据集概述" class="headerlink" title="4.1 数据集概述"></a>4.1 数据集概述</h2><p>ILSVRC2013数据集被划分成3个集合：训练集（395918），验证集（20121），和测试集（40152），每一个集合的图像的数量都在括号中。val和test的拆分是从相同的图像分布中绘制的。这些图像是类似于场景的，并且在复杂度上与PASCAL VOC的图像相似（物体的数量，混乱的数量，姿势的变化等等）。val和test拆分具有详细的注解，意味着在每个图像中，所有200类的所有实例都用边界框标记。相反，训练集是从ILSVRC2013分类图像分布中提取到的。这些图像具有更多变化的复杂性，单个居中物体的图像较多。不像val和test，训练图像没有完全标注（由于它们的数量较多）。在任意给定的图像中，这200类的实例有可能标注了也有可能没有标注。除了这些图像集合，每一个类都有一个额外的负例图像的集合。手动检查这些负例图像，以验证它们不包含他们关联类的任何实例。负例图像集合在本工作中没有被用到。关于ILSVRC是怎么收集和标注的更多的信息可以在[11,36]中查看到。</p><p>这些拆分的本质为训练R-CNN提供了大量的选择。训练图像不能用于hard negative mining，因为标注是不充分的。负例应该来自于哪里呢？训练图像具有与val和test不同的统计量。训练图像应该被使用吗？如果是，以什么程度呢？虽然我们没有完全评估大量的选择，基于以前的经验，我们提出了似乎是最明显的路径。</p><p>　我们的一般策略就是大量依赖于val集，并且使用训练图像中的一些作为一个正例的辅助源。为了将val用于训练和验证，我们将它大致分成相等的大小“val1”和”val2”集合。因为在val中一些类具有非常少的样本（最小的只有31个，并且一半的少于110个），重要的是产生大致类均衡的分区。为此，产生了大量的候选拆分并且选择了最大相关类不平衡最小的拆分。通过使用它们的类计数作为特征来对val图像进行聚类，然后进行可以改进分割平衡的随机局部搜索来生成每个候选分割。这里使用的特定分裂具有约11%的最大相对不平衡和4%的中值相对不平衡。val1和val2的分割和用于生成他们的代码将会被公开，以允许其它研究人员比较其在本报告中使用的val分割的方法。</p><h2 id="4-2-候选区域"><a href="#4-2-候选区域" class="headerlink" title="4.2 候选区域"></a>4.2 候选区域</h2><p>我们遵循用于在PASCAL上检测的相同的候选区域的方法。对val1，val2和test上的每一个图像（但不是训练中的图像），以“快速模式”运行选择性搜索。需要一个小的修改来处理选择性搜索不是尺度不变性的事实，因此产生的候选区域的数量取决于图像分辨率。ILSVRC图像大小范围从很小到几个几兆像素，因此我们在运行选择性搜索之前将每个图像调整为固定宽度（500pixels）。在val上，选择性搜索在每幅图像上得到了平均2403个候选区域，具有所有的真实的边界框的91.6%的召回率（0.5的IoU的阈值）。这个召回率明显低于在PASCAL上面的，在PASCAL上大约是98%，表明在候选区域阶段还有很大的提升。</p><h2 id="4-3-训练数据"><a href="#4-3-训练数据" class="headerlink" title="4.3 训练数据"></a>4.3 训练数据</h2><p>对于训练数据，我们形成了一组图像和框，其包括来自val1的所有选择性搜索和真实框，以及来自训练的每个类别的多达N个真实框（如果在训练中，一个类有少于N个真实框，然后这些我们就都要了）。我们将会称这个图像和框的数据集为val1+trainN。在消融研究中，我们在val2上为N∈0,500,1000展示了mAP（4.5节）。</p><p>在R-CNN中在三个过程中需要训练数据：（1）CNN微调，（2）检测器SVM的训练，和（3）边界框回归训练。CNN微调使用与用于PASCAL VOC上的完全相同的配置，在val1+trainN上运行了50k的SGD迭代。使用Caffe在在单个NVIDIA Tesla K20上花费了13个小时进行微调。对于SVM训练，来自val1+trainN的所有真实值被用于它们各自类的正例。对从val1随机选择的5000个图像的子集进行hard negative mining。最初的实验表明，对比于一个5000个图像的子集（大约一半），从所有的val1上挖掘负例，仅仅造成了在mAP上0.5个百分点的下降，然而将SVM的训练时间减少了一半。没有从训练集取得负例，因为标注不详尽。未使用额外的验证的负例图像的集合。边界框回归是在val1上训练的。</p><h2 id="4-4-验证和评估"><a href="#4-4-验证和评估" class="headerlink" title="4.4 验证和评估"></a>4.4 验证和评估</h2><p>在提交结果到评估服务器之前，我们使用上述训练数据验证了数据使用选择以及在val2集合上的微调和边界框回归的效果。所有的系统超参数都与PASCAL中使用的值相同（例如，SVM C超参数，在区域扭曲中的填充，NMS阈值，边界框回归超参数）。毫无疑问，一些超参数的选择对于ILSVRC来说是次优，然而这项工作的目标是在ILSVRC上产生初步的R-CNN结果，而没有广泛的数据集调整。在val2上选择了最好的选择之后，我们提交给了ILSVRC2013评估服务器两个结果文件。第一个是没有边界框回归的，第二个是带有边界框回归的。对于这些提交，我们扩展了SVM和边界框回归训练集去分别使用val+train1k和val。我们使用在val1+train1k微调的CNN来避免重新运行微调和特征计算。</p><h2 id="4-5-消融研究"><a href="#4-5-消融研究" class="headerlink" title="4.5 消融研究"></a>4.5 消融研究</h2><p>表4显示了不同数量的训练数据，微调和边界框回归的效果的消融研究。第一个观察到的是，在val2上的mAP和在测试上的mAP匹配的非常接近。这使得我们相信在val2上的mAP是测试集性能的一个很好的指标。第一个结果，20.9%，是R-CNN使用在ILSVRC2012分类数据集上预训练的CNN实现的（没有微调），并且允许访问val1中的少量训练集（回想在val1中的一半的类具有15到55个实例）。将训练集扩展到val1+trainN将性能提高到24.1%，在N=500和N=1000之间基本没有差别。使用仅仅来自val1中的样例微调CNN给出了26.5%的适度改进，然而由于少量的正训练示例，可能存在着显著的过拟合。将微调集扩展到val1+train1k，从而从训练集中每类增加了1000个正例，效果显著，将mAP提升到29.7%。边界框回归将结果提高到31.0%，这是在PASCAL中观察到的相对较小的增益。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha8b2ouu2j31bq09qac3.jpg" srcset="/img/loading.gif" alt="image-20200731162120493"></p><blockquote><p><strong>表4：ILSVRC2013消融研究</strong>，不同的数据使用选择，微调，和边界回归</p></blockquote><h2 id="4-6-与OverFeat的关系"><a href="#4-6-与OverFeat的关系" class="headerlink" title="4.6 与OverFeat的关系"></a>4.6 与OverFeat的关系</h2><p>在R-CNN和OverFeat之间有一个有趣的关系：OverFeat可以看成是R-CNN的一个特殊的例子。如果要用正规的正方形区域的多尺度金字塔替换选择搜索候选区域，并且将每类的边界框回归改成单一的边界框回归，则系统则会非常类似（以某些潜在的重要的不同为模，如他们是怎么训练的：CNN检测微调，使用SVM等）。值得注意的是OverFeat比R-CNN具有显著的速度优势：基于[34]引用的每个图像2秒的数字，它快了大约9倍。该速度来自于OverFeat的滑动窗口（即候选区域）在图像不级别不扭曲的事实，因此在重叠窗口之间可以容易的共享计算。通过在任意大小的输入上以卷积的方式运行整个网络来实现共享。加快R-CNN应该可以以各种方法，并且作为未来的工作。</p><h1 id="5-语义分割"><a href="#5-语义分割" class="headerlink" title="5.语义分割"></a>5.语义分割</h1><p>区域分类是用于语义分割的标准技术，允许我们容易地将R-CNN应用于PASCAL VOC 分割挑战中。为了促进与当前领先的语义分割系统（称为O2P二阶池化）的直接比较[4]，我们研究了他们的开源框架。O2P使用CPMC为每个图像产生了150个候选区域，然后使用支持向量回归（SVR）为每个类预测每个区域的质量。他们的方法的高性能是由于CPMC区域的质量和多个特征类型的强大的二级池化（丰富的SIFT和LBP变体）。我们也注意到Farabet等人使用一个CNN作为多尺度每像素分类器证明了良好的结果。我们遵循[2,4]并扩展PASCAL分割训练集以包括由Hariharan等人提供的额外注释。设计决策和超参数在VOC 2011验证集上进行了交叉验证。最终测试结果仅评估一次。 　　</p><p>用于分割的CNN特征。我们评估了三种计算CPMC区域特征的策略，所有的这些策略都是通过将该区域周围的矩形窗口扭曲成227227来开始的。第一个策略（full）忽略了区域的形状，并且直接在扭曲的窗口上计算CNN特征，正如我们为检测做的一样。然而，这些特征忽略了区域的非矩形形状。两个区域可能有非常相似的边界框，然而几乎没有重叠。因此，第二种策略（fg）仅仅在区域的前景掩码上计算CNN特征。我们用平均输入替换背景，使得背景区域在减去平均值后为零。第三种策略（full+fg）仅仅简单地连接了full和fg特征；我们的实验结果验证了他们的互补性。 　　在VOC 2011上的结果。*表5显示了我们对VOC 2011验证集与O2P相比的结果的总结。（完整的每个类别的结果请看附录E）在每个特征计算策略中，fc6总是胜过fc7，并且以下的讨论都涉及fc6的特征。fg策略稍稍优于full策略，表明掩蔽区域形状提供更强的信号，匹配我们的直觉。然而，full+fg策略实现了47.9%的平均准确度，我们最好的结果是4.2%的边界（也适度超过了O2P），表明即使给定fg特征，由full特征提供的上下文也是有高度信息的。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha8f60itdj30lq03qdga.jpg" srcset="/img/loading.gif" alt="image-20200731162516907" style="zoom:50%;" /></center><blockquote><p>表5：在VOC 2011验证集上的分割平均准确性（%）。第一列展示了O2P；2-7列使用了我们在ILSVRC2012上预训练的CNN。</p></blockquote><p>在表6中，我们展示了在VOC 2011测试集上的结果，将我们的表现最好的方法，fc6(full+fg)，与两个强基准线进行了对比。我们的方法在21个类别中的11个达到最高的分割精度，并且达到了最高的总体分割精度47.9%，在所有类别中平均得来（但是在任何合理的误差范围内可能与O2P结果相关）。通过微调可能实现更好的性能。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gha8i9f4ftj31ca058q4m.jpg" srcset="/img/loading.gif" alt="image-20200731162815321"></p><blockquote><p>表6：在VOC2011测试集上的分割准确性（%）。我们对比了两个强大的基线：[2]中的“Region and Parts”（R&amp;P）方法，和[4]中的second-order pooling（O2P）方法。没有任何微调，我们的CNN实现了最好的分割性能，优于P&amp;R并且大致匹配了O2P。</p></blockquote><h1 id="6-结论"><a href="#6-结论" class="headerlink" title="6.结论"></a>6.结论</h1><p>在最近几年，目标检测性能停滞不前。最好的系统是将多个低级图像特征与来自于目标检测器和场景分类器的高级上下文组合在一起的复杂集合。本文提出了一种简单的和可扩展的对象检测算法，与PASCAL VOC2012上的最佳以前的结果相比提供了30%的相对改进。<br>　　我们通过两个见解实现了这一表现。第一个是将高容量的卷积神经网络应用于自下而上的候选区域以便定位和分割对象。第二个就是当标记的训练数据不足时训练大的CNN的范例。我们表明，对于一个具有丰富数据集（图像分类）的辅助任务，带监督的预训练网络，并且为数据稀缺的（检测）的目标任务微调网络是非常有效的。我们推测，“监督的预训练/特定领域的微调”范例将对于各种数据稀缺的视觉问题都非常有效。<br>　　我们最后指出，我们通过使用计算机视觉和深度学习的经典工具（从底向上的候选区域和卷积神经网络）的组合实现了这些结果是非常重要的。这两者不是反对科学家探究的线，而是自然的和不可避免的伙伴。</p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="A-目标建议转换"><a href="#A-目标建议转换" class="headerlink" title="A.目标建议转换"></a>A.目标建议转换</h2><p>这项工作中用到的卷积神经网络需要一个固定的227∗227的像素。对于检测来说，我们考虑到候选对象是任意大小的图像矩形。我们评估了两种用于将候选目标转换成一个有效的CNN输入的方法。<br>　　第一个方法（“具有上下文的最紧密方框”）将每个候选对象包围在最紧凑的正方形内，然后将包含在该正方形中的图像（各向同性）绽放为CNN的输入大小。 图7的B列展示了这种改变。此方法的一个变种（“没有上下文的最紧凑的方形”）排除了原始候选对象周围的图像内容。图7的（C）列展示了这种改变。第二种方法（“warp”）将每个候选对象各向异性地缩放为CNN输入大小。图7的（D）列展示了这种扭曲转换。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahfbaof0j30ns0ecwss.jpg" srcset="/img/loading.gif" alt="image-20200731213649238"></p><blockquote><p><strong>图7：不同的候选对象转换。</strong>（A）相对于经变换的CNN的输入的具有实际大小的原始候选对象；（B）最紧密的带有上下文的方格；（C）不带有上下文的最紧密的方框；（D）扭曲；在每一列和候选示例中，顶行对应于上下文填充的p=0个像素，而底行具有p=16个上下文填充的像素。</p></blockquote><p>对于这些变换中的每一个，我们还考虑在原始的候选对象周围包括上额外的图像上下文。上下文填充量（p）被定义为在经过变换了的输入坐标系中原始候选对象周围的边界大小。图7展示了在每个示例的顶行p=0并且在底行p=16。在所有方法中，如果源矩阵扩展延伸超过了图像，则丢失的数据就由图像均值替代（然后在将图像输入到CNN之前被减去）。一组实验表明，使用上下文填充（p=16像素）的扭曲优于替代方案很多（3-5mAP）。显然，更多的选择也是有可能的，包括使用复制而不是平均值进行填充。对这些替代品的详尽评价留作未来的工作。</p><h2 id="B-正例对负例和softmax"><a href="#B-正例对负例和softmax" class="headerlink" title="B 正例对负例和softmax"></a>B 正例对负例和softmax</h2><p>两种设计选择值得进一步的讨论。为什么为微调CNN和训练对象检测SVM时定义的正例和负例是不同的？简单回顾一下定义，对于微调，我们将每一个候选对象映射到它具有最大的IoU重叠（如果有）真实实例上，并且如果IoU至少为0.5，就将它标记为匹配的真实类的正例。所有其它的候选区域都被标记为“背景”（即所有类别的负例）。相反的，对于训练SVMs，我们只把真实框作为它们各自类别和候选标签的正例，与一个类的所有实例的IoU少于0.3的作为该类的负例。落入灰色区域（超过0.3IoU重叠，但不是真实值）的候选区域被忽略。<br>　　从历史上来说，我们得出这些定义，因为我们开始通过由ImageNet预训练的CNN计算的特征训练SVM，因此在当时没有考虑微调。在该设置中，我们发现我们的用于训练SVM的特定的标签定义在我们评估的选项集（其中包括我们现在用于微调的配置）中是最佳的。当我们开始使用微调时，我们使用与SVM训练时使用的相同的正例和负例。然而，我们发现结果比使用我们现在的正例和负例的定义要差的多。<br>　　我们的假设是在正例和负例如何定义上面的不同并不是根本重要的，而是来自于微调数据有限这个事实。我们目前的方案引入了许多“抖动”方案（这些候选值具有0.5到1之间的重叠，但不是真实值），这将正例的数量扩展了大约30倍。我们推测当微调整个网络而避免过拟合的话，是需要这个大的集合的。然而，我们也注意到使用这些抖动样例也可能是次优的，因为这些网络没有微调以精确定位。<br>　　这也导致了第二个问题，为什么微调后训练SVM？简单地应用微调网络的最后一层（这是一种21路的softmax回归分类器）作为对象检测器是非常清晰的。我们尝试这么做了，发现在VOC 2007上的性能从54.2%mAP降到50.9%mAP。这种性能的下降可能是由于几种因素的结合，包括在微调中使用的微调的定义不强调精确定位，并且训练softmax分类器是在随机采样的负例上进行的，而不是在用于SVM训练使用的“hard negative”的子集中进行的。<br>　　该结果表明，在微调之后不训练SVM可能会获得接近相同水平的性能。我们推测通过一些额外的对微调的调整，剩余的表现差距可能会被关闭。如果是真的话，这将简化和加速R-CNN训练，而在检测性能上不会有损失。</p><h2 id="c-边界框回归"><a href="#c-边界框回归" class="headerlink" title="c.边界框回归"></a>c.边界框回归</h2><p>我们用一个简单的边界框回归阶段来提高定位性能。在使用一个特定类检测的SVM对每个选择性搜索候选值进行评分之后，我们使用特定类的边界框回归来预测用于检测的新边界框。这类似于在可变形部分模型[17]中使用的边界框回归。这两种方法主要的不同是，这里我们从CNN计算的特征，而不是从推断的DPM部分位置计算的几何特征回归。 　　我们训练算法的输入是一个N个训练对的集合(Pi,Gi)i=1,…,N，其中Pi=(Pix,Piy,Piw,Pih)指的是候选Pi的边界框的中心和Pi的宽和高的坐标。因此接下来，我们删除上标i，除非它是必须的。每一个真实的边界框G是以相同的方式指定的：G=(Gx,Gy,Gw,Gh)。我们的目标就是学习一个转换，其将一个候选框P映射到真实框G上。 　　我们根据四个函数（dx(P),dy(P),dw(P),dh(P)）来对这种转换进行参数化。前两个指的是P的边界框的中心的尺度不变的转换，后两个指定P的边界框的宽度和高度的对数空间平移。在学习了这些函数之后，我们可以通过应用这些转换将一个输入的候选P转换到一个预测的真实框G^。</p><p>Gx^=Pwdx(P)+Px　　　　(1)</p><p>Gx^=Pwdx(P)+Px　　　　(2)</p><p>Gx^=Pwdx(P)+Px　　　　(3)</p><p>Gx^=Pwdx(P)+Px　　　　(4)</p><p>每一个函数d∗(P)（∗是x,y,h,w中的一个）被建模成候选P的pool5特征的一个线性函数，标记成ϕ5(P)。隐含地假设ϕ5(P)对于图像数据是依赖的。因此我们有dx(P)=wT∗ϕ5(P)，其中w∗是一个可学习的模型的参数。我们通过优化正则化的最小二乘目标函数（岭回归）来学习w∗</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahil2jekj30n003mdg3.jpg" srcset="/img/loading.gif" alt="image-20200731213958131" style="zoom:33%;" /></p><p>对于训练对（P,G）的回归目标t∗定义如下：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahiyo9w4j30gg072q3h.jpg" srcset="/img/loading.gif" alt="image-20200731214019913" style="zoom: 50%;" /></p><p>作为一种标准的正则化最小二乘问题，这可以以封闭形式有效地解决。<br>　　我们在实现边界框回归的时候发现了两个微妙的问题。第一个就是正则化是非常重要的：基于验证集，我们设置λ=1000。第二个问题是，在选择使用哪个训练对（P；G）时必须小心。直观地，如果远离所有的真实框，那么将P转换到真实框G的任务就没有意义。使用像P这样的例子将会导致一个无望的学习问题。因此，我们只从这样的候选P中进行学习，其至少与一个真实框离的比较近。我们通过将P分配给真实框G，当前仅当重叠大于阈值（我们使用一个验证集设置成0.6）时，它与其具有最大的IoU重叠（以防重叠超过一个）。所有未分配的候选区域都被丢弃。对于每一个对象类我们只做一次，以便学习一组特定类边界框的回归器。<br>　　在测试的时候，我们为每一个候选框打分，并且预测一次它的新检测窗口。原则上来说，我们可以迭代这个过程（即，为新得到的预测框重新打分，然后从它再预测一个新的边界框，以此类推）。然而，我们发现迭代没有改善结果。</p><h2 id="D-附加的功能可视化"><a href="#D-附加的功能可视化" class="headerlink" title="D.附加的功能可视化"></a>D.附加的功能可视化</h2><p>图12为20个pool5单元展示了附加的可视化。对于每一个单元来说，我们展示了可以最大激活VOC 2007 测试集的全部的大约1000万个区域中的24个候选区域。我们在6∗6∗256维的pool5特征图上为每个单元都标记了它的（y,x,channel）位置。在每个通道内，CNN计算输入区域的完全相同的函数，</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghahko3v1nj30u010xhdu.jpg" srcset="/img/loading.gif" alt="image-20200731214157080"></p><blockquote><p>图12：我们展示了在VOC2007测试中大约1000万个候选区域中的24个候选区域，其最强烈地激活20个单元中的每一个。每个剪辑都用6∗6∗256&lt;维的pool5特征图的单元（y, x, channel）的位置标记。每一个图像区域都用白色的单元的接受场的覆盖图绘制。激活值（我们通过除以通道中所有单元的最大激活值来进行归一化）显示在接受场的左上角。建议放大来看</p></blockquote><h2 id="E-每个类的分类结果"><a href="#E-每个类的分类结果" class="headerlink" title="E 每个类的分类结果"></a>E 每个类的分类结果</h2><p>在表7中，我们展示了我们6个分割方法中的每一个（除了O2P方法）在VOC 2011val集上的每类分割准确度。这些结果展示了对于20个PASCAL类别加上背景类，哪一个方法是最强的。</p><h2 id="F-交叉数据集冗余分析"><a href="#F-交叉数据集冗余分析" class="headerlink" title="F 交叉数据集冗余分析"></a>F 交叉数据集冗余分析</h2><p>当在辅助数据集上进行训练时，一个问题是它与测试集之间可能存在冗余。即使对象检测和整个图像分类的任务有很大的不同，为了使这种交叉冗余不那么令人担忧，我们仍然进行了彻底的调查，量化了PASCAL测试图像包含在ILSVRC2012训练和验证集的程度。我们发现可能对那些有兴趣使用ILSVRC2012作为PASCAL图像分类任务的训练数据的研究人员有用。我们对重复（和近重复）图像执行了再次检查。第一个测试是基于flicker图像ID的精确匹配，这些ID包括在VOC 2007测试注释中（这些ID有意的为后续的PASCAL测试集保密）。所有的PASCAL图像，和约一半的ILSVRC图像，从flickr.com收集。这个检查证明了在4952有31个匹配（0.63%）。<br>　　第二个检测使用了GIST描述符匹配，在[13]中显示在大（&gt;100万）图像集合中的近似图像检测中具有优异的性能。在[13]之后，我们计算了所有的ILSVRC2012trainval和PASCAL 2007测试图像的扭曲32*32像素版本上的GIST描述符。GIST描述符的欧氏距离最近邻匹配揭示了38个近似重复图像（包括通过flickrID匹配找到的31个）。匹配在JPEG压缩级别和分辨率略有变化，并且趋向较小程度的裁剪。这些发现表明重叠是小的，小于1%。对于VOC 2012来说，因为flickrID是不可用的，我们只使用了GIST匹配方法。基于GIST匹配，VOC 2012测试图像的1.5%是在ILSVRC 2012trainval中的。对于VOC 2012略高的比率可能是由于这两个数据集在时间上收集的比VOC 2007和ILSVRC 2012更接近。</p><blockquote><p>本文转自：<a href="https://www.jianshu.com/p/5c3eac760077" target="_blank" rel="noopener">https://www.jianshu.com/p/5c3eac760077</a></p><p>作者：苟且偷生小屁屁</p><p>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p></blockquote>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>R-CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pytorch学习笔记</title>
    <link href="/2020/07/28/pytorch-note/"/>
    <url>/2020/07/28/pytorch-note/</url>
    
    <content type="html"><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h1><h2 id="1-1-Pytorch-简介"><a href="#1-1-Pytorch-简介" class="headerlink" title="1.1 Pytorch 简介"></a>1.1 Pytorch 简介</h2><p>说到Pytorch首先要了解torch，Torch是一个与Numpy类似的张量(tensor)操作库,与Numpy不同的是Torch对GPU的支持很好，Lua是Torch的上层包装。</p><p>PyTorch和Torch使用包含所有相同性能的C库：TH, THC, THNN, THCUNN，并且它们将继续共享这些库。<strong>PyTorch和Torch都使用的是相同的底层，只是使用了不同的上层包装语言。</strong></p><p>PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。 它主要由Facebook的人工智能研究小组开发。</p><p>总结：Pytorch是一个Python包，提供两个高级功能：</p><ul><li>具有强大的GPU加速的张量计算（如NumPy)</li><li>包含自动求导系统的深度神经网络</li></ul><p>此外，具有以下特点：</p><ul><li>类似于Numpy，但可以使用GPU。</li><li>可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用</li></ul><p>此外具有如下优点：</p><ul><li>简洁、优雅、高效、快速</li><li>设计追求最少的封装，避免重复造轮子</li><li>符合人们的思维，让用户专注实现自己的想法</li><li>良好的FAIR团队支持，确保Pytorch获得持续的开发更新</li><li>文档完善，Pytorch的作者亲自维护论坛</li><li>入门简单</li></ul><h2 id="1-2-环境搭建"><a href="#1-2-环境搭建" class="headerlink" title="1.2 环境搭建"></a>1.2 环境搭建</h2><p>笔者使用的操作系统为MAC OS Catelina 10.15.5，conda 4.8.3。可在<a href="https://links.jianshu.com/go?to=https%3A%2F%2Fpytorch.org" target="_blank" rel="noopener">官网</a>选择环境，复制生成的命令安装即可。由于MAC不支持CUDA，我安装的是torch-1.5.1不含CUDA。</p><p>使用以下命令安装：</p><p><code>pip install torch torchvision</code></p><p>由于在安装过程中下载包的速度较慢，故将pip换成清华源：</p><pre><code class="hljs vim">pip install pip -Upip config <span class="hljs-keyword">set</span> <span class="hljs-keyword">global</span>.<span class="hljs-built_in">index</span>-url http<span class="hljs-variable">s:</span>//pypi.tuna.tsinghua.edu.<span class="hljs-keyword">cn</span>/simple</code></pre><p>验证是否安装成功：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchtorch.__version__<span class="hljs-comment">#得到结果：1.5.1</span></code></pre><p>安装Jupyter:</p><p><code>pip3 install --user jupyer</code></p><p>在当前目录打开Jupyter：</p><p><code>python -m IPython notebook</code></p><h2 id="1-3-Pytorch深度学习：60分钟快速入门（官方）"><a href="#1-3-Pytorch深度学习：60分钟快速入门（官方）" class="headerlink" title="1.3 Pytorch深度学习：60分钟快速入门（官方）"></a>1.3 Pytorch深度学习：60分钟快速入门（官方）</h2><h3 id="1-3-1张量"><a href="#1-3-1张量" class="headerlink" title="1.3.1张量"></a>1.3.1张量</h3><pre><code class="hljs python"><span class="hljs-keyword">from</span> __future__ <span class="hljs-keyword">import</span> print_function<span class="hljs-keyword">import</span> torch</code></pre><p>创建未初始化的矩阵：<code>x = torch.empty(5,3)</code></p><p>创建随机初始化的矩阵：<code>x = torch.rand(5,3)</code></p><p>创建0填充的矩阵，数据类型为long: <code>x = torch.zeros(5, 3, dtype=torch.long)</code></p><p>创建tensor并初始化: <code>x = torch.tensor([5.5, 3])</code></p><p>根据现有张量创建张量。将重用原张量的属性，除非设置新的值进行覆盖：</p><pre><code class="hljs python">x = x.new_ones(<span class="hljs-number">5</span>, <span class="hljs-number">3</span>, dtype=torch.double)x = torch.randn_like(x, dtype=torch.float) <span class="hljs-comment">#随机产生一个与x形状相同的tensor</span></code></pre><p>获取尺寸：<code>print(x.size())</code></p><p>加法运算：</p><p><code>x+y,  torch.add(x, y, out=result),  y.add_(x)</code>  <strong>（任何以_结尾的操作都会替换原变量）</strong></p><p>改变张量的维度和大小：torch.view()</p><pre><code class="hljs python">x = torch.randn(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)y = x.view(<span class="hljs-number">16</span>)z = x.view(<span class="hljs-number">-1</span>, <span class="hljs-number">8</span>)  <span class="hljs-comment">#  size为-1 从其他维度推断</span>print(x.size(), y.size(), z.size())<span class="hljs-comment">#torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])</span></code></pre><p>只有一个元素的张量使用<code>x.item()</code>得到Python数据类型的值</p><p><strong>Numpy与Torch Tensor相互转换：</strong></p><p>Torch Tensor与Numpy数组共享低层内存地址，修改一个会导致另一个的变化。</p><p>Torch Tensor转换为Numpy数组：</p><pre><code class="hljs python">a = torch.ones(<span class="hljs-number">5</span>)  <span class="hljs-comment">#tensor([1., 1., 1., 1., 1.])</span>b = a.numpy() <span class="hljs-comment">#[1. 1. 1. 1. 1.]</span></code></pre><p>Numpy数组转换为Torch Tensor:</p><pre><code class="hljs python">a = np.ones(<span class="hljs-number">5</span>)b = torch.from_numpy(a)</code></pre><h3 id="1-3-2-Autograd：自动求导"><a href="#1-3-2-Autograd：自动求导" class="headerlink" title="1.3.2 Autograd：自动求导"></a>1.3.2 Autograd：自动求导</h3><p>PyTorch 中所有神经网络的核心是 <code>autograd</code> 包。 </p><p><code>autograd</code>包为张量上的所有操作提供了自动求导。 它是一个在运行时定义的框架，这意味着反向传播是根据你的代码来确定如何运行，并且每次迭代可以是不同的。</p><h4 id="张量（Tensor）"><a href="#张量（Tensor）" class="headerlink" title="张量（Tensor）"></a>张量（Tensor）</h4><p><code>torch.Tensor</code>是这个包的核心类。如果设置 <code>.requires_grad</code> 为 <code>True</code>，那么将会追踪所有对于该张量的操作。 当完成计算后通过调用 <code>.backward()</code>，自动计算所有的梯度， 这个张量的所有梯度将会自动积累到 <code>.grad</code> 属性。</p><p>要阻止张量跟踪历史记录，可以调用<code>.detach()</code>方法将其与计算历史记录分离，并禁止跟踪它将来的计算记录。</p><p>为了防止跟踪历史记录（和使用内存），可以将代码块包装在<code>with torch.no_grad()：</code>中。 在评估模型时特别有用，因为模型可能具有<code>requires_grad = True</code>的可训练参数，但是我们不需要梯度计算。<code>.requires_grad_( ... )</code> 可以改变现有张量的 <code>requires_grad</code>属性。 如果没有指定的话，默认输入的flag是 <code>False</code>。</p><p>在自动梯度计算中还有另外一个重要的类<code>Function</code>.</p><p><code>Tensor</code> 和 <code>Function</code>互相连接并生成一个非循环图，它表示和存储了完整的计算历史。 每个张量都有一个<strong><code>.grad_fn</code>属性</strong>，这个属性引用了一个创建了<code>Tensor</code>的<code>Function</code>（除非这个张量是用户手动创建的，即，这个张量的 <code>grad_fn</code> 是 <code>None</code>）。</p><p>如果需要计算导数，你可以在<code>Tensor</code>上调用<code>.backward()</code>。 如果<code>Tensor</code>是一个标量（即它包含一个元素数据）则不需要为<code>backward()</code>指定任何参数， 但是如果它有更多的元素，你需要指定一个<code>gradient</code> 参数来匹配张量的形状。</p><h3 id="1-3-3-神经网络"><a href="#1-3-3-神经网络" class="headerlink" title="1.3.3 神经网络"></a>1.3.3 神经网络</h3><p>使用torch.nn包来构建神经网络。</p><p>上一讲已经讲过了<code>autograd</code>，<code>nn</code>包依赖<code>autograd</code>包来定义模型并求导。 一个<code>nn.Module</code>包含各个层和一个<code>forward(input)</code>方法，该方法返回<code>output</code>。</p><p>它是一个简单的前馈神经网络，它接受一个输入，然后一层接着一层地传递，最后输出计算的结果。</p><p>神经网络的典型训练过程如下：</p><ol><li>定义包含一些可学习的参数(或者叫权重)神经网络模型； </li><li>在数据集上迭代； </li><li>通过神经网络处理输入； </li><li>计算损失(输出结果和正确值的差值大小)；</li><li>将梯度反向传播回网络的参数； </li><li>更新网络的参数，主要使用如下简单的更新原则： <code>weight = weight - learning_rate * gradient</code></li></ol><p>在模型中必须要定义 <code>forward</code> 函数，<code>backward</code> 函数（用来计算梯度）会被<code>autograd</code>自动创建。 可以在 <code>forward</code> 函数中使用任何针对 Tensor 的操作。</p><p> <code>net.parameters()</code>返回可被学习的参数（权重）列表和值</p><p><strong>回顾:</strong></p><ul><li><code>torch.Tensor</code>：一个用过自动调用 <code>backward()</code>实现支持自动梯度计算的 <em>多维数组</em> ， 并且保存关于这个向量的<em>梯度</em> w.r.t.</li><li><code>nn.Module</code>：神经网络模块。封装参数、移动到GPU上运行、导出、加载等。</li><li><code>nn.Parameter</code>：一种变量，当把它赋值给一个<code>Module</code>时，被 <em>自动</em> 地注册为一个参数。</li><li><code>autograd.Function</code>：实现一个自动求导操作的前向和反向定义，每个变量操作至少创建一个函数节点，每一个<code>Tensor</code>的操作都回创建一个接到创建<code>Tensor</code>和 <em>编码其历史</em> 的函数的<code>Function</code>节点。</li></ul><p><strong>重点如下：</strong></p><ul><li>定义一个网络</li><li>处理输入，调用backword</li></ul><p><strong>还剩：</strong></p><ul><li>计算损失</li><li>更新网络权重</li></ul><p>一个损失函数接受一对 (output, target) 作为输入，计算一个值来估计网络的输出和目标值相差多少。</p><p><strong>反向传播</strong></p><p>调用loss.backward()获得反向传播的误差。</p><p>但是在调用前需要清除已存在的梯度，否则梯度将被累加到已存在的梯度。</p><p><strong>更新权重</strong></p><p>在实践中最简单的权重更新规则是随机梯度下降（SGD）：</p><p> weight = weight - learning_rate * gradient</p><p>当使用神经网络是想要使用各种不同的更新规则时，比如SGD、Nesterov-SGD、Adam、RMSPROP等，PyTorch中构建了一个包<code>torch.optim</code>实现了所有的这些规则。 使用它们非常简单：</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<span class="hljs-comment">#create your optimizer</span>optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.01</span>)<span class="hljs-comment">#in your training loop:</span>optimizer.zero_grad()   <span class="hljs-comment"># zero the gradient buffers</span>output = net(input)loss = criterion(output, target)loss.backward()optimizer.step()    <span class="hljs-comment"># Does the update</span></code></pre><h3 id="1-3-4-训练一个分类器"><a href="#1-3-4-训练一个分类器" class="headerlink" title="1.3.4 训练一个分类器"></a>1.3.4 训练一个分类器</h3><h4 id="关于数据？"><a href="#关于数据？" class="headerlink" title="关于数据？"></a>关于数据？</h4><p>一般情况下处理图像、文本、音频和视频数据时，可以使用标准的Python包来加载数据到一个numpy数组中。 然后把这个数组转换成 <code>torch.*Tensor</code>。</p><ul><li>图像可以使用 Pillow, OpenCV</li><li>音频可以使用 scipy, librosa</li><li>文本可以使用原始Python和Cython来加载，或者使用 NLTK或 SpaCy 处理</li></ul><p>特别的，对于图像任务，我们创建了一个包 <code>torchvision</code>，它包含了处理一些基本图像数据集的方法。这些数据集包括 Imagenet, CIFAR10, MNIST 等。除了数据加载以外，<code>torchvision</code> 还包含了图像转换器， <code>torchvision.datasets</code> 和 <code>torch.utils.data.DataLoader</code>。</p><p><code>torchvision</code>包不仅提供了巨大的便利，也避免了代码的重复。</p><p>在这个教程中，我们使用CIFAR10数据集，它有如下10个类别 ：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’。CIFAR-10的图像都是 3x32x32大小的，即，3颜色通道，32x32像素。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh86lpcwqfj30d40a940j.jpg" srcset="/img/loading.gif" alt=""></p><h4 id="训练一个图像分类器"><a href="#训练一个图像分类器" class="headerlink" title="训练一个图像分类器"></a>训练一个图像分类器</h4><p>依次按照下列顺序进行：</p><ol><li>使用<code>torchvision</code>加载和归一化CIFAR10训练集和测试集</li><li>定义一个卷积神经网络</li><li>定义损失函数</li><li>在训练集上训练网络</li><li>在测试集上测试网络</li></ol><ol><li>读取和归一化 CIFAR10</li></ol><p>使用<code>torchvision</code>可以非常容易地加载CIFAR10。</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<span class="hljs-keyword">import</span> torchvision<span class="hljs-keyword">import</span> torchvision.transforms <span class="hljs-keyword">as</span> transforms</code></pre><p>torchvision的输出是[0,1]的PILImage图像，我们把它转换为归一化范围为[-1, 1]的张量。</p><pre><code class="hljs python">transform = transforms.Compose(    [transforms.ToTensor(),     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])trainset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">'./data'</span>, train=<span class="hljs-literal">True</span>,                                        download=<span class="hljs-literal">True</span>, transform=transform)trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="hljs-number">4</span>,                                          shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">2</span>)testset = torchvision.datasets.CIFAR10(root=<span class="hljs-string">'./data'</span>, train=<span class="hljs-literal">False</span>,                                       download=<span class="hljs-literal">True</span>, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="hljs-number">4</span>,                                         shuffle=<span class="hljs-literal">False</span>, num_workers=<span class="hljs-number">2</span>)classes = (<span class="hljs-string">'plane'</span>, <span class="hljs-string">'car'</span>, <span class="hljs-string">'bird'</span>, <span class="hljs-string">'cat'</span>,           <span class="hljs-string">'deer'</span>, <span class="hljs-string">'dog'</span>, <span class="hljs-string">'frog'</span>, <span class="hljs-string">'horse'</span>, <span class="hljs-string">'ship'</span>, <span class="hljs-string">'truck'</span>)</code></pre><p>2.定义神经网络</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span><span class="hljs-params">(nn.Module)</span>:</span>    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>        super(Net, self).__init__()        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">5</span>)        self.pool = nn.MaxPool2d(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)        self.conv2 = nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, <span class="hljs-number">5</span>)        self.fc1 = nn.Linear(<span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>, <span class="hljs-number">120</span>)        self.fc2 = nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>)        self.fc3 = nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>)    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>        x = self.pool(F.relu(self.conv1(x)))        x = self.pool(F.relu(self.conv2(x)))        x = x.view(<span class="hljs-number">-1</span>, <span class="hljs-number">16</span> * <span class="hljs-number">5</span> * <span class="hljs-number">5</span>)        x = F.relu(self.fc1(x))        x = F.relu(self.fc2(x))        x = self.fc3(x)        <span class="hljs-keyword">return</span> xnet = Net()</code></pre><p>3.定义损失函数和优化器</p><p>我们使用交叉熵作为损失函数，使用带动量的随机梯度下降。</p><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optimcriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)</code></pre><p>4.训练网络</p><p>只需在数据迭代器上循环，将数据输入给网络，并优化。</p><pre><code class="hljs python"><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>):  <span class="hljs-comment"># 多批次循环</span>    running_loss = <span class="hljs-number">0.0</span>    <span class="hljs-keyword">for</span> i, data <span class="hljs-keyword">in</span> enumerate(trainloader, <span class="hljs-number">0</span>):        <span class="hljs-comment"># 获取输入</span>        inputs, labels = data        <span class="hljs-comment"># 梯度置0</span>        optimizer.zero_grad()        <span class="hljs-comment"># 正向传播，反向传播，优化</span>        outputs = net(inputs)        loss = criterion(outputs, labels)        loss.backward()        optimizer.step()        <span class="hljs-comment"># 打印状态信息</span>        running_loss += loss.item()        <span class="hljs-keyword">if</span> i % <span class="hljs-number">2000</span> == <span class="hljs-number">1999</span>:    <span class="hljs-comment"># 每2000批次打印一次</span>            print(<span class="hljs-string">'[%d, %5d] loss: %.3f'</span> %                  (epoch + <span class="hljs-number">1</span>, i + <span class="hljs-number">1</span>, running_loss / <span class="hljs-number">2000</span>))            running_loss = <span class="hljs-number">0.0</span>print(<span class="hljs-string">'Finished Training'</span>)</code></pre><p>5.在测试集上测试网络</p><p>我们在整个训练集上进行了2次训练，但是我们需要检查网络是否从数据集中学习到有用的东西。 通过预测神经网络输出的类别标签与实际情况标签进行对比来进行检测。 如果预测正确，我们把该样本添加到正确预测列表。 第一步，显示测试集中的图片并熟悉图片内容。</p><pre><code class="hljs python">dataiter = iter(testloader)images, labels = dataiter.next()<span class="hljs-comment"># 显示图片</span>imshow(torchvision.utils.make_grid(images))print(<span class="hljs-string">'GroundTruth: '</span>, <span class="hljs-string">' '</span>.join(<span class="hljs-string">'%5s'</span> % classes[labels[j]] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(<span class="hljs-number">4</span>)))</code></pre><p><strong>在GPU上训练</strong></p><p>把一个神经网络移动到GPU上训练就像把一个Tensor转换GPU上一样简单。并且这个操作会递归遍历有所模块，并将其参数和缓冲区转换为CUDA张量。</p><pre><code class="hljs python">device = torch.device(<span class="hljs-string">"cuda:0"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<span class="hljs-comment"># 确认我们的电脑支持CUDA，然后显示CUDA信息：</span>print(device)</code></pre><h3 id="1-3-5-数据并行"><a href="#1-3-5-数据并行" class="headerlink" title="1.3.5 数据并行"></a>1.3.5 数据并行</h3><p>在这个教程里，我们将学习如何使用 <code>DataParallel</code> 来使用多GPU。</p><p>PyTorch非常容易就可以使用多GPU，用如下方式把一个模型放到GPU上：</p><pre><code class="hljs abnf"><span class="hljs-attribute">device</span> = torch.device(<span class="hljs-string">"cuda:0"</span>)model.to(device)</code></pre><p>GPU: 然后复制所有的张量到GPU上：</p><pre><code class="hljs ini"><span class="hljs-attr">mytensor</span> = my_tensor.to(device)</code></pre><p>请注意，只调用<code>my_tensor.to(device)</code>并没有复制张量到GPU上，而是返回了一个copy。所以你需要把它赋值给一个新的张量并在GPU上使用这个张量。</p><p>在多GPU上执行前向和反向传播是自然而然的事。 但是PyTorch默认将只使用一个GPU。</p><p>使用<code>DataParallel</code>可以轻易的让模型并行运行在多个GPU上。</p><pre><code class="hljs gams"><span class="hljs-keyword">model</span> = nn.DataParallel(<span class="hljs-keyword">model</span>)</code></pre><p>DataParallel会自动的划分数据，并将作业发送到多个GPU上的多个模型。 并在每个模型完成作业后，收集合并结果并返回。</p>]]></content>
    
    
    <categories>
      
      <category>学习笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Pytorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>YOLO系列总结 （7.24 阅读笔记）</title>
    <link href="/2020/07/25/7-24/"/>
    <url>/2020/07/25/7-24/</url>
    
    <content type="html"><![CDATA[<p>最近研读了YOLO系列的4篇论文，并加以对比分析。</p><p>[1].Redmon J , Divvala S , Girshick R , et al. You Only Look Once: Unified, Real-Time Object Detection[J]. 2015.</p><p>[2]. Redmon J, Farhadi A. YOLO9000: better, faster, stronger[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 7263-7271.</p><p>[3] Redmon J, Farhadi A. Yolov3: An incremental improvement[J]. arXiv preprint arXiv:1804.02767, 2018.</p><p>[4] Bochkovskiy, A., Wang, C. Y., &amp; Liao, H. Y. M. (2020). YOLOv4: Optimal Speed and Accuracy of Object Detection. arXiv preprint arXiv:2004.10934.</p><p>总的来说YOLO是将目标检测问题当成回归来做，而Fast-CNN是将目标检测问题转换为分类。</p><h1 id="1-YOLOv1："><a href="#1-YOLOv1：" class="headerlink" title="1.   YOLOv1："></a><strong>1.</strong>   <strong>YOLOv1</strong>：</h1><p>处理图像的流程：首先将输入图像调整到448x448大小，然后将图像送入卷积神经网络，并对预测结果执行非极大值抑制算法。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3moqzft7j31510hvdk6.jpg" srcset="/img/loading.gif" alt="img"></p><p>YOLOv1的神经网络输入为448x448x3,经过24个卷积层和2个全连接层，输出为7x7x30，其中7x7为将图像分为7x7的网格，每个网格负责检测中心落在该网格中的物体。30可以拆分为2x5+20，每个网格预测2个bounding box，每个bounding box预测5个参数，分别为：x,y,w,h,confidence:</p><p>(1)   (x,y)坐标为bounding box 的中心相对于 grid cell 左上角点的相对位置，归一化为[0,1]。</p><p>(2)   w,h为边界框相对于整个图像的宽和高，也被归一化为[0,1]。此处预测的均为相对位置，而非绝对像素位置，可以避免图像resize后产生错误。</p><p>(3)   Confidence定义为Pr(Object) * IOU。如果ground truth落在这个grid cell里，那么Pr（Object）就取1，否则就是0，IOU就是bounding box与实际的ground truth之间的交并比。confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息</p><p>而20对应的是类别数，原文是基于Pascal Voc数据集训练的，因此类别数为20。为每个类预测一个条件类别概率：Pr(Classi|Object)，即在一个网格包含一个Object的前提下，它属于某个类的概率。只为每个网格预测一组（C个）类概率，而不考虑边界框B的数量。</p><p> 注意：类别信息是针对每个网格的，而confidence是针对每个bounding box的</p><p>在测试时将每个网格的条件类概率和和每个边界框的置信度预测相乘，这既包含了该类出现在框中的概率又包含了预测框拟合目标的程度。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3mon9kazj30u202g74e.jpg" srcset="/img/loading.gif" alt="image-20200702092439875" style="zoom:50%;" /></center><p>损失函数设计：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3mop6f53j30qg0gkmy4.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>(1)   前两行为坐标损失，对于包含了目标中心点的网格，需要计算coordinate损失。x,y坐标通过计算相对于网格左上角的坐标可以归一化到0-1之间，w,h通过图像的宽和高也归一化到0-1之间。每个网格会预测两个bounding box，训练的时会选择与ground truth box的IOU最大的那个bounding box进行训练，这样做的好处是随着训练的逐步深入，不同的bounding box会逐渐适应特定尺寸的物体（可以从两个bounding box的预测中择优）。</p><p>w,h的计算进行开根号，这是作者人为添加的，因为大的bounding box的损失会超过小的bounding box的损失，平方根有助于减少二者之间的差距。同样的，为了抵制过多的不含物体的网格confidence损失的影响力，避免真正有意义的、包含物体的bounding box信息被湮没，加入了一个系数λcoord=5进行加强。</p><p>(2)   第三、四行为置信度损失，置信度损失是区分正负样本的关键。其分成grid cell包含与不包含object两种情况。对于包含物体中心点的网格来说，confidence包含了两层信息：所预测的bounding box中含有物体的confidence和这个bounding box预测的有多准。在一张图像中大多数的网格都不包含物体，使得这些网格中confidence的损失趋向于0，这将极大的削弱真正包含物体中心点的网格预测的confidence的影响力，因此加入了λnoobj=0.5进行弱化。</p><p>(3)   第五行为预测类别误差，前面的系数只有在grid cell包含object的时候才为1。</p><h2 id="总结："><a href="#总结：" class="headerlink" title="总结："></a><strong>总结：</strong></h2><p>1.优点：</p><p>(1)   YOLOv1将目标检测问题转换为回归问题，采用端到端的网络架构，易于进行模型构建和训练。</p><p>(2)   检测速度快。每秒可以实时处理45帧图像。较小的网络Fast YOLO，其处理能力达到155帧/秒，精度实现了两倍于当时其他实时检测网络的mAP。</p><p>(3)   背景误检率低。YOLOv1在预测时关注整张图像，联系上下文信息，将背景检测为目标的可能性更小。与Fast-R-CNN相比，YOLOv1产生的背景误检率少于一半。</p><p>(4)   通用性强。YOLO可以学习物体的泛化表示，在迁移到其他领域如艺术领域时有着更好的表现</p><p>2.缺点</p><p>(1)   对成群的小目标识别效果不好。YOLOv1在边界框预测上实加了强大的空间约束，因为每个网格单元只预测两个bounding box，并且必须属于同一个类。这个空间约束限制了能够预测附近物体的数量。</p><p>(2)   YOLOv1直接从数据中学习预测bounding box，因此很难在新的或不寻常的宽高比的对象中进行泛化。</p><p>(3)   定位误差大。yolov1的损失函数没有处理好小的bounding box和大的bounding box之间的区别。因为大的bounding box的小误差通常是良性的，但小的bounding box的小误差对IOU的影响要大得多。</p><h1 id="2-YOLOv2"><a href="#2-YOLOv2" class="headerlink" title="2.   YOLOv2"></a><strong>2.</strong>   <strong>YOLOv2</strong></h1><p>建立在YOLOv1的基础上，经过Joseph Redmon等的改进，YOLOv2和YOLO9000算法在2017年CVPR上被提出，重点解决YOLOv1召回率和定位精度方面的误差。相比v1提高了训练图像的分辨率；引入了Faster R-CNN中anchor box的思想，对网络结构的设计进行了改进，输出层使用卷积层替代全连接层。</p><p>YOLOv2提出了一种联合目标检测和分类的训练方法。使用这种方法，在COCO检测数据集和ImageNet分类数据集上同时训练YOLO9000，可以实时预测9000多种不同对象类别的检测结果。文章实际提出了两个模型：YOLOv2和YOLO9000。后者在前者基础上提出，主体结构是一致的，区别是YOLO9000的每个单元格只采用3个Anchor Boxes而不是5个。</p><p>通常检测数据集较少而分类数据集较多，同时为构建并标记检测数据集的开销比分类数据集更大。因此可以利用带标注的分类数据集量比较大的特点，解决带标注的检测数据集量比较少的问题。具体方法是：一方面采用WordTree融合数据集，另一方面联合训练分类数据集和检测数据集。分类信息学习自ImageNet分类数据集，而物体位置检测则学习自 COCO 检测数据集。</p><p>检测数据集只有通用目标和通用标签，如“狗”或“船”。分类数据集具有更广更深的标签范围，比如“狗”就包括各个品种的狗。大多数分类方法使用softmax层来计算概率分布，而softmax假定类之间是互相独立的。所以如果想同时在检测数据集与分类数据集上进行训练，就需要用一种一致性的方法融合这些标签信息。</p><p>文章提出了WordTree，一个视觉概念的分层模型，可以将两种数据集按照层级进行结合。ImageNet标签是从WordNet中提取的，采用以下策略构建WordTree：遍历ImageNet中的视觉名词，并查看它们通过WordNet图到根节点的路径（根节点是“physical object”）。如果到根节点的路径只有一条，将该路径直接加入到WordTree结构中。否则，选择一条最短路径，加入到WordTree结构中。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3moskptqj30u00v9k8w.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>这样，在WordTree的某些节点上就可以计算一些条件概率，例如在terrier节点可以预测：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3mtkqlysj30b8046jtb.jpg" srcset="/img/loading.gif" alt="image-20200725232434308" style="zoom: 50%;" /></center><p>如果想要计算一个特定节点的绝对概率，只需沿着通过树到达根节点的路径，再乘以条件概率。所以如果想知道一张图片是否是Norfolk terrier，则可计算：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3moos95lj30hj06njs3.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>YOLO与Fast R-CNN相比有大量的定位误差。此外，与基于区域候选网络（RPN）相比，YOLO召回率相对较低。因此，YOLOv2在保持分类准确性的同时主要侧重于提高召回率和改进定位。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3moryo6nj30uk0ckdhi.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>改进一：添加了Batch Normalization层。</strong>Batch Normalization可显著提高收敛性，同时消除了对其他形式正则化的依赖。通过在YOLO的所有卷积层上添加批标准化，使得mAP提升了2%。同时也有助于模型正则化，并可以从模型中去掉dropout而不会过拟合。BN算法在卷积或池化之后，激活函数之前，对每个数据输出进行标准化，有助于提高训练速度，提升训练效果。</p><p><strong>改进二：采用高分辨率分类器。</strong>YOLOv1在预训练时采用的是224<em>224的输入（在ImageNet数据集上进行），然后在检测的时候采用448</em>448的输入，这会导致从分类模型切换到检测模型的时候，模型还要适应图像分辨率的改变。YOLOv2将分辨率从 224<em>224 增加到了 448</em>448，这就意味着网络需要适应新的输入分辨率。YOLOv2则将预训练分成两步：先用224<em>224的输入在ImageNet数据集训练分类网络，大概160个epoch。后将输入调整到448</em>448，再训练10个epoch进行微调，让网络有时间调整滤波器以便更好的运行在新分辨率上（这两步都是在ImageNet数据集上操作）。然后利用预训练得到的模型在检测数据集上fine-tuning。这样训练得到的模型，在检测时用448*448的图像作为输入可以顺利检测，还需要微调用于检测的 Resulting Network。最终通过使用高分辨率，mAP 提升了 4%。</p><p>​    此处引入了fine-tuning（微调）概念：用已经训练好的模型，加上自己的数据集，来训练新的模型。即使用别人的模型的前几层，来提取浅层特征，而非完全重新训练模型，从而提高效率。一般新训练模型准确率都会从很低的值开始慢慢上升，但是fine-tuning能够让我们在比较少的迭代次数之后得到一个比较好的效果。</p><p>​    <strong>改进三：引入Anchor boxes。</strong>YOLOv1将输入图像分成7*7的网格，每个网格预测2个Bounding Box，因此一共有98个Box，同时YOLOv1包含有全连接层，从而能直接预测Bounding Boxes的坐标值，但也导致丢失较多的空间信息，定位不准。 Faster R-CNN的方法只用卷积层与RPN来预测Anchor Box的偏移值与置信度，而不是直接预测坐标值。作者发现通过预测偏移量而不是坐标值能够简化问题，让神经网络学习起来更容易。</p><p>YOLOv2首先将YOLOv1的全连接层去掉，使用Anchor Boxes来预测 Bounding Boxes。同时将最后一个池化层去掉，使得最后的卷积层可以有更高分辨率的特征。然后缩减网络让其运行在 416<em>416 而不是 448</em>448，使得网络输出的特征图有奇数大小的宽和高，进而使得每个特征图在划分单元格（Cell）的时候只有一个中心单元格（Center Cell）。由于图片中的物体都倾向于出现在图片的中心位置，特别是大物体，所以有一个单元格单独位于物体中心的位置用于预测这些物体，而不要使用附近的四个单元格。</p><p>YOLOv2通过引入Anchor Boxes，通过预测Anchor Box的偏移值与置信度，而不是直接预测坐标值。使用Anchor Box会让精确度稍微下降，但用了它能让YOLO能预测出大于一千个框，同时Recall达到88%，mAP达到69.2%。YOLOv2的卷积层采用32来下采样图片，所以通过选择416<em>416用作输入尺寸最终能输出一个13</em>13的特征图。若采用FSRCNN中的方式，每个Cell可预测出9个Anchor Box，共13<em>13</em>9=1521个（YOLOv2确定Anchor Boxes的方法是维度聚类，每个Cell选择5个Anchor Box）。</p><p>以FSRCNN中一个51<em>39大小的特征图为例，其可以看做一个尺度为51</em>39的图像，对于该图像的每一个位置，考虑9个可能的候选窗口：3种面积3种比例。这些候选窗口称为Anchor Boxes。下图示出的是51*39个Anchor Box中心，以及9种Anchor Box示例：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3momdqc9j30cc0be0zz.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>此处将YOLOv1和YOLOv2的特征图数据结构进行对比：</p><ul><li>YOLOv1：SxSx(Bx5+C) ——&gt; 7x7x(2x5+20)</li></ul><p>其中SxS为网格大小，取7。B为边界框的数量，YOLOv1中为2。5对应边界框的定位信息：x,y,w,h,confidence。C为类别数量，YOLOv1使用Pascal Voc数据集训练，因此类别数为20。</p><p>​    <strong>YOLOv1只为每个网格预测一套类概率，供两个boxes共享。</strong></p><ul><li>YOLOv2：SxSxKx(5+C) ——&gt; 13x13x5x(5+20)</li></ul><p>S为分辨率，提升至13x13，对小目标适应性更好。借鉴了Faster R-CNN的思想，每个Cell对应K个Anchor box，YOLOv2中K取5。</p><p><strong>YOLOv2为每个位置的各个Anchor box都单独预测一套类概率值，和SSD比较类似。</strong></p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3momrf4qj30lq0b23yy.jpg" srcset="/img/loading.gif" alt="img" style="zoom:60%;" /></center><p>​    <strong>改进4：维度聚类。</strong>在Faster R-CNN中Anchor Box的大小和比例是按经验设定的，然后网络会在训练过程中调整Anchor Box的尺寸，最终得到准确的Anchor Boxes。若一开始就选择了更好的、更有代表性的先验Anchor Boxes，那么网络就更容易学到准确的预测位置。YOLOv2使用K-means聚类方法类训练Bounding Boxes，可以自动找到更好的宽高维度的值用于一开始的初始化。</p><p>如果用标准的欧式距离的k-means，尺寸大的框比小框产生更多的错误，聚类结果可能会偏移。由于聚类目的是确定更精准的初始Anchor Box参数，即提高IOU值，这应与Box大小无关，因此YOLOv2采用IOU值为评判标准，即K-means 采用的距离函数（度量标准） 为：d(box,centroid) = 1 - IOU(box,centroid)。（即距离d减少时，IOU增大）。</p><p>作者运行了各种k值的k-means，并画出聚类的簇个数和IOU的关系。在权衡模型复杂度、召回率与IOU值后，YOLOv2最终选择k=5。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3morgm00j30to0goq4a.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>​    <strong>改进5：直接位置预测。</strong>用 Anchor Box 的方法，会让 model 变得不稳定，尤其是在最开始的几次迭代。大多数不稳定因素产生自预测 Box 的（x,y）位置的时候。YOLOv2的网络在特征图（13*13）的每一个单元格中预测出5个Bounding Boxes（对应5个Anchor Boxes），每个Bounding Box预测出5个值（tx,ty,tw,th,t0），其中前4个是坐标偏移值，t0是置信度结果（类似YOLOv1中的边界框置信度Confidence）。YOLOv2借鉴了如下的预测方式，即当Anchor Box的中心坐标和宽高分别是（xa,ya）和（wa,ha）时，Bounding Box坐标的预测偏移值（tx,ty,tw,th）与预测框的坐标宽高（x,y,w,h）的关系如下：             </p><p>tx = (x-xa)/wa</p><p>ty = (y-ya)/ha</p><p>tw = log(w/wa)</p><p>th = log(h/ha)</p><p>其中：x,y,w,h来自于预测框。xa,wa,wa,ha来自于anchor框。</p><p>基于这种思想，YOLOv2在预测Bounding Box的位置参数时采用了如下强约束方法：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3moobb94j30o00i6400.jpg" srcset="/img/loading.gif" alt="img" style="zoom: 33%;" /><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3moq2l8xj30es076glx.jpg" srcset="/img/loading.gif" alt="img" style="zoom:50%;" /></center><p>上图中，黑色虚线框是Anchor Box，蓝色矩形框就是预测的Bounding Box结果，预测出的Bounding Box的坐标和宽高为（bx,by）和（bw,bh），计算方式如图中所示，其中：对每个Bounding Box预测出5个值（tx,ty,tw,th,t0），Cell与图像左上角的横纵坐标距离为（cx,cy），σ定义为sigmoid激活函数（将函数值约束到［0,1］），该Cell对应的Anchor Box对应的宽高为（pw,ph）。</p><p>  简而言之，（bx,by）就是（cx,cy）这个Cell附近的Anchor Box针对预测值（tx,ty）得到的Bounding Box的坐标预测结果，同时可以发现这种方式对于较远距离的Bounding Box预测值（tx,ty）能够得到很大的限制。</p><p>​    <strong>改进5：Fine-grained Features（细粒度特征）。</strong>YOLOv2 修改后的特征图大小为 13<em>13，这个尺寸对检测图片中尺寸大物体来说足够了，同时使用这种细粒度的特征对定位小物体的位置可能也有好处。Faster-RCNN、SSD 都使用不同尺寸的特征图来取得不同范围的分辨率，而 YOLO 采取了不同的方法，YOLO 加上了一个 Passthrough Layer 来取得之前的某个 26</em>26 分辨率的层的特征。这个 Passthrough layer 能够把高分辨率特征与低分辨率特征联系在一起，联系起来的方法是把相邻的特征堆积在不同的 Channel 之中，这一方法类似于 Resnet 的 Identity Mapping，从而把 26<em>26</em>512 变成 13<em>13</em>2048。YOLOv2 中的检测器位于扩展后的特征图的上方，所以他能取得细粒度的特征信息，这提升了 YOLO 1% 的性能。</p><p>​    <strong>改进6：多尺度训练。</strong>YOLOv2每迭代几次就会改变网络参数，以适应不同尺度下的检测任务。每10个Batch，网络会随机地选择一个新的图片尺寸，由于使用了下采样参数是32，因此也采用32的倍数作为输入的size，即采用{320，352，…，608}的输入尺寸，最小 320<em>320，最大 608</em>608，网络会自动改变尺寸，并继续训练的过程。这一策略让网络在不同的输入尺寸上都能达到较好的预测效果，使同一网络能在不同分辨率上进行检测。当输入图片较大时，检测速度较慢，但精度较高。输入图片较小时，检测速度较快，总体上提高了准确率。因此多尺度训练在准确率和速度上提供了一个折中。</p><p>​    </p><p>YOLOv2使用的是 GoogLeNet 架构，比 VGG-16 快，完成一次前向传播只用 85.2 亿次运算，而 VGG-16 要 306.9 亿次。然而，它的准确性比VGG-16略差。在ImageNet上，对于224×224分辨率下的单张裁剪图像，YOLO的自定义模型获得了88.0%的top-5准确率，而VGG-16则为90.0%。</p><p>YOLO v2 基于一个新的分类模型darknet19。YOLO v2 使用 3<em>3 的 filter，每次池化之后都增加一倍 Channels 的数量。YOLO v2 使用全局平均池化，使用Batch Normilazation来让训练更稳定，加速收敛，使模型规范化。最终的模型Darknet19，有 19 个卷积层和 5 个maxpooling 层，处理一张图片只需要 55.8 亿次运算，在 ImageNet 上达到 72.9% top-1 精确度，91.2% top-5 精确度。在训练时，如果把整个网络在更大的448</em>448分辨率上微调10个 epoches，初始学习率设置为0.001，网络达到76.5%top-1精确度，93.3%top-5精确度。</p><h2 id="总结：-1"><a href="#总结：-1" class="headerlink" title="总结："></a><strong>总结：</strong></h2><p>相比于YOLOv1，YOLOv2更好，更快，更强。在多种检测数据集中都要快过其他检测系统，并可以在速度与精确度上进行权衡。</p><p>YOLO 9000 的网络结构允许实时地检测超过9000种物体分类，这归功于使用WordTree合并来自各种来源的数据，并使用联合优化技术在ImageNet和COCO上同时进行训练，朝着缩小检测与分类之间的数据集大小差距迈出的重要一步。此外，文章还提出了添加Batch Normalization层、采用高分辨率分类器、引入Anchor boxes、使用维度聚类、细粒度特征、多尺度训练等改进方法。</p><h1 id="3-YOLOv3"><a href="#3-YOLOv3" class="headerlink" title="3.   YOLOv3"></a><strong>3.</strong>   <strong>YOLOv3</strong></h1><p>相较于v1和v2，YOLOv3主要改进了：提出了新的darknet-53基础网络、多尺度预测、改进损失函数。</p><p><strong>(1)</strong>  <strong>提出新的Darknet-53基础网络</strong></p><p>Darknet-53比Darknet-19更深，一共有53个卷积层，且连续使用了3x3和1x1的卷积层。借鉴了ResNet网络中的残差结构，使得网络能够达到很深程度的同时避免了梯度消失问题。去除了池化层，改用步长为2的卷积层进行特征图的降维。测试发现Darknet-53比ResNet-101更好，且速度提高了1.5倍。Darknet-53具有与ResNet-152相似的性能，并且快2倍。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3n1xful3j30h40o2ne8.jpg" srcset="/img/loading.gif" alt="image-20200725233235895" style="zoom:50%;" /></center><p>​    </p><p><strong>(2)</strong>  <strong>多尺度预测</strong></p><p>YOLOv1仅利用最后一个卷积层进行特征预测，导致对小物体的检测效果不佳（因为小物体的信息经过层层卷积后几乎完全丧失）。YOLOv2中采用Passthrough提取细粒度信息，但只提升了1%的性能，效果依然不理想。YOLOv3在3种不同尺度上预测边界框，并使用类似特征金字塔网络的概念进行多尺度级联。</p><p>具体来说，416<em>416</em>3的图像输入网络后，经过5次下采样，得到第一张尺寸为13<em>13的预测特征图。随后，为了实现细粒度特征检测，在第79层特征图处进行上采样，并与来自第61层的特征图进行特征拼接，得到了第二张尺寸为26</em>26的预测特征图。同样地，在第91层特征图处再次进行上采样，与第36层特征图进行特征拼接，得到了第三张尺寸为52*52的预测特征图。这种方法能够从上采样的特征中获得更有意义的语义信息，并从较早的特征图中获得更细粒度的信息。</p><p>最终得到三张特征图，大小分别为13<em>13,26</em>26,52<em>52。13</em>13的特征图由于下采样倍数大，单元网格的感受野比较大，适合检测尺寸比较大的目标物；26<em>26的特征图中单元网格感受野适中，适合检测尺寸中等的目标物；52</em>52的特征图中单元网格感受野相对较小，适合检测尺寸较小的目标物。</p><p>YOLOv3延续了YOLOv2使用的k-means聚类方法来确定Anchor box的尺寸。为每张特征图确定3个Anchor box的尺寸，最终聚类得到9种尺寸的Anchor box。在coco数据集上，这9个anchor box分别是(10x13)，(16x30)，(33x23)，(30x61)，(62x45)，(59x119)，(116x90)，(156x198)，(373x326)。分配的方式见下表所示，遵循的原则是特征图的尺寸越小，则分配的anchor box的尺寸越大。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3monpkm2j30k502fjsb.jpg" srcset="/img/loading.gif" alt="preview"></p><p>YOLOv3通过多尺度预测，大大提升了对小目标的检测效果。</p><p><strong>(3)</strong>  <strong>改进损失函数</strong></p><p>YOLOv3不再采用softmax分类器，而采用独立的logistic classifiers。并且在训练过程中，使用二元交叉熵损失来进行类别预测。因为在更复杂的Open Image中，有很多标签的含义是重叠的（例如女人和人）。而softmax需要假定每一个需要预测的类别都是互相独立的，即每个框中只能有一个类别，显然softmax不适合这种情况。</p><h1 id="4-YOLOv4"><a href="#4-YOLOv4" class="headerlink" title="4.    YOLOv4"></a><strong>4.</strong>    <strong>YOLOv4</strong></h1><p><strong>主要贡献：</strong></p><ol><li>提出了一个高效且强大的目标检测模型。任何人可以使用一个1080Ti或者2080Ti的GPU就可以训练出一个快速并且高精度的目标检测器。</li><li>在检测器训练的过程中，测试了目标检测中最高水准的 Bag-of-Freebies和Bat-of-Specials方法。</li><li>改进了最先进的算法，使得它们更加高效并且适合于在一个GPU上进行训练，比如CBN, PAN, SAM等。</li></ol><p>作者总结了近几年的单阶段和双阶段的目标检测算法以及技巧，并用一张图概括了单阶段和双阶段目标检测网络的差别。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh3molwjybj317w0lo45w.jpg" srcset="/img/loading.gif" alt="img"></p><p>检测器通常由两部分组成：backbone和head。前者在ImageNet上进行预训练，后者用来预测类别信息和目标物体的边界框。在GPU平台上运行的检测器，它们的backbone可能是VGG, ResNet, ResNetXt,或者是DenseNet。在CPU平台上运行的检测器，它们的backbone可能是SqueezeNet，MobileNet或者是ShuffleNet。对于head部分，通常分为两类：one-stage和two-stage的目标检测器。Two-stage的目标检测器的代表是R-CNN系列，还有基于anchor-free的Two-stage的目标检测器，比如RepPoints。One-stage目标检测器的代表模型是YOLO, SSD和RetinaNet。在最近几年，出现了基于anchor-free的one-stage的算法，比如CenterNet, CornerNet, FCOS等等。</p><p>​    作者将目前的调优方法分为两类：Bag of freebies 和 Bag of specials。将只改变训练策略或者只增加训练成本的方法称之为“bag of freebies”，常见的方法有：数据増广（包括模拟图片遮挡、多图片融合）、解决样本不均衡问题（包括focal loss，example mining）、解决难以描述类别关联度的方法（Label smoothing）、边界框回归的目标函数（MSE,GIOU,DIOU,CIOU）。将即插即用模块、后处理方法这些仅稍微增加推理成本但可极大提高准确度的方法称之为“bag of specials”。这一类常见的有：提高感受野（SPP,ASPP,RFB）、注意力模块、特征融合（FPN,SFAM,ASFF,BiFPN）、激活函数（Relu,SELU,Swish）.</p><p>为了使设计的检测器更适合于单GPU上的训练，作者做了如下的附加设计和改进：(1)介绍了一种新的数据增强Mosaic法和Self-AdversarialTraining 自对抗训练法。(2)应用遗传算法选择最优超参数。(3)改进SAM，改进PANet，和交叉小批量标准化(CmBN)，使模型更适合于有效的训练和检测</p><p><strong>YOLOv4的最终架构：</strong>Backbone:CSPDarknet53 Neck:SPP,PANet Head:YOLOv3 。</p><p><strong>YOLOv4使用的调优技巧：</strong></p><p><strong>BOF(Bag of freebies):</strong> </p><p>backbone：CutMix和mosaic数据增强，DropBlock正则化，类标签平滑。</p><p>detector：CIO-loss,CmBN, DropBlock正则化，Mosaic数据增强，自对抗训练，消除网格敏感性，</p><p>为单个groundtruth使用多个anchors，余弦退火调度器，最优超参数，随机训练形状.</p><p><strong>BOS(Bag of Specials):</strong></p><p>backbone：Mish激活、跨级部分连接(CSP)、多输入加权剩余连接(MiWRC)。</p><p>detector：Mish activation,SPP-block, SAM-block, PAN path-aggregation block,DIoU-NMS.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>YOLOv4其实并没有特别大的创新点，文章可以概括为三个部分：目标检测算法综述+最新算法的大量实验+最优的算法组合。也就是将当前目标检测的方法进行了对比实验研究，最终找到了一个最优的组合，这个组合所带来的整体增益最高。</p>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——YOLOv4:Optimal Speed and Accuracy of Object Detection</title>
    <link href="/2020/07/23/YOLOv4/"/>
    <url>/2020/07/23/YOLOv4/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>目前有很多可以提高CNN准确性的算法。这些算法的组合在庞大数据集上进行测试、对实验结果进行理论验证都是非常必要的。有些算法只在特定的模型上有效果，并且只对特定的问题有效，或者只对小规模的数据集有效；然而有些算法，比如batch-normalization和residual-connections，对大多数的模型、任务和数据集都适用。我们认为这样通用的算法包括：Weighted-Residual-Connections（WRC), Cross-Stage-Partial-connections（CSP）, Cross mini-Batch Normalization（CmBN）, Self-adversarial-training（SAT）以及Mish-activation。我们使用了新的算法：WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, Dropblock regularization 和CIoU loss以及它们的组合，获得了最优的效果：在MS COCO数据集上的AP值为43.5%(65.7% AP50)，在Tesla V100上的实时推理速度为65FPS。源代码在<a href="https://github.com/AlexeyAB/darknet" target="_blank" rel="noopener">https://github.com/AlexeyAB/darknet</a>.</p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>大部分基于CNN的目标检测器主要只适用于推荐系统。举例来说，通过城市相机寻找免费停车位置的系统使用着慢速但是高精度的模型，然而汽车碰撞警告却使用着快速但是低精度的模型。提高实时目标检测器的精度不仅能够应用在推荐系统上，而且还能用于独立的流程管理以及减少人员投入。在GPU上运行的实时目标检测操作允许在可接受的开销下进行大规模使用。目前大部分高精度的神经网络不仅不能实时运行，并且需要较大的mini-batch-size在多个GPUs上进行训练。我们构建了仅在一块GPU上就可以实时运行的CNN解决了这个问题，并且它只需要在一块GPU上进行训练。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0w34l1nuj30r20lmjvs.jpg" srcset="/img/loading.gif" alt="image-20200723142824281" style="zoom:50%;" /></p><blockquote><p>图一：YOLOv4和其他先进检测器的对比。YOLOv4比EffcientDet快两倍并且性能相似。相对于YOLOv3，分别提高了10%的AP和12%的FPS。</p></blockquote><p>Yolo-V4的主要目的在于设计一个能够应用于实际工作环境中的快速目标检测系统，且能够被并行优化，并没有很刻意的去追求理论上的低计算量（BFLOP）。我们希望这个检测器能够轻松的训练和使用。具体来说就是任何一个人仅仅使用一个GPU进行训练和测试就可以得到<strong>实时的，高精度的以及令人信服</strong>的目标检测结果，正如在图1中所示的YOLOv4的结果。我们的贡献总结如下：</p><ol><li><strong>我们提出了一个高效且强大的目标检测模型</strong>。任何人可以使用一个1080Ti或者2080Ti的GPU就可以训练出一个快速并且高精度的目标检测器。</li><li>我们在检测器训练的过程中，测试了目标检测中最高水准的 Bag-of-Freebies和Bat-of-Specials方法。</li><li><strong>我们改进了最高水准的算法</strong>，使得它们更加高效并且适合于在一个GPU上进行训练，比如CBN, PAN, SAM等。</li></ol><h1 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h1><h2 id="2-1-目标检测模型"><a href="#2-1-目标检测模型" class="headerlink" title="2.1 目标检测模型"></a>2.1 目标检测模型</h2><p><strong>检测器通常由两部分组成：</strong>backbone<strong>和</strong>head。前者在ImageNet上进行预训练，后者用来预测类别信息和目标物体的边界框。在GPU平台上运行的检测器，它们的backbone可能是VGG, ResNet, ResNetXt,或者是DenseNet。在CPU平台上运行的检测器，它们的backbone可能是SqueezeNet，MobileNet或者是ShuffleNet。对于head部分，通常分为两类：one-stage和two-stage的目标检测器。Two-stage的目标检测器的代表是R-CNN系列，包括：fast R-CNN, faster R-CNN,R-FCN和Libra R-CNN. 还有基于anchor-free的Two-stage的目标检测器，比如RepPoints。One-stage目标检测器的代表模型是YOLO, SSD和RetinaNet。在最近几年，出现了基于anchor-free的one-stage的算法，比如CenterNet, CornerNet, FCOS等等。在最近几年，目标检测器在backbone和head之间会插入一些网络层，这些网络层通常用来从不同的步骤中收集特征图。我们将其称之为目标检测器的neck。通常，一个neck由多个bottom-up路径和多个top-down路径组成。使用这种机制的网络包括Feature Pyramid Network（FPN）,Path Aggregation Network（PAN），BiFPN和NAS-FPN。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0wtzqztoj30hp08kjrv.jpg" srcset="/img/loading.gif" alt="img"></p><p>除了上面的这些模型，一些学者将重点放在为目标检测器构建新的backbone（DetNet, DetNASNet）或者是一整个新的模型（SpinNet, HitDetector）。</p><p>综上所述，一个普通的目标检测器由下面四个部分组成：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0wwndik4j30qq0wqtem.jpg" srcset="/img/loading.gif" alt="image-20200723145649089" style="zoom:33%;" /></center><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0x8gyowuj31jp0u0gta.jpg" srcset="/img/loading.gif" alt="image-20200723150810800"></p><h2 id="2-2-Bag-of-freebies"><a href="#2-2-Bag-of-freebies" class="headerlink" title="2.2 Bag of freebies"></a>2.2 Bag of freebies</h2><p>通常来说，目标检测器都是进行离线训练的（训练的时候对GPU数量和规格不限制）。因此，研究者总是喜欢扬长避短，使用最好的训练手段，因此可以在不增加推理成本的情况下，获得最好的检测精度。<strong>我们将只改变训练策略或者只增加训练成本的方法称之为“bag of freebies”</strong>在目标检测中经常使用并且满足bag of freebies的定义的算法称是<strong>①数据增广</strong>。数据增广的目的是增加输入图片的可变性，因此目标检测模型对从不同场景下获取的图片有着更高的鲁棒性。举例来说，光度畸变（photometric distoitions）和几何畸变（geometric distortions）是用来数据增强方法的两个常用的手段，他们确实对目标检测任务有用。在处理光度畸变时，我们调整图像的亮度，对比度，色相，饱和度和噪点。对于几何畸变，我们会随机增加尺度变化，裁剪，翻转以及旋转。</p><p>上面提及的数据增广的手段都是<strong>像素级别</strong>的调整，它保留了调整区域的所有原始像素信息。此外，一些研究者将数据增广的重点放在了<strong>②模拟目标物体遮挡问题上</strong>。具体来说，random erase和CutOut可以随机选择图像上的矩形区域，然后使用随机值或者使用零补值来进行填充。对于hide-and-seek和grid mask，他们随机地或者均匀地在一幅图像中选择多个矩形区域，并且使用零来代替矩形区域中的像素值。如果将相似的概念用到特征图中，出现了DropOut, DropConnect和DropBlock方法。此外，<strong>一些研究者提出一起使用多张图像进行数据增强的方法</strong>。举例来说，MixUp使用两张图片进行相乘并且使用不同的系数比进行叠加，然后使用它们的叠加比来调整标签。对于CutMix，它将裁剪的图片覆盖到其他图片的矩形区域，然后根据混合区域的大小调整标签。除了上面提及的方法，style transfer GAN也用来数据增广，CNN可以学习如何有效的减少纹理偏差。</p><p>一些和上面所提及的不同的方法用来<strong>解决数据集中的语义分布可能存在偏差的问题</strong>。处理语义分布偏差的问题，一个非常重要的问题就是<strong>在不同类别之间存在数据不平衡</strong>，并且这个问题在two-stage目标检测器中，通常使用hard negative example mining或者online hard example mining来解决。但是example mining 方法并不适用于one-stage的目标检测器，因为这种类型的检测器属于dense prediction架构。因此focal loss算法用来解决不同类别之间数据不均衡的问题。<strong>③另外一个非常重要的问题就是使用one-hot</strong>很难描述不同类别之间关联度的关系。执行标记时经常使用这种表示方法。Label smothing提出在训练的时候，将hard label转换成soft label，这个方法可以使得模型更加的鲁棒性。为了得到一个最好的soft label, Islam引入了知识蒸馏的概念来设计标签细化网络。</p><p>最后一个bag of freebies是<strong>④设计边界框回归的目标函数</strong>。传统的目标检测器通常使用均方根误差（MSE）在Bbox的中心坐标以及宽高上进行直接的回归预测，即{x<sub>center</sub>,y<sub>center</sub>,w,h}, 或者左上角和右下角的两个点，即{x<sub>top_left</sub>,y<sub>top_left</sub>,x<sub>bottom_right</sub>,y<sub>bottom_right</sub>}. 对于anchor-based方法，去预测相应的offset，{x<sub>centeroffset</sub>,y<sub>centeroffset</sub>,w<sub>offset</sub>,h<sub>offset</sub>}和{x<sub>topleftoffset</sub>,y<sub>topleftoffset</sub>,x<sub>bottomrightoffset</sub>,y<sub>bottomrightoffset</sub>}.<strong>但是，预测Bbox每个点的坐标值是将这些点作为独立的变量，但是实际上并没有将目标物体当成一个整体进行预测</strong>。为了更好的解决这个问题，一些研究者最近提出了<strong>IoU</strong>损失函数，它能够将Bbox区域和ground truth的BBox区域的作为整体进行考虑。IoU损失函数需要计算BBox四个坐标点以及ground truth的IoU，然后将生成的结果连接到整个代码中。因为IoU具有尺度不变性，它可以解决传统算法计算{x,y,w,h}的L1或L2范数时损失随着尺度增加而增大的问题。最近，一些研究者继续提高IoU损失函数的性能。举例来说，除了覆盖范围，GIoU还包括目标物体的形状和方向。他们提出寻找同时包括预测的BBox和ground truth的BBox的<strong>最小区域BBox</strong>，然后使用这个BBox作为分母去代替原来Iou损失函数中的分母。DIoU损失函数额外考虑了目标物体的中心距离，CIoU另一方面同时将重叠区域，中心点间的距离和长宽比考虑在内。CIoU在BBox回归问题上可以获得最好的收敛速度和准确率。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh10zqf7zjj30m70dgjsi.jpg" srcset="/img/loading.gif" alt="img"></p><h2 id="2-3-Bag-of-specials"><a href="#2-3-Bag-of-specials" class="headerlink" title="2.3 Bag of specials"></a>2.3 Bag of specials</h2><p>对于那些<strong>插件模块</strong>和<strong>后处理方法</strong>，它们仅仅稍微的增加了推理成本，但是可以极大地提高目标检测的准确度，我们将其称之为“bag of specials”。一般来说，这些插件模块用来提高一个模型中特定的属性，比如增加感受野，引入注意力机制或者提高特征整合的能力等等；后处理方法是用来抑制模型预测结果的一种方法。</p><p>可以用来<strong>提升感受野</strong>的常规的方法是SPP, ASPP和RFB。SPP模型来源于空间金字塔匹配（SPM），而且SPMs原始的方法将特征图划分成很多d*d个相等的块，其中d可以是{1,2,3,…}，因此可以形成空间金字塔，然后提取bag-of-word的特征。SPP将SPM应用在CNN中，然后使用max-pooling代替bag-of-word运算。因为SPP输出的是一维的特征向量，因此它不能应用在全卷积网络（FCN）中。在YOLOv3中，Redmon和Farhadi改进了SPP模块，将max-pooling输出和内核尺寸k*k连接起来，其中k={1,5,8,13},stride=1。基于这个设计，一个相对较大的k*k的max-pooling有效地提高了backbone特征的感受野。在添加了改进后的SPP模型之后，YOLO-v3-608在COCO数据集上，虽然增加了0.5%的额外计算量，但是提高了2.7%的AP50。ASPP模块和改进的SPP模块的区别主要在：原始的k*k过滤器尺寸，从stride=1到3*3内核尺寸的max-pooling，在stride=1的碰撞卷积运算中膨胀比为k。RFB模块使用一些k*k的内核，膨胀比为k，步长为1的碰撞卷积，它比ASPP获得了更全面的空间覆盖率。RFB在MS COCO数据集上仅仅增加了7%的额外推理时间，但是得到了5.7%的AP50提升。</p><p>目标检测上经常使用的注意力模块主要分成channel-wise注意力模块和point-wise注意力模块，这两个注意力模块主要的代表分别是Squeeze-and-Excitation(SE)和Spatial Attention Module(SAM)。尽管SE模块在ImageNet图像分类工作上仅仅增加了2%的计算量而提高了1%的top-1准确率，但是在GPU上提高了10%的推理时间，因此SE模块更适合在移动设备上使用。但是对于SAM模块来说，在ImageNet图像分类任务中，它仅仅需要0.1%的额外计算量却能够提升ResNet-SE 0.5%的top-1准确率。它在GPU上并没有有效地影响推理速度。</p><p>关于特征融合，早期的是使用skip connection或者是hyper-column将低级的特征和高级的语义特征进行融合。因为多尺度预测方法比如FPN逐渐受到追捧，因此提出了很多将不同特征金字塔融合的轻量级模型。这类别的模型包括SFAM, ASFF和BiFPN。SFAM的主要思想是在多尺度连接特征图上使用channel-wise级别的调整。对于ASFF，它使用softmax作为point-wise级别的调整，然后将不同尺度的特征图加在一起。在BiFPN中，提出使用多输入权重残差连接去执行scale-wise级别的调整，然后将不同尺度的特征图加在一起。</p><p>在深度学习的研究中，一些人重点关心去寻找一个优秀的激活函数。一个优秀的激活函数可以让梯度更有效的进行传播，与此同时它不会增加额外的计算量。在2010年，Nair和Hinton提出了ReLU激活函数充分地解决了梯度消失的问题，这个问题在传统的tanh和sigmoid激活函数中会经常遇到。随后，LReLU,PReLU,ReLU6,Scaled Exponential Linear Unit(SELU),Swish,hard-Swish和Mish等等相继提出，它们也用来解决梯度消失的问题。LReLU和PReLU主要用来解决当输出小于零的时候，ReLU的梯度为零的问题。ReLU6和hard-Swish主要为量化网络而设计。对于神经网络的自归一化，提出SELU激活函数去实现这个目的。需要注意的是Swish和Mish都是连续可导的激活函数。</p><p>在基于深度学习的目标检测中使用的后处理方法是NMS,它可以用来过滤那些预测统一物体、但是效果不好的BBoxes，然后仅仅保留较好的BBoxes。优化NMS和优化目标方程的方法异曲同工。NMS提出的最初的方法并没有将上下文信息考虑在内，因此Girshick在R-CNN中添加了分类置信度作为参考，然后根据置信度得分的顺序，由高到低执行greedy NMS。对于soft NMS来说，它考虑了这样一个问题：在greedy NMS使用IoU的时候，目标遮挡可能会造成置信度得分的退化。在soft NMS基础上，DIoU NMS将重心坐标的距离信息添加到Bbox的筛选处理中了。值得一提的是，上面提到的后处理方法中都不直接引用捕获的图像特征，后续的anchor-free方法开发中不再需要后处理。<br><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gknj9u3585j30p20c9q3k.jpg" srcset="/img/loading.gif" alt="img"></p><h1 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h1><p>我们工作基本的目标就是在生产系统和优化并行预算中加快神经网络的速度，而非降低计算量理论指标（BFLOP）。我们提供了两个实时神经网络的选择： </p><p>（1）GPU 在卷积层中，我们使用少量的组(1-8): CSPResNeXt50 / CSPDarknet53</p><p>（2）VPU 我们使用分组卷积，但是我们不使用Squeeze-and-excitement(SE)模块，具体包括以下模型：EfficientNet-lite / MixNet / GhostNet / MobileNetV3</p><h2 id="3-1网络架构的选择"><a href="#3-1网络架构的选择" class="headerlink" title="3.1网络架构的选择"></a>3.1网络架构的选择</h2><p>我们的目标是寻找<strong>输入网络的分辨率、卷积层的个数、参数的数量</strong> (filtersize<sup>2</sup><em> filters </em> channel / groups)以及<strong>输出层的个数</strong>（filters）之间的最优的平衡。举例来说，大量的研究表明：在ILSVRC2012（ImageNet）的目标检测上，CSPResNext50比CSPDarket53的效果更好，但是在MS COCO的目标检测中，两个的效果恰好相反。</p><p>下一个目标就是选择额外的模块去<strong>增加感受野</strong>以及为不同检测器不同的backbone选择<strong>参数聚合</strong>的最佳方法。比如：FPN, PAN, ASFF, BiFPN。</p><p>在分类任务上最优的模型在检测上未必就是最优的。和分类任务相比，检测器需要以下要求：</p><p>（1）<strong>更高的输入尺寸</strong>（分辨率）- 为了检测多个小物体</p><p>（2）<strong>更多网路层 -</strong> 为了获得更大的感受野去覆盖不断增大的输入尺寸</p><p>（3）<strong>更多的参数</strong> - 提高模型的能力从而能够在一张图片上检测到不同尺寸的多个物体。</p><p>假设来说，我们可以认为具有<strong>更大感受野</strong>（有大量的3*3的卷积层）和<strong>具有大量参数的模型</strong>应当作为检测器的backbone。表格1展示了CSPResNetXt50, CSPDarkent53以及EfficientNet B3的相关信息。CSPResNetXt50仅仅只有16个3*3的卷积层，一个425*425的感受野和20.6M个参数，然而CSPDarkent53有29个3<em>3的卷积层，725</em>725的感受野和27.6M个参数。从理论证明和大量的实验表明在这两个模型中，<strong>CSPDarkent53是作为检测器的backbone最优的选择</strong>。</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gknja3g27rj31z00dsdjr.jpg" srcset="/img/loading.gif" alt="image-20200727115116828"></p><p>不同尺寸感受野的影响总结如下：</p><ol><li>等于目标物体尺寸时：能够看到整个目标物体。</li><li>等于网络尺寸：能够看到目标物体周围的上下文信息。</li><li>大于网络尺寸：增加图像点和最终激活之间连接的数量。</li></ol><p>我们将<strong>SPP模块</strong>添加到CSPDarknet53中，因为它极大<strong>提高了感受野</strong>，能够分离出最重要的上下文特征而且没有降低网络运行的速度。我们使用<strong>PANet作为</strong>不同检测器不同backbone训练阶段<strong>聚集参数</strong>的方法，而非YOLOv3的FPN模块。</p><p>最后，我们选择CSPDarknet53作为backbone, SPP作为附加的模块，PANet 作为neck，使用YOLOv3作为YOLOv4架构的head。</p><p>未来，我们计划扩展检测器的Bag of freebies，它们在理论上可以解决某些问题并且能够提高检测器的精度，后续会以实验的形式探究每个算法对检测器的影响。</p><h2 id="3-2-BoF-和-BoS的选择"><a href="#3-2-BoF-和-BoS的选择" class="headerlink" title="3.2 BoF 和 BoS的选择"></a>3.2 BoF 和 BoS的选择</h2><p>为了提高目标检测的训练，CNN通常使用下面一些技巧：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5gl0abr0j30tq0t644q.jpg" srcset="/img/loading.gif" alt="image-20200727131951927" style="zoom:33%;" /></center><p>对于训练激活函数，因为PReLU和SELU难以训练，并且RELU6是专门为量化网络设计的，我们因此不考虑这这三个激活函数。在正则化方法中，提出DropBlok的学者将其算法和其他算法进行了比较，然后DropBolck效果更好。<strong>因此我们毫不犹豫的选择DropBlock作为我们的正则化方法</strong>。在归一化方法的选择中，因为我们关注在一块GPU上的训练策略，因此我们不考虑syncBN。</p><h2 id="3-3-额外的改进"><a href="#3-3-额外的改进" class="headerlink" title="3.3 额外的改进"></a>3.3 额外的改进</h2><p>为了让检测器更适合在单个GPU上进行训练，我们做了以下额外的设计和改进：</p><ol><li>我们提出了新的数据増广方法：Mosaic和Self-Adversarial Training(SAT)</li><li>在应用遗传算法时选用一个最优的超参数。</li><li>我们改进了已有的算法，让我们的设计更适合高效的训练和检测——改进SAM，改进PAN，以及Cross mini-Batch Normalization(CmBN).</li></ol><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5gzc8j9yj30v00mmtxa.jpg" srcset="/img/loading.gif" alt="image-20200727133338788" style="zoom:50%;" /></center><p>Mosaic是一种新型的数据增广的算法，它混合了四张训练图片。因此有四种不同的上下文进行融合，然而CutMix仅仅将两张图片进行融合。此外，batch normalization在每个网络层中计算四张不同图片的激活统计。这极大减少了一个大的mini-batch尺寸的需求。</p><p>自适应对抗训练（SAT）也表示了一个新的数据增广的技巧，它在前后两阶段上进行操作。在第一阶段，神经网络代替原始的图片而非网络的权重。用这种方式，神经网络自己进行对抗训练，代替原始的图片去创建图片中此处没有期望物体的描述。在第二阶段，神经网络使用常规的方法进行训练，在修改之后的图片上进检测物体。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5h89y07oj30uu0nan70.jpg" srcset="/img/loading.gif" alt="image-20200727134213874" style="zoom:50%;" /></center><p>正如图4中显示，CmBN（Cross mini-Batch Normalization）代表CBN改进的版本。它只收集了一个批次中的mini-batches之间的统计数据。</p><p>我们将SAM的spatial-wise注意力变成了point-wise注意力机制，然后将PAN中的shortcut连接变成了concatenation连接，正如图5和图6所表示的那样。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5hbtckgwj30sy10m78x.jpg" srcset="/img/loading.gif" alt="image-20200727134538015" style="zoom: 33%;" /></center><h2 id="3-4-YOLOv4"><a href="#3-4-YOLOv4" class="headerlink" title="3.4 YOLOv4"></a>3.4 YOLOv4</h2><p>在这个部分，我们会参数YOLOv4的细节：</p><div class="table-container"><table><thead><tr><th>网络组成</th><th style="text-align:center">Backbone</th><th style="text-align:center">Neck</th><th style="text-align:center">Head</th></tr></thead><tbody><tr><td><strong>采用模块</strong></td><td style="text-align:center">CSPDarknet53</td><td style="text-align:center">SPP,PAN</td><td style="text-align:center">YOLOv3</td></tr><tr><td>BoF</td><td style="text-align:center">CutMix and Mosaic data augmentation <br />DropBlock regularization<br /> Class label smoothing</td><td style="text-align:center"></td><td style="text-align:center">CIoU-loss<br/>CmBN<br/>DropBlock regularization<br/>Mosaic data augmentation<br/>Self-Adversarial Training<br/>Eliminate grid sensitivity<br/>Using multiple anchors for a single ground truth<br/>Cosine annealing scheduler<br/>Optimal hyperparameters<br/>Random training shapes<br/></td></tr><tr><td>BoS</td><td style="text-align:center">Mish activation<br />Cross-stage partial connections(CSP)<br />Multi-input weighted residual connections (MiWRC)</td><td style="text-align:center"></td><td style="text-align:center">Mish activation<br />SPP-block<br />SAM-block<br />PAN path-aggregation block<br />DIoU-NMS</td></tr><tr><td>模块作用</td><td style="text-align:center">在ImageNet上进行预训练</td><td style="text-align:center">融合不同位置上的特征图</td><td style="text-align:center">进行预测</td></tr></tbody></table></div><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h1><p>我们测试了不同训练提升技巧在ImageNet（ILSVRC2012 val）数据集上的精度影响，然后又验证了检测器在MS COCO（test-val 2017）数据集的准确率。</p><h2 id="4-1-实验参数配置"><a href="#4-1-实验参数配置" class="headerlink" title="4.1 实验参数配置"></a>4.1 实验参数配置</h2><p>在ImageNet图像分类实验中，默认的超参数如下：训练步长为8,000,000；batch size和mini-batch size分别为128和32；polynominal decay learning rate scheduling strategy初始的学习率为0.1；warm-up步长为1000；momentum和weight decay分别设置为0.9和0.005。所有的BoS实验使用相同的、默认的超参数，在BoF实验中，我们增加了一半的训练步长。在BoF实验中，我们验证了MixUp, CutMix, Mosaic, Bluring数据增加一节label smoothing regularization方法。在BoS实验中，我们比较了LReLU，Swish和Mish激活函数的影响。所有的实验都在1080Ti或者2080Ti GPU上进行训练。</p><p>在MS COCO目标检测实验中，默认的超参数如下：训练步长为500,500；the step decay learning rate scheduling strategy初始化学习率为0.01在步长为400,000和450,000的时候乘以0.1；momentum和weight decay分别设置为0.9和0.0005。所有的架构在一块GPU进行多尺度训练，它的batch size为64，然而它的mini-batch为8还是4取决于网络架构和GPU的内存限制。除了对寻找最优的超参数使用遗传算法之外，其他所有的实验都使用默认的设置。遗传算法和GIoU使用YOLOv3-SPP进行训练，并且为5k个min-val进行300个epochs。对我们采用搜索的学习率为0.00261，momentum为0.949，IoU阈值为设置为0.213，遗传算法实验的损失标准化为0.07。我们还验证了大量的BoF算法，包括grid sensitivity elimination， mosaic数据增广，IoU阈值化，遗传算法，class label smoothing， cross mini-batch normalization，self-adversarial training,cosine anneling scheduler, dynamic mini-batch size, DropBlock, Optimized Anchors, 不同的IoU损失函数。我们也在不同BoS算法上进行了实验，包括Mish，SPP，SAM,RFB,BiFBN以及Gaussiian YOLO。所有的实验我们仅仅使用一个GPU进行训练，因此比如syncBN的优化多个GPU的技巧我们并没有使用。</p><h2 id="4-2-不同算法在分类器训练上的影响"><a href="#4-2-不同算法在分类器训练上的影响" class="headerlink" title="4.2 不同算法在分类器训练上的影响"></a>4.2 不同算法在分类器训练上的影响</h2><p>首先，我们研究了不同算法在分类器训练上的影响；具体来说，Class label smoothing的影响，不同数据增广技巧，bilateral blurring，MixUp, CutMix和Mosaic的印象在图7中显示，以及不同激活函数的影响，比如Leaky-ReLU（默认的），Swish和Mish。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5l289v5jj30uo0m8ngb.jpg" srcset="/img/loading.gif" alt="image-20200727155449355" style="zoom:50%;" /></center><p>在表2中所示，在我们的实验中，通过引入一些算法，分类器的准确率得到了提升，这些算法包括：CutMix和Mosaic数据增广，Class label smoothing和Mish激活函数。结果，我们的用于分类器训练的BoF-backbone(Bag of Freebies)包括：Cutmix 和Mosaic数据增广算法以及Class labelsmoothing。正如表2和表3所示，我们将Mish激活函数作为补充的选项。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5i5dje4jj30u00uq436.jpg" srcset="/img/loading.gif" alt="image-20200727141402304" style="zoom:33%;" /></center><h2 id="4-3-不同算法在检测器训练上的影响"><a href="#4-3-不同算法在检测器训练上的影响" class="headerlink" title="4.3 不同算法在检测器训练上的影响"></a>4.3 不同算法在检测器训练上的影响</h2><p>进一步的研究关注不同Bag-of-Freebies（BoF-detector）在检测器训练准确度的影响，正如表4所示。通过研究能够提高检测器准确度的算法，我们极大地扩展了BoF的算法选项，而且并没有影响FPS：</p><p>S：消除栅格的敏感度   这个方程在YOLOv3中用于评估目标物体的坐标，自重cx和cy通常是整数，因此，当bx的值非常接近cx或者cx+1的时候，tx的绝对值会非常大。我们通过给sigmoid函数乘以一个大于1的因子来解决这个问题，因此，这样就消除了栅格对不可检测物体的影响。</p><ul><li>M：Mosaic数据增广 - 在训练过程中，使用四张图片而非一张进行增广处理</li><li>IT：IoU阈值 - 为一个ground truth的IoU使用多个anchors，ground truth IoU(truth, anchor) &gt; IoU 阈值</li><li>GA：遗传算法 - 在前10%的训练时间内使用遗传算法选择最优的超参数</li><li>LS：Class label smoothing - 为sigmoid激活函数使用class label smoothing。</li><li>CBN：CmBN - 在整个批次中通过使用Cross mini-Batch Normalization收集统计数据，而非在单独的mini-batch中收集统计数据。</li><li>CA：Cosine annealing scheduler - 在sinusoid训练中改变学习率</li><li>DM：动态的mini-batch尺寸 - 在低分辨率的训练过程中，通过随机训练形状自动的改提高mini-batch的尺寸。</li><li>OA: 优化Anchors - 使用优化的anchors进行训练，网络的分辨率为512*512</li><li>GIoU, CIoU, DIoU, MSE - 为边界框回归使用不同的损失函数。</li></ul><p>下一步的研究关心检测器训练准确度上，不同Bag-of-Specials（BoS-detector）的影响，包括PAN, RFB, SAM, Gaussian YOLO(G)，以及ASFF，正如表5所示。在我们的实验中，当使用SPP, PAN和SAM的时候，检测器得到了最好的性能。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5jdc5qrhj30sm0h20w2.jpg" srcset="/img/loading.gif" alt="image-20200727145615305"></p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5jdpq2c6j30n208sabu.jpg" srcset="/img/loading.gif" alt="image-20200727145639628" style="zoom:50%;" /></center><h2 id="4-4-不同的backbones和预训练权重在检测器训练中的影响"><a href="#4-4-不同的backbones和预训练权重在检测器训练中的影响" class="headerlink" title="4.4 不同的backbones和预训练权重在检测器训练中的影响"></a>4.4 不同的backbones和预训练权重在检测器训练中的影响</h2><p>下一步我们研究不同backbones模型在检测器准确率上的影响，正如表6所示。我们注意到拥有最佳分类准确率的模型，检测器的准确度未必是最佳的。</p><p>首先，尽管使用不同算法训练得到的CSPResNeXt-50模型的分类精度比CSPDarknet53模型的要高，但是CSPDarknet53模型的检测精度更高。</p><p>再者，CSPResNeXt-50分类器训练使用BoF和Mish提高了它的分类准确率，但是检测器训练使用的预训练权重的进一步使用减少了检测器的精度。但是，CSPDarknet53分类器训练使用BoF和Mish提高了分类器和检测器的准确率，它使用分类器预训练权重。这表示CSPDarknet53比CSPResNeXt-50更适合作为检测器。</p><p>我们观察到，由于各种改进，<strong>CSPDarknet53模型显示出了更大的提高的检测器精度的能力</strong>。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5jlr796sj30qs0gudj8.jpg" srcset="/img/loading.gif" alt="image-20200727150422977" style="zoom:50%;" /></center><h2 id="4-5-不同的mini-batch尺寸在检测器训练上的影响"><a href="#4-5-不同的mini-batch尺寸在检测器训练上的影响" class="headerlink" title="4.5 不同的mini-batch尺寸在检测器训练上的影响"></a>4.5 不同的mini-batch尺寸在检测器训练上的影响</h2><p>最后，我们分析了不同mini-batch尺寸的训练的模型的结果，并且结果在表7中显示出来。从表7中我们发现在添加了BoF和BoS训练策略之后，mini-batch尺寸几乎对检测器的性能没有影响。结果显示在引入了BoF和BoS之后，就不需要使用昂贵的GPUs进行训练。换句话说，任何人可以仅仅使用一个GPU去训练一个优秀的检测器。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5jr8a1qzj30qk0dm0vd.jpg" srcset="/img/loading.gif" alt="image-20200727150939061" style="zoom:50%;" /></center><h1 id="5-结果"><a href="#5-结果" class="headerlink" title="5. 结果"></a>5. 结果</h1><p>图8中显示了与其他最新的物体检测器获得的结果的比较.YOLOv4位于帕累托最优曲线上，在速度和准确性方面均优于最快，最准确的检测器。</p><p>由于不同的方法使用不同架构的GPU进行推理时间验证，因此我们在Maxwell，Pascal和Voltaarchitectures常用的GPU上运行YOLOv4，并将它们与其他最新方法进行比较。</p><p>表8列出了使用Maxwell GPU的帧速率比较结果，可以是GTX Titan X（Maxwell）或Tesla M40 GPU。表9列出了使用Pascal GPU的帧率比较结果，可以是Titan X（Pascal），Titan Xp，GTX 1080 Ti或Tesla P100 GPU。表10，列出了使用VoltaGPU的帧率比较结果，可以是Titan Volta或Tesla V100 GPU。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5jv11i85j310j0u0nfv.jpg" srcset="/img/loading.gif" alt="image-20200727151317192"></p><h1 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h1><p>我们提出了一个最先进的目标检测器，它比所有检测器都要快而且更准确。这个检测器可以仅在一块8-16GB的GPU上进行训练，这使得它可以广泛的使用。One-stage的anchor-based的检测器的原始概念证明是可行的。我们已经验证了大量的特征，并且其用于提高分类器和检测器的精度。这些算法可以作为未来研究和发展的最佳实践。</p><h1 id="7-致谢"><a href="#7-致谢" class="headerlink" title="7. 致谢"></a>7. 致谢</h1><p>作者要感谢Glenn Jocher进行Mosaic data augmentation的思想，使用遗传算法选择超参数并解决网格敏感性问题.</p><p><img src="https://tva1.sinaimg.cn/large/0081Kckwly1gknjaujih4j30u012tgxr.jpg" srcset="/img/loading.gif" alt="image-20200727152219681"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5k4omssoj30u0135h22.jpg" srcset="/img/loading.gif" alt="image-20200727152234763"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh5k4v6chzj30u00wdgzc.jpg" srcset="/img/loading.gif" alt="image-20200727152245313"></p>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——YOLOv3:An Incremental Improvement</title>
    <link href="/2020/07/19/YOLOv3/"/>
    <url>/2020/07/19/YOLOv3/</url>
    
    <content type="html"><![CDATA[<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>我们给YOLO提供一些更新！ 我们做了一些小的设计更改以使其更好。 我们也训练了这个非常庞大的新网络。它比上次（YOLOv2）稍大一些，但更准确。它仍然很快，所以不用担心。在320×320的输入下，YOLOv3只需运行22.2ms，获得28.2的mAP，像SSD一样准确，但速度快三倍。当我们看看老的0.5 IOU mAP检测指标时，YOLOv3是相当不错的。在Titan X上，它在51 ms内实现了57.9的AP50，与RetinaNet在198 ms内实现的57.5 AP50对比，性能相似但速度快3.8倍。与往常一样，所有代码均在<a href="https://pjreddie.com/yolo/。" target="_blank" rel="noopener">https://pjreddie.com/yolo/。</a></p><h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h1><p>有时候，一年你主要只是在打电话，你知道吗？今年我没有做很多研究。我在Twitter上花了很多时间。玩了一下GAN。去年我留下了一点点的动力[10] [1]；我设法对YOLO进行了一些改进。但是诚然，没有什么比这超级有趣的了，只是一小堆（bunch）改变使它变得更好。我也帮助了其他人的做一些研究。</p><p>实际上，这就是今天把我们带到这里的原因。我们有一个付印截止日期，我们需要引用我对YOLO所做的一些随机更新，但我们没有源头。因此，为技术报告做准备！</p><p>关于技术报告的好处是他们不需要介绍，你们都知道我们为什么来到这里。因此，介绍部分的结尾将为论文剩余部分进行导引。首先我们会告诉你YOLOv3做了什么。然后我们会告诉你我们是怎么做的。我们还会告诉你我们尝试过的一些没有奏效的事情。最后，我们将考虑这一切意味着什么。</p><h1 id="2-详细内容"><a href="#2-详细内容" class="headerlink" title="2.详细内容"></a>2.详细内容</h1><p>这里是YOLOv3的详细内容：我们主要从其他人那里获得好点子。我们也训练了一个比其他人更好的新分类器网络。我们将带您从头开始学习整个系统，以便您完全理解所有内容。</p><h2 id="2-1-边界框预测"><a href="#2-1-边界框预测" class="headerlink" title="2.1 边界框预测"></a>2.1 边界框预测</h2><p>遵循YOLO9000，我们的系统使用维度聚类（dimension clusters ）作为锚盒（anchor boxes）来预测边界框[13]。网络为每个边界框预测4个坐标，tx，ty，tw，th。 如果单元格从图像的左上角偏移了（cx，cy）并且先验边界框的宽度和高度为pw，ph，则预测对应于：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggwltd4idbj30as08uwev.jpg" srcset="/img/loading.gif" alt="image-20200719213048979" style="zoom:50%;" /></center><p> 在训练期间，我们使用平方误差损失的总和。如果一些坐标预测的真值是t<sup>^</sup><sub>*</sub>，则我们的梯度是用真值（由ground box计算得到）减去我们的预测：t<sup>^</sup><sub>*</sub>-t<sub>*</sub>。 通过反转上面的方程可以很容易地计算出这个真值。</p><p>YOLOv3使用逻辑回归预测每个边界框的<strong>objectness score</strong>。如果某个边界框的先验与真值的重合率比其他先验边界框高，那么该边界框的<strong>objectness score</strong>分值为1。如果某个边界框的先验不是最好的，但是确实与真值重叠了一定的阈值以上，我们会和[17]R-CNN一样，忽略该预测。我们将这个阈值设为0.5。与[17]R-CNN不同，我们的系统为每个真值对象分配一个先验边界框。如果没有将边界框先验分配给真值对象，则不会对坐标或类预测造成任何损失，而只会引起<strong>objectness score</strong>损失。</p><p><code>YOLO V3 模型里有个objectness标签，开始我也是一脸懵逼。查阅资料后总结如下： objectness可以具象化理解为“置信度”；objectness对应的是布尔值类型的二分类标签通常是1和0（也有-1），理解成对应True和false逻辑值。 由于每个真实框只对应一个objectness标签为正的预测框，如果有些预测框跟真实框之间的IoU很大，但并不是最大的那个，那么直接将其objectness标签设置为0当作负样本，可能并不妥当。为了避免这种情况，YOLO-V3算法设置了一个IoU阈值iou_thresh，当预测框的objectness不为1，但是其与某个真实框的IoU大于iou_thresh时，就将其objectness标签设置为-1，不参与损失函数的计算。</code></p><h2 id="2-2-类别预测"><a href="#2-2-类别预测" class="headerlink" title="2.2 类别预测"></a>2.2 类别预测</h2><p>每个框使用多标签分类来预测边界框可能包含的类。我们不使用softmax，因为我们发现它对于高性能没有必要，相反，我们只是使用独立的逻辑分类器。 在训练过程中，我们使用<strong>二元交叉熵损失</strong>来进行类别预测。这个公式有助于我们转向更复杂的领域，如<a href="https://github.com/openimages" target="_blank" rel="noopener">Open Image Dataset</a>[5]。在这个数据集中有许多重叠的标签（如女性和人物）。使用softmax会强加了一个假设，即每个框中只有一个类别，但通常情况并非如此。多标签方法更好地对数据建模。</p><h2 id="2-3-跨尺度预测"><a href="#2-3-跨尺度预测" class="headerlink" title="2.3 跨尺度预测"></a>2.3 跨尺度预测</h2><p>YOLOv3在3种不同尺度上预测边界框。我们的系统使用与特征金字塔网络相同的概念，来从这些尺度中提取特征。从我们的基本特征提取器中，我们添加了几个卷积层。其中最后一个卷积层预测了一个三维张量，编码了：边界框预测，objectness分值,和类别预测。在COCO上的实验中，在每个尺度上预测三个框，因此张量为NxNx[3x(4+1+80)]，4个边界框偏移量，1个<strong>objectness</strong> prediction，和80个类别预测。</p><p>接下来，我们从先前的2层中获取特征图，并将其上采样2倍。再使用级联对原来网络的特征图和上采样后的特征图进行合并。这种方法使我们能够从上采样的特征中获得更有意义的语义信息，并从较早的特征图中获得<strong>更细粒度的信息</strong>。然后，我们再添加一些卷积层以处理这种组合的特征图，并最终预测相似的张量，尽管现在的大小是原来的两倍。</p><p>我们再次执行相同的设计来预测最终尺度的方框。因此，我们对第三种尺度的预测将从所有先前的计算中获益，并从早期的网络中获得<strong>细粒度的特征</strong>。</p><p>我们仍然使用k-means聚类来确定我们的先验边界框。我们只是随意选择了9个聚类和3个尺度，然后在各个比例上均匀地划分了聚类。在COCO数据集上，9个聚类是：（10×13）;（16×30）;（33×23）;（30×61）;（62×45）; （59×119）; （116×90）; （156×198）; （373×326）。</p><h2 id="2-4-特征提取"><a href="#2-4-特征提取" class="headerlink" title="2.4 特征提取"></a>2.4 特征提取</h2><p>我们使用新的网络来实现特征提取。我们的新网络是YOLOv2，Darknet-19中使用的网络与新的残差网络内容之间的混合方法。我们的网络使用了连续的3×3和1×1卷积层，但现在也有了一些捷径（shortcut）连接，并且明显更大。它有53个卷积层，所以我们称之为Darknet-53！</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxgvej68zj30q211u7ac.jpg" srcset="/img/loading.gif" alt="image-20200720152518167" style="zoom:33%;" /></center><p>这个新的网络比Darknet-19更强大，而且比ResNet-101、ResNet-152更有效。以下是在ImageNet上的结果。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gh0onrtmdsj30vk07owg0.jpg" srcset="/img/loading.gif" alt="image-20200720152801106" style="zoom:50%;" /></p><p>表2 骨干对比。精度，数十亿次操作，每秒十亿次浮点操作以及各种网络的FPS。</p><p>每个网络都使用相同的设置进行训练，并以256×256的单精度测试进行测试。运行时间是在Titan X上以256×256进行测量的。因此Darknet-53可与最先进的分类器媲美，但浮点运算更少，速度更快。Darknet-53比ResNet-101更好，速度提高了1.5倍。Darknet-53具有与ResNet-152相似的性能，并且快2倍。</p><p>Darknet-53还实现了每秒最高的测量浮点运算。这意味着网络结构可以更好地利用GPU，从而使其评估效率更高，速度更快。这主要是因为ResNets层太多了，效率不高。</p><h2 id="2-5-训练"><a href="#2-5-训练" class="headerlink" title="2.5 训练"></a>2.5 训练</h2><p>我们仍然会训练完整的图像，没有使用难例挖掘(hard negative mining)和其他任何方法。我们使用多尺度训练，大量的数据增强方法，批量规范化，以及所有标准的东西。 使用Darknet神经网络框架进行训练和测试。</p><h1 id="3-我们如何做"><a href="#3-我们如何做" class="headerlink" title="3. 我们如何做"></a>3. 我们如何做</h1><p>YOLOv3性能很好，见表3。在COCO数据集上，它的mAP与SSD及其变体相当，但比它快三倍。尽管如此，与其它模型如RetinaNet相比还是稍显落后。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxhnxaglzj31dk0gyte7.jpg" srcset="/img/loading.gif" alt="image-20200720155245562"></p><p>表3  认真地说，我是从[9]中偷走了所有这些表，他们花了很长时间才从头开始制作。好的，YOLOv3一切正常。请记住，RetinaNet的图像处理时间要长3.8倍。YOLOv3比SSD变体好得多，可与最先进的模型在AP<sub>50</sub>指标上相媲美。</p><p>但是，当我们以IOU = 0.5（或图表中的AP50）查看mAP的“旧”检测指标时，YOLOv3非常强大。它几乎与RetinaNet相当，并且远远超过SSD变体。这表明YOLOv3是一个非常强大的检测器，擅长于为目标生成像样的边界框。但是，随着IOU阈值的增加，性能会显着下降，这表明YOLOv3难以使框与目标完美对齐。</p><p>过去，YOLO在小目标检测上表现一直不好。但是，现在我们看到了这种趋势的逆转。通过新的多尺度预测，我们看到YOLOv3具有相对较高的APS性能。但是，它在中型和大型目标上的性能相对较差。要深入了解这一点，还需要进行更多调查。</p><p>当我们在AP50指标上绘制精度与速度的关系图（参见图3）时，我们看到YOLOv3比其他检测系统具有明显优势。即更快，更好。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggxkujacpgj317k0s07a9.jpg" srcset="/img/loading.gif" alt="image-20200720174254282"></p><p>图3.再次根据[9]进行改编，这显示了在mAP上以0.5 IOU指标的速度/精度折衷。您可以说YOLOv3很好，因为它很高而且非常靠左。你可以引用自己的论文吗？猜猜谁要尝试，这个人→[16]。哦，我忘了，我们还修复了YOLOv2中的数据加载错误，该错误通过2 mAP的帮助而得以解决。只是潜入这里不放弃布局。</p><h1 id="4-我们尝试了但无效的事情"><a href="#4-我们尝试了但无效的事情" class="headerlink" title="4. 我们尝试了但无效的事情"></a>4. 我们尝试了但无效的事情</h1><p>在研究YOLOv3时，我们尝试了很多东西。很多都行不通。这是我们能记住的东西。</p><p><strong>锚框x,y偏移量预测。</strong>我们尝试使用标准锚框预测机制，在该机制中，您可以使用线性激活将x，y偏移量预测为框宽度或高度的倍数。我们发现此公式降低了模型的稳定性，并且效果不佳。</p><p><strong>线性x,y预测而不是逻辑预测。</strong>我们尝试使用线性激活直接预测x，y偏移，而不是逻辑激活。这导致mAP下降了两点。</p><p><strong>Focal Loss</strong> 我们尝试使用焦点损失(focal loss)。它使我们的AP下降了约2点。YOLOv3可能已经对解决焦点损失正在尝试解决的问题具有鲁棒性，因为YOLOv3具有objectness预测和条件类预测。因此，对于大多数示例而言，分类预测不会带来损失吗？或者其他的东西？我们不确定。</p><p><strong>双IOU阈值和真值分配。</strong>Faster R-CNN在训练期间使用两个IOU阈值。如果预测值与真值的重叠率为0.7，则它是一个正例；如果在0.3-0.7之间则被忽略，小于0.3为负例。我们尝试了类似的策略，但收效不佳。</p><p>我们非常喜欢我们当前的公式，似乎至少是局部最优。这些技术中的某些可能最终会产生良好的结果，也许它们只需要进行一些调整即可稳定训练。</p><h1 id="5-这一切意味着什么"><a href="#5-这一切意味着什么" class="headerlink" title="5. 这一切意味着什么"></a>5. 这一切意味着什么</h1><p>YOLOv3是一个好的检测器，它又快又准。在COCO数据集上，使用IOU在0.5到0.95上的AP作为度量时，它并不是太好。但采用旧的0.5IOU的度量时，它的性能非常好。</p><p>为什么我们无论如何要转换指标？原始的COCO论文只是含糊不清的句子：”评估服务器完成后，将添加对评估指标的完整讨论”。Russakovsky等人说，人类很难区分.3和.5的IOU！令人惊讶的是，训练人员以视觉方式检查IOU为0.3的边界框并将其与IOU为0.5的边界框区分开来是非常困难的。人类很难分辨出差异，这有多重要？</p><p>但是也许更好的问题是：“既然有了探测器，我们将如何处理这些探测器？从事这项研究的许多人都在Google和Facebook。我想至少我们知道这项技术掌握得很好，而且绝对不会被用来收集您的个人信息并将其出售给…。等等，您是说这就是它的用途？</p><p>好吧，那些为视觉研究投入大量资金的人是军队，他们从来没有做过任何可怕的事情，例如用新技术杀死许多人，等等。</p><p>我非常希望大多数使用计算机视觉的人都在用它来做快乐，好事，例如统计国家公园中斑马的数量[13]，或者当猫徘徊在屋子里时追踪它们的踪迹。但是计算机视觉已经被质疑使用，并且作为研究人员，我们有责任至少考虑到我们的工作可能造成的危害，并想出减轻它的方法。我们非常感激这个世界。</p><p>最后，不要@我。（因为我终于退出了Twitter）。</p>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——YOLO9000:Better,Faster,Stronger(YOLOv2)</title>
    <link href="/2020/07/17/YOLOv2/"/>
    <url>/2020/07/17/YOLOv2/</url>
    
    <content type="html"><![CDATA[<h1 id="YOLO9000-Better-Faster-Stronger"><a href="#YOLO9000-Better-Faster-Stronger" class="headerlink" title="YOLO9000:Better,Faster,Stronger"></a>YOLO9000:Better,Faster,Stronger</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们介绍了YOLO9000，这是一种先进的实时对象检测系统，可以检测9000多个对象类别。首先，我们提出了对YOLO检测方法的各种改进，既有新颖性，也有前期的工作。改进后的模型YOLOv2在PASCAL VOC和COCO等标准检测任务上是最先进的。使用一种新颖的，多尺度训练方法，同样的YOLOv2模型可以以不同的尺寸运行，从而在速度和准确性之间提供了一个简单的折衷。在67FPS时，YOLOv2在VOC 2007上获得了76.8 mAP。在40FPS时，YOLOv2获得了78.6 mAP，比使用ResNet的Faster R-CNN和SSD等先进方法表现更出色，同时仍能以更快的速度运行。最后，我们提出了一种联合训练目标检测和分类的方法。使用这种方法，我们在COCO检测数据集和ImageNet分类数据集上同时训练YOLO9000。我们的联合训练允许YOLO9000预测未标注的检测数据的物体类别的检测结果。我们在ImageNet检测任务上验证了我们的方法。YOLO9000在ImageNet检测验证集上获得19.7 mAP，尽管200个类别中只有44个具有检测数据。在COCO数据集不包含的156个类别上，YOLO9000获得16.0 mAP。但YOLO可以检测到200多个类别。YOLO9000可以实时预测9000多种不同对象类别的检测结果。</p><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>通用目标检测应该快速，准确并且能够识别各种各样的目标。自从引入神经网络以来，检测框架已经变得越来越快和准确。但是，大多数检测方法仍然局限于少数目标。</p><p>与其他任务（如分类和标记）的数据集相比，当前的目标检测数据集受到限制。最常见的检测数据集包含成千上万到数十万张具有成百上千个标签的图像。分类数据集包含数百万个具有数万或数十万个类别的图像。</p><p>我们希望检测能够扩展到目标分类的级别。但是，为检测标记图像比为分类或贴标签标记图像要昂贵得多（标签通常是用户免费提供的）。因此，我们不太可能在近期内看到与分类数据集相同规模的检测数据集。</p><p>我们提出了一种新方法来利用我们已经拥有的大量分类数据，并使用它来扩展当前检测系统的范围。我们的方法使用对象分类的分层视图，使我们可以将不同的数据集组合在一起。</p><p>我们还提出了一种联合训练算法，该算法允许我们在检测数据和分类数据上训练目标检测器。我们的方法利用标记的检测图像来学习精确定位对象，同时使用分类图像来增加其词汇量和鲁棒性。使用这种方法，我们训练了YOLO9000，这是一种实时对象检测器，可以检测9000多种不同的目标类别。首先，我们改进YOLO基础检测系统，产生最先进的实时检测器YOLOv2。然后我们使用数据集组合方法和联合训练算法来训练来自ImageNet的9000多个类别的模型以及来自COCO的检测数据。</p><p>我们的所有代码和预训练模型都可在线获得：<a href="https://link.jianshu.com/?t=http%3A%2F%2Fpjreddie.com%2Fyolo9000%2F" target="_blank" rel="noopener">http://pjreddie.com/yolo9000/</a>。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggu2vddexcj30u01sph4n.jpg" srcset="/img/loading.gif" alt="img"></p><center>图1：YOLO9000. YOLO9000可以实时检测多种目标类别</center><h2 id="2-更好"><a href="#2-更好" class="headerlink" title="2. 更好"></a>2. 更好</h2><p>与最先进的检测系统相比，YOLO存在许多缺点。YOLO与Fast R-CNN的误差分析对比表明，YOLO有大量的定位误差。此外，与基于区域提出的方法相比，YOLO召回率相对较低。因此，我们主要侧重于提高召回率和改进定位，同时保持分类准确性。</p><p>计算机视觉一般趋向于更大，更深的网络。更好的性能通常取决于训练更大的网络或将多个模型组合在一起。但是，在YOLOv2中，我们需要一个更精确的检测器，它仍然很快。我们不是扩大我们的网络，而是简化网络，然后让表示更容易学习。我们将过去的工作与我们自己的新概念汇集起来，以提高YOLO的性能。表2列出了结果总结。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggu3bsnlfwj30uk0ck0wa.jpg" srcset="/img/loading.gif" alt="img"></p><p><strong>表2：从YOLO到YOLOv2的路径。</strong>列出的大部分设计决定都会导致mAP的显著增加。有两个例外是切换到具有Anchor Box的一个全卷积网络和使用新网络。切换到Anchor Box风格的方法增加了召回，而不改变mAP，而使用新网络会削减33%的计算量。</p><p><strong>批标准化。</strong>批标准化导致收敛性的显著改善，同时消除了对其他形式正则化的需求[7]。通过在YOLO的所有卷积层上添加批标准化，我们在mAP中获得了超过2%的提升。批标准化也有助于模型正则化。通过批标准化，我们可以从模型中删除dropout而不会过拟合。</p><p><strong>高分辨率分类器</strong>。所有最先进的检测方法都使用在ImageNet[16]上预训练的分类器。从AlexNet开始，大多数分类器对小于256×256[8]的输入图像进行操作。原来的YOLO以224×224的分辨率训练分类器网络，并将分辨率提高到448进行检测。这意味着网络必须同时切换到学习目标检测和调整到新的输入分辨率。</p><p>对于YOLOv2，我们首先ImageNet上以448×448的全分辨率对分类网络进行10个迭代周期的微调。这给了网络时间来调整其滤波器以便更好地处理更高分辨率的输入。然后在检测时对结果网络进行微调。这个高分辨率分类网络使我们增加了近4%的mAP。</p><p><strong>具有Anchor Box的卷积</strong>。YOLO使用卷积特征提取器顶部的完全连接层直接预测边界框的坐标。Faster R-CNN使用手动选择优先来预测边界框而不是直接预测坐标[15]。Faster R-CNN中的区域提议网络（RPN）仅使用卷积层来预测Anchor Box的偏移量和置信度。由于预测层是卷积的，因此RPN会在特征图中的每个位置预测这些偏移。预测偏移而不是坐标简化了问题，并且使网络更容易学习。</p><p>我们从YOLO中移除全连接层，并使用Anchor Box来预测边界框。首先，消除一个池化层，以使网络卷积层的输出具有更高的分辨率。我们还缩小了网络，操作416×416的输入图像而不是448×448。之所以这样做，是因为我们希望特征图中的位置数为奇数，因此只有一个中心单元。对象（尤其是大对象）往往会占据图像的中心，因此最好在中心位置使用一个位置来预测这些对象，而不要使用附近的四个位置。YOLO的卷积层将图像下采样32倍，因此使用416的输入图像，我们得到了13×13的输出特征图。</p><p>当我们转向锚框时，我们还将类预测机制与空间位置分离，而不是为每个锚框预测类和客观性。YOLO之后，目标预测仍然预测了实际值和提出的边界框的IOU，并且类别预测预测了当存在目标时该类别的条件概率。</p><p>使用锚框，准确性会略有下降。YOLO只能预测每个图像98个框，但使用锚框，我们的模型可以预测一千多个框。没有锚框，我们的中间模型将获得69.5 mAP，81％的召回率。使用Anchor Box，我们的模型获得69.2 mAP，召回率达到88％。尽管mAP下降，但召回率的上升意味着我们的模型有更大的提升空间。</p><p><strong>维度聚类</strong>。当Anchor Box与YOLO一起使用时，我们遇到了两个问题。首先是边界框的尺寸是手工挑选的。网络可以学习适当调整边界框，但如果我们为网络选择更好的先验，我们可以使网络更容易学习它以便预测好的检测。</p><p>无需手动选择先验，我们在训练集边界框上运行k-means聚类以自动找到良好的先验。如果我们使用具有欧几里得距离的标准k-means，那么较大的边界框比较小的边界框产生更多的误差。然而，我们真正想要的是导致好的IOU分数的先验，这是独立于边界框大小的。因此，对于我们的距离度量，我们使用：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gguar2msr4j30my03iwep.jpg" srcset="/img/loading.gif" alt="image-20200717213653687" style="zoom:50%;" /></center><p>我们运行各种k值的k-means，并画出平均IOU与最接近的几何中心，见图2。我们选择k=5作为模型复杂性和高召回率之间的良好折衷。聚类中心与手工挑选的Anchor Box明显不同。有更短更宽的边界框和更高更细的边界框。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggudtnfx6mj30to0gogn0.jpg" srcset="/img/loading.gif" alt="image-20200717214016397"></p><p><strong>图2：VOC和COCO的聚类边界框尺寸</strong>。我们对边界框的维度进行k-means聚类，以获得我们模型的良好先验。左图显示了我们通过对k的各种选择得到的平均IOU。我们发现k = 5给出了一个很好的召回率与模型复杂度的权衡。右图显示了VOC和COCO的相对中心。这两种先验都赞成更薄更高的边界框，而COCO比VOC在尺寸上有更大的变化。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gguazn4do7j30jg07kgmk.jpg" srcset="/img/loading.gif" alt="image-20200717214508193" style="zoom:50%;" /></center><p><strong>表1：VOC 2007上最接近先验的边界框平均IOU</strong>。VOC 2007上目标的平均IOU与其最接近的，使用不同生成方法之前未经修改的平均值。聚类结果比使用手工选择的先验结果要更好。</p><p>在表1中我们将平均IOU与我们聚类策略中最接近的先验以及手工选取的Anchor Box进行了比较。仅有5个先验中心的平均IOU为61.0，其性能类似于9个Anchor Box的60.9。如果我们使用9个聚类中心，我们会看到更高的平均IOU。这表明使用k-means来生成我们的边界框会以更好的表示开始训练模型，并使得任务更容易学习。</p><blockquote><p>（ YOLOv2采用的5种Anchor的Avg IOU是61，而采用9种Anchor Boxes的Faster RCNN的Avg IOU是60.9，也就是说本文仅选取5种box就能达到Faster RCNN的9中box的效果。选择值为9的时候，AVG IOU更有显著提高。说明K-means方法的生成的boxes更具有代表性。）</p></blockquote><p><strong>直接位置预测</strong>。当YOLO使用Anchor Box时，我们会遇到第二个问题：模型不稳定，特别是在早期的迭代过程中。大部分的不稳定来自预测边界框的(x,y)位置。在区域提出网络中，网络预测值t_x和t_y，以及(x,y)中心坐标计算如下：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggub92n8jrj30bo048q32.jpg" srcset="/img/loading.gif" alt="image-20200717215411895" style="zoom:50%;" /></center><p>例如，若预测为tx=1，则将框向右移动锚框的宽度，预测为tx=-1将使框向左移动相同的量</p><p>这个公式是不受限制的，所以任何Anchor Box都可以在图像任一点结束，而不管在哪个位置预测该边界框。随机初始化模型需要很长时间才能稳定以预测合理的偏移量。我们没有预测偏移量，而是按照YOLO的方法预测相对于网格单元位置的位置坐标。这将真实值限制落到0和1之间。我们使用逻辑激活约束网络的预测落入此范围。</p><p>网络在输出特征图中对每个单元预测5个边界框。网络为每个边界框预测5个坐标，t_x，t_y，t_w，t_h和t_o。如果单元从图像的左上角偏移了(c_x, c_y)，并且边界框先验的宽度和高度为p_w，p_h，那么预测对应：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggubnbd1cjj30mu0b2js8.jpg" srcset="/img/loading.gif" alt="image-20200717220752807" style="zoom:50%;" /></center><p>由于我们限制位置预测参数化更容易学习，使网络更稳定。使用维度聚类以及直接预测边界框中心位置的方式比使用Anchor Box的版本将YOLO提高了近5%。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggubxlpzb4j30uc0toadf.jpg" srcset="/img/loading.gif" alt="image-20200717221746076" style="zoom: 50%;" /></center><p><strong>图3：具有维度先验和位置预测的边界框</strong>。我们预测边界框的宽度和高度作为聚类中心的偏移量。我们使用sigmoid函数预测边界框相对于滤波器应用位置的中心坐标。</p><p><strong>细粒度功能</strong>。这个修改后的YOLO在13×13特征映射上预测检测结果。虽然这对于大型目标来说已经足够了，但它可以从用于定位较小目标的更细粒度的特征中受益。Faster R-CNN和SSD都在网络的各种特征映射上运行他们提出的网络，以获得一系列的分辨率。我们采用不同的方法，仅仅添加一个通道层，从26x26分辨率的更早层中提取特征。</p><p>与ResNet中的身份映射相似，直通层通过将相邻要素堆叠到不同的通道中而不是空间位置，从而将高分辨率特征与低分辨率特征连接在一起。这会将26×26×512特征图转换为13×13×2048的特征图，可与原始特征级联使用。我们的检测器在此扩展的特征图上运行，因此可以访问细粒度的特征。</p><p><strong>多尺度训练</strong>。原来的YOLO使用448×448的输入分辨率。通过添加Anchor Box，我们将分辨率更改为416×416。但是，由于我们的模型只使用卷积层和池化层，因此它可以实时调整大小。我们希望YOLOv2能够鲁棒的运行在不同大小的图像上，因此我们可以将其训练到模型中。</p><p>我们不固定输入图像的大小，而是每隔几次迭代就更改网络。每隔10个批次我们的网络会随机选择一个新的图像尺寸大小。由于我们的模型下采样了32倍，因此我们从以下32的倍数中提取：{320,352，…，608}。因此最小的选项是320×320，最大的是608×608。我们调整网络的尺寸并继续训练。</p><p>这种机制迫使网络学习如何在各种输入维度上做好预测。这意味着相同的网络可以预测不同分辨率下的检测结果。在更小尺寸上网络运行速度更快，因此YOLOv2在速度和准确性之间提供了一个简单的折中。</p><p>在低分辨率YOLOv2作为一个便宜，相当准确的检测器。在288×288的分辨率下，其运行速度超过90FPS，mAP与Fast R-CNN差不多。这使其成为小型GPU，高帧率视频或多视频流的理想选择。</p><p>在高分辨率下，YOLOv2是VOC 2007上最先进的检测器，达到了78.6 mAP，同时仍保持运行在实时速度之上。请参阅表3，了解YOLOv2与VOC 2007其他框架的比较。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggudekqsiwj30tc0i8n12.jpg" srcset="/img/loading.gif" alt="image-20200717230840914" style="zoom:50%;" /></center><p><strong>表3：PASCAL VOC 2007的检测框架</strong>。YOLOv2比先前的检测方法更快，更准确。它也可以以不同的分辨率运行，以便在速度和准确性之间进行简单折衷。每个YOLOv2条目实际上是具有相同权重的相同训练模型，只是以不同的大小进行评估。所有的时间信息都是在Geforce GTX Titan X（原始的，而不是Pascal模型）上测得的。图4。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggudgi3ralj30t00o6wgt.jpg" srcset="/img/loading.gif" alt="image-20200717231030953" style="zoom:50%;" /></center><p><strong>进一步实验</strong>。我们在VOC 2012上训练YOLOv2进行检测。表4显示了YOLOv2与其他最先进的检测系统的比较性能。YOLOv2取得了73.4 mAP同时运行速度比竞争方法快的多。我们在COCO上进行了训练，并在表5中与其他方法进行比较。在VOC度量（IOU = 0.5）上，YOLOv2得到44.0 mAP，与SSD和Faster R-CNN相当。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggudk9d4sxj31g208k787.jpg" srcset="/img/loading.gif" alt="image-20200717231409033"></p><p><strong>表4：PASCAL VOC2012 test上的检测结果</strong>。YOLOv2与最先进的检测器如具有ResNet的Faster R-CNN、SSD512在标准数据集上运行，YOLOv2比它们快2-10倍。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggudlxhz3aj319g0dkjvt.jpg" srcset="/img/loading.gif" alt="image-20200717231544822"></p><center>表5：在COCO test-dev2015上的结果。表参考[11]</center><h2 id="3-更快"><a href="#3-更快" class="headerlink" title="3. 更快"></a>3. 更快</h2><p>我们希望检测是准确的，但我们也希望它是快速的。大多数检测应用程序（例如机器人技术或自动驾驶汽车）都依赖于低延迟预测。为了最大化性能，我们将YOLOv2设计为从一开始就非常快。</p><p>大多数检测框架依赖于VGG-16作为的基本特征提取器[17]。VGG-16是一个强大的，准确的分类网络，但它是不必要的复杂。VGG-16的卷积层需要对224×224分辨率的单幅图像进行306.9亿次浮点运算。</p><p>YOLO框架使用基于Googlenet架构[19]的自定义网络。这个网络比VGG-16更快，一次前馈传播只有85.2亿次的操作。然而，它的准确性比VGG-16略差。在ImageNet上，对于224×224分辨率下的单张裁剪图像，YOLO的自定义模型获得了88.0%的top-5准确率，而VGG-16则为90.0%。</p><p><strong>Darknet-19</strong>。我们提出了一个新的分类模型作为YOLOv2的基础。我们的模型建立在网络设计的先期工作以及该领域的常识基础之上。与VGG模型类似，我们主要使用3×3滤波器，并在每个池化步骤之后使通道数量加倍[17]。按照Network in Network（NIN）的工作，我们使用全局平均池化做预测以及1×1滤波器来压缩3×3卷积之间的特征表示[9]。我们使用批标准化来稳定训练，加速收敛，并正则化模型[7]。</p><p>我们的最终模型叫做Darknet-19，它有19个卷积层和5个最大池化层。完整描述请看表6。Darknet-19只需要55.8亿次运算来处理一张图像，但在ImageNet上却达到了72.9$的top-1准确率和91.2%的top-5准确率。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gguenfnukmj30jm0rgq7c.jpg" srcset="/img/loading.gif" alt="image-20200717235147844" style="zoom:60%;" /></center><p><strong>训练分类</strong>我们使用Darknet神经网络框架，使用随机梯度下降法在160个迭代的标准ImageNet 1000类分类数据集上训练网络，其初始学习率为0.1，多项式率衰减为4的幂，权重衰减为0.0005，动量为0.9，使用Darknet神经网络框架[13]。在训练过程中，我们使用标准的数据增强技巧，包括随机裁剪，旋转以及色调，饱和度和曝光偏移</p><p>如上所述，在我们对224×224的图像进行初始训练之后，我们对网络在更大的448x448尺寸上进行了微调。对于这种微调，我们使用上述参数进行训练，但是只有10个迭代周期，并且以10<sup>-3</sup>的学习率开始。在这种更高的分辨率下，我们的网络达到了76.5%的top-1准确率和93.3%的top-5准确率。</p><p><strong>训练检测</strong> 我们修改这个网络进行检测，删除了最后一个卷积层，加上了三个具有1024个滤波器的3×3卷积层，其后是最后的1×1卷积层与我们检测需要的输出数量。对于VOC，我们预测5个边界框，每个边界框有5个坐标和20个类别，所以有125个滤波器。我们还添加了从最后的3×3×512层到倒数第二层卷积层的直通层，以便我们的模型可以使用细粒度特征。</p><p>我们训练网络160个迭代周期，初始学习率为10<sup>-3</sup>，在60个和90个迭代周期时将学习率除以10。我们使用0.0005的权重衰减和0.9的动量。我们对YOLO和SSD进行类似的数据增强，随机裁剪，色彩偏移等。我们对COCO和VOC使用相同的训练策略。</p><h2 id="4-更强"><a href="#4-更强" class="headerlink" title="4. 更强"></a>4. 更强</h2><p>我们提出了一个联合训练分类和检测数据的机制。我们的方法使用标记过的图像进行检测，以学习特定于检测的信息，例如边界框坐标预测和objectness，以及如何对常见对象进行分类。它使用仅带有类别标签的图像来扩展它可以检测到的类别数量。</p><p>在训练期间，我们混合来自检测和分类数据集的图像。当我们的网络看到标记为检测的图像时，我们可以基于完整的YOLOv2损失函数进行反向传播。当它看到一个分类图像时，我们只能从该架构的分类特定部分反向传播损失。</p><p>这种方法提出了一些挑战。检测数据集只有通用目标和通用标签，如“狗”或“船”。分类数据集具有更广更深的标签范围。ImageNet有超过一百种品种的狗，包括<code>Norfolk terrier</code>，<code>Yorkshire terrier</code>和<code>Bedlington terrier</code>。如果我们想在两个数据集上训练，我们需要一个连贯的方式来合并这些标签。</p><p>大多数分类方法都在所有可能的类别中使用softmax层来计算最终概率分布。使用softmax假定这些类是相互排斥的。这给数据集的组合带来了问题，例如你不想用这个模型来组合ImageNet和COCO，因为类<code>Norfolk terrier</code>和<code>dog</code>不是相互排斥的。</p><p>我们可以改为使用多标签模型来组合不假定互斥的数据集。这种方法忽略了我们已知的关于数据的所有结构，例如，所有的COCO类是互斥的。</p><p><strong>分层分类</strong>。ImageNet标签是从WordNet中提取的，这是一个构建概念及其相互关系的语言数据库[12]。在WordNet中，<code>Norfolk terrier</code>和<code>Yorkshire terrier</code>都是<code>terrier</code>的下义词，<code>terrier</code>是一种<code>hunting dog</code>，<code>hunting dog</code>是<code>dog</code>，<code>dog</code>是<code>canine</code>等。分类的大多数方法为标签假设一个扁平结构，但是对于组合数据集，结构正是我们所需要的。</p><p>WordNet的结构是有向图，而不是树，因为语言是复杂的。例如，<code>dog</code>既是一种<code>canine</code>犬，也是一种<code>domestic animal</code>家畜，它们都是WordNet中的同义词。我们不是使用完整的图结构，而是通过从ImageNet的概念中构建分层树来简化问题。</p><p>为了构建这棵树，我们检查了ImageNet中的视觉名词，并查看它们通过WordNet图到根节点的路径，在这种情况下是“物理对象”。许多同义词集在图中只有一条路径，因此首先我们将所有这些路径添加到树中。然后我们迭代检查剩下的概念，并添加使树长得尽可能小的路径。所以如果一个概念有两条路径到一个根，一条路径会给我们的树增加三条边，另一条只增加一条边，我们选择更短的路径。</p><p>最终的结果是WordTree，一个视觉概念的分层模型。为了使用WordTree进行分类，我们预测每个节点的条件概率，以得到同义词集合中每个同义词下义词的概率。例如，在<code>terrier</code>节点我们预测：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gguvxc6koxj30i408u0te.jpg" srcset="/img/loading.gif" alt="image-20200718094929477" style="zoom:33%;" /></center><p>如果我们想要计算一个特定节点的绝对概率，我们只需沿着通过树到达根节点的路径，再乘以条件概率。所以如果我们想知道一张图片是否是<code>Norfolk terrier</code>，我们计算：</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gguw3jyqd8j30rc0acq49.jpg" srcset="/img/loading.gif" alt="image-20200718095528061" style="zoom: 33%;" /></center><p>为了分类目的，我们假定图像包含一个目标：Pr(physical object) = 1.</p><p>为了验证这种方法，我们在使用1000类ImageNet构建的WordTree上训练Darknet-19模型。为了构建WordTree1k，我们在所有中间节点，将标签空间从1000扩展到1369。在训练过程中，我们将真实标签向树上面传播，以便如果图像被标记为<code>Norfolk terrier</code>，则它也被标记为<code>dog</code>和<code>mammal</code>等。为了计算条件概率，我们的模型预测了具有1369个值的向量，并且我们计算了相同概念的下义词在所有同义词集上的softmax，见图5。</p><center><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gguwef1ucyj30wm0s6djh.jpg" srcset="/img/loading.gif" alt="image-20200718100554426" style="zoom:50%;" /></center><p><strong>图5：在ImageNet与WordTree上的预测</strong>。大多数ImageNet模型使用一个较大的softmax来预测概率分布。使用WordTree，我们可以在共同的下义词上执行多次softmax操作。</p><p>使用与以前相同的训练参数，我们的分级Darknet-19达到71.9%的<code>top-1</code>准确率和90.4%的<code>top-5</code>准确率。尽管添加了369个其他概念，并且让我们的网络预测了树形结构，但我们的准确性仅略有下降。以这种方式进行分类也有一些好处。在新的或未知的对象类别上，性能会明显下降。例如，如果网络看到了一条狗的图片，但不确定它是哪种类型的狗，它仍将以较高的置信度预测“狗”，但在下位词中的置信度较低。</p><p>这个构想也适用于检测。现在，我们不是假定每张图像都有一个目标，而是使用YOLOv2的目标预测器给我们Pr(physical object)的值。我们向下遍历树，在每次拆分时都采用最高置信度，直到达到某个阈值并预测该对象类别。</p><p><strong>数据集与WordTree组合</strong> 我们可以使用WordTree以明智的方式将多个数据集组合在一起。我们仅将树中的数据集中的类别映射到同义词集中。图6显示了使用WordTree组合ImageNet和COCO的标签的示例。WordNet非常多样化，因此我们可以对大多数数据集使用此技术。</p><p><strong>联合分类和检测</strong>。现在我们可以使用WordTree组合数据集，我们可以在分类和检测上训练联合模型。我们想要训练一个非常大规模的检测器，所以我们使用COCO检测数据集和完整的ImageNet版本中的前9000个类来创建我们的组合数据集。我们还需要评估我们的方法，以便我们添加ImageNet检测挑战中尚未包含的任何类。此数据集的相应WordTree具有9418个类。ImageNet是一个更大的数据集，因此我们通过对COCO进行过度采样来平衡数据集，以使ImageNet仅以4：1的比例扩大。</p><p>使用此数据集，我们训练YOLO9000。我们使用基于YOLOv2的体系结构，但是仅使用3个优先级而不是5个优先级来限制输出大小。当我们的网络看到检测图像时，我们会像往常一样反向传播损失。对于分类损失，仅反向传播等于或高于标签相应水平的损失。例如，如果标签为“ dog”，我们会在树中“ GermanShepherd”与“ Golden Retriever”之间的预测中分配任何误差，因为我们没有该信息。</p><p>当看到分类图像时，我们仅反向传播分类损失。为此，我们只需找到预测该类别最高概率的边界框，我们仅在其预测树上计算损失。我们还假设，预测框至少与真值标签重叠0.3IOU。我们根据这个假设反向传播目标损失。</p><p>使用这种联合训练，YOLO9000学习使用COCO中的检测数据来查找图像中的目标，并学习使用来自ImageNet的数据对各种目标进行分类。</p><p>我们在ImageNet检测任务上评估YOLO9000。ImageNet的检测任务与COCO共享44个对象类别，这意味着YOLO9000仅具有大多数测试类别的可见分类数据。YOLO9000在从未见过任何标记的检测数据的情况下，整体上获得了19.7 mAP，在不相交的156个目标类别中获得了16.0 mAP。</p><p>该mAP高于DPM达到的结果，但是YOLO9000在不同的数据集上进行了仅部分监督的训练[4]。它还可以同时实时检测9000个其他类别。</p><p>YOLO9000很好地学习了新的动物种类，但是却在学习服装和设备等学习类别时遇到了麻烦。新动物更容易学习，因为目标预测可以从COCO中的动物泛化的很好。相反，COCO没有任何类型的衣服的边界框标签，只针对人，因此YOLO9000正在努力建模“墨镜”或“泳裤”等类别。</p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>我们介绍了YOLOv2和YOLO9000，两个实时检测系统。YOLOv2在各种检测数据集上都是最先进的，也比其他检测系统更快。此外，它可以运行在各种图像大小，以提供速度和准确性之间的平滑折衷。</p><p>YOLO9000是通过联合优化检测和分类来检测9000多个对象类别的实时框架。我们使用WordTree合并来自各种来源的数据，并使用联合优化技术在ImageNet和COCO上同时进行训练。YOLO9000是朝着缩小检测与分类之间的数据集大小差距迈出的重要一步。</p><p>我们的许多技术都可以泛化到目标检测之外。我们提出的ImageNet的WordTree表示形式为图像分类提供了更丰富，更详细的输出空间。使用分层分类的数据集组合在分类和分割领域将是有用的。诸如多尺度训练之类的训练技术可以为各种视觉任务提供益处。</p><p>对于未来的工作，我们希望使用类似的技术来进行弱监督图像分割。我们还计划使用更强大的匹配策略为训练过程中将弱标签分配给分类数据，以改善检测结果。计算机视觉拥有大量的被标记数据。我们将继续寻找方法，将不同的数据源和不同的数据结构组合在一起，以构建更强大的视觉世界模型。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] S. Bell, C. L. Zitnick, K. Bala, and R. Girshick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. arXiv preprint arXiv:1512.04143, 2015. 6</p><p>[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE, 2009. 1</p><p>[3] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303– 338, 2010. 1</p><p>[4] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester. Discriminatively trained deformable part models, release 4. <a href="https://link.jianshu.com/?t=http%3A%2F%2Fpeople.cs.uchicago.edu%2Fpff%2Flatent-release4%2F" target="_blank" rel="noopener">http://people.cs.uchicago.edu/pff/latent-release4/</a>. 8</p><p>[5] R. B. Girshick. Fast R-CNN. CoRR, abs/1504.08083, 2015. 4, 5, 6</p><p>[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385, 2015. 2, 4, 5</p><p>[7] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 2, 5</p><p>[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. 2</p><p>[9] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. 5</p><p>[10] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. In European Conference on Computer Vision, pages 740–755. Springer, 2014. 1, 6</p><p>[11] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, and S. E. Reed. SSD: single shot multibox detector. CoRR, abs/1512.02325, 2015. 4, 5, 6</p><p>[12] G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller. Introduction to wordnet: An on-line lexical database. International journal of lexicography, 3(4):235–244, 1990. 6</p><p>[13] J. Redmon. Darknet: Open source neural networks in c. <a href="https://link.jianshu.com/?t=http%3A%2F%2Fpjreddie.com%2Fdarknet%2F" target="_blank" rel="noopener">http://pjreddie.com/darknet/</a>, 2013–2016. 5</p><p>[14] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. arXiv preprint arXiv:1506.02640, 2015. 4, 5</p><p>[15] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal net- works. arXiv preprint arXiv:1506.01497, 2015. 2, 3, 4, 5, 6</p><p>[16] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 2015. 2</p><p>[17] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 2, 5</p><p>[18] C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. CoRR, abs/1602.07261, 2016. 2</p><p>[19] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. 5</p><p>[20] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):64–73, 2016. 1</p><h2 id="原文："><a href="#原文：" class="headerlink" title="原文："></a>原文：</h2><div class="row">    <embed src="./yolo2.pdf" width="100%" height="550" type="application/pdf"></div>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>阅读笔记（7.10）</title>
    <link href="/2020/07/10/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%887-10%EF%BC%89/"/>
    <url>/2020/07/10/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%887-10%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>最近主要阅读了一篇实验室发表的文章、四篇目标检测方法综述和一篇YOLO算法的论文。对目标检测有了基本概念，了解了目前主流的目标检测算法。具体如下：</p><p>[1]. L. Mao, Y. Yan, J. Xue and H. Wang, “Deep Multi-task Multi-label CNN for Effective Facial Attribute Classification,” in IEEE Transactions on Affective Computing, doi: 10.1109/TAFFC.2020.2969189.</p><p>[2]吴帅,徐勇,赵东宁.基于深度卷积网络的目标检测综述[J].模式识别与人工智能,2018,31(04):335-346.</p><p>[3]栗佩康,袁芳芳,李航涛.目标检测方法简要综述[J].科技风,2020(18):157.</p><p>[4]Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi; You Only Look Once: Unified, Real-Time Object Detection .The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 779-788</p><p>[5]胡伏原,李林燕,尚欣茹,沈军宇,戴永良.基于卷积神经网络的目标检测算法综述[J].苏州科技大学学报(自然科学版),2020,37(02):1-10+25.</p><p>[6]赵永强,饶元,董世鹏,张君毅.深度学习目标检测方法综述[J].中国图象图形学报,2020,25(04):629-654.</p><h2 id="1-精读了《Deep-Multi-task-Multi-label-CNN-for-Effective-Facial-Attribute-Classiﬁcation》"><a href="#1-精读了《Deep-Multi-task-Multi-label-CNN-for-Effective-Facial-Attribute-Classiﬁcation》" class="headerlink" title="1.   精读了《Deep Multi-task Multi-label CNN for Effective Facial Attribute Classiﬁcation》"></a>1.   精读了《Deep Multi-task Multi-label CNN for Effective Facial Attribute Classiﬁcation》</h2><p>研究动机：</p><p>(1)  最新的FAC方法独立的进行面部检测和属性分类，没有完全利用这两部分之间的依赖性。</p><p>(2)  许多方法使用相同的CNN网络结构预测所有的面部属性，忽略了不同面部属性学习的复杂性。</p><p>针对以上两点问题，作者提出了一种多任务多标签CNN，即DMM-CNN（deep multi-task multi-label CNN），将人脸关键点检测和属性分类联系起来，并将面部属性分为两组：客观属性和主观属性，为这两组属性分别涉及不同的网络结构。此外还提出了一种新颖的动态加权方案来为训练过程中的每一个属性自动分配损失权重，提出了一种自适应阈值策略缓解多标签分类不平衡的问题。在公开的CelebA和LFWA数据集上进行的实验表明，与几种最新的FAC方法相比，DMM-CNN具有更高的性能。</p><p>在阅读过程中发现作者有些许笔误：在文章的3.2.1节Objective Attributes and Subjective Attributes第二句所举的客观属性和主观属性的例子似乎弄反了，原文为：</p><p>In this paper, we propose to classify facial attributes into two groups: objective attributes (such as “Attractive”, “Big Nose”) and subjective attributes (such as “Bald”, “Male”). </p><p>客观是指不依赖人的意识就客观存在的事物，而主观指受人的意识影响较大。显然“秃头”和“男性”应该是客观的，“有吸引力”、“大鼻子”应该是主观的。文章后续对于主观属性和客观属性所举的例子没有问题,符合上述定义。此处可能为笔误。</p><h2 id="2-精读了《基于深度卷积网络的目标检测综述》"><a href="#2-精读了《基于深度卷积网络的目标检测综述》" class="headerlink" title="2.精读了《基于深度卷积网络的目标检测综述》"></a>2.精读了《基于深度卷积网络的目标检测综述》</h2><p>该文章系统总结了基于深度网络的目标检测方法，将其归为两类：基于候选窗口的目标检测框架、基于回归的目标检测框架。基于候选窗口的目标检测算法准确率较高，基于回归的目标检测方法检测效率较高。</p><p>(1)  基于候选窗口的目标检测框架有：基于区域的卷积神经网络（RCNN）、尺度金字塔池化网络（SPPnet）、快速基于区域的神经网络（Fast-RCNN）、更快的基于区域的卷积神经网络（Faster-RCNN）、基于区域的全卷积网络（RFCN）。</p><p>[1]  RCNN由3个独立的步骤组成：产生候选窗口、特征提取、SVM分类及窗口回归。</p><p>[2]  SPPnet和Fast-RCNN不需要将所有候选窗口送入网络，只需送入一次，在将所有候选窗口在网络中某层上进行映射，提高检测速度。</p><p>[3]  Faster-RCNN利用候选网络（RPN）产生候选窗口，利用与Fast-RCNN相同的结构进行分类和窗口回归，RPN和Fast-RCNN共享主要的深度网络。Faster-RCNN中所有窗口经过ROI池化后都会送入后续子网络进行分类和窗口回归。</p><p>[4]  RFCN使用位置敏感的ROI池化层，使池化结果不需再经过子网络，直接进行分类和窗口回归。</p><p>(2)  基于回归的目标检测框架有：基于Mask的目标检测模型、窗口位置回归、神经元预测窗口框架。</p><p>[1]  基于Mask的目标检测模型，首先通过深度网络产生一张dxd大小的Mask，其值为0或1，表示原图所对应的区域是否为目标区域的一部分。进而采用多尺度检测，将得到的窗口不断送入网络进行精简，最终确定目标位置。</p><p>[2]  窗口位置回归有：AttentionNet和G-CNN。AttentionNet对目标窗口左上角和右下角的点不断进行回归，判断这两个点的移动状态，当这两个点都不动时，即找到目标物体，AttentionNet只能检测单个目标。G-CNN首先定义少量Grid，通过不断迭代将初始Grid回归成包含目标的窗口。</p><p>[3]  神经元预测窗口框架有：YOLO、SSD、DSSD。</p><p>此外，文章还介绍了：</p><ul><li><p>提升目标检测准确率的辅助方法：超网络（HyperNet）、特征金字塔网络（FPN）、内外网络（ION）、对抗网络。</p><p>​        HyperNet通过融合不同网络层的特征，提高检测准确率。FPN设计一个Top-to-Down的额外路径，通过Upsample的方式扩张网络顶端的特征层。ION不仅关注目标窗口内的信息，还关注目标窗口外的上下文信息。对抗网络的主要思想是：提高目标检测的性能，不仅可以分析对目标检测有用的信息，也可分析对目标检测不利的信息。A-Fast-RCNN利用网络本身产生遮挡、形变的样本，以提高处理这两类问题的能力。</p></li><li><p>处理形变问题的方法有：STN网络、DPM、DeepID、形变卷积网络（DCN）</p></li><li><p>目前使用的主流数据库有：PASCAL_VOC、IMAGENET、COCO。</p></li></ul><h2 id="3-精读了《目标检测方法简要综述》"><a href="#3-精读了《目标检测方法简要综述》" class="headerlink" title="3.精读了《目标检测方法简要综述》"></a>3.精读了《目标检测方法简要综述》</h2><p>该文章同样将基于深度学习的目标检测算法分为两类：两阶段和单阶段。两阶段算法（即基于候选区域）首先在图像中选取候选区域，然后对候选区域进行目标分类和位置精修；单阶段算法（即基于全局回归分类）直接产生目标物体的位置及类别。</p><ul><li><p>R-CNN采用区域性搜索算法提取2000个候选区域，然后使用CNN对各候选区域进行特征提取，接着使用SVM进行分类，利用NMS回归目标位置。缺点：输入CNN的图像为固定尺寸（由于全连接层的限制），无特征提取共享，重复计算。</p></li><li><p>SPP-Net复用第五卷积层的特征响应图，将任意尺寸的候选区域转为固定长度的特征向量，最后一个卷积层后接入SPP层。优点：只对原图做一次卷积，共享卷积层的计算过程，提高速度。不同尺度候选区域输入能提高检测精度，共享不同候选区域SPP层前序的计算，提升检测效率。</p></li><li><p>Fast -RCNN使用ROI池化层代替SPP，原图经过卷积后产生特征相应图，然后从每个ROI池化层提取定长特征向量，每个特征向量输入到全连接层，分支两个输出层，一个是Softmax分类器用来预测类别，另一个用作物体位置定位框回归预测。优点：端到端、基于多任务损失函数的一阶段训练过程，节省了存储空间，减少训练，提升检测精度。缺点：候选区域提取仍采用区域选择性搜索算法，耗费时间。</p></li><li><p>Faster-RCNN摒弃了区域选择性搜索算法，使用区域选择网络（RPN）产生候选区域框，与检测网络共享卷积特征响应，减少计算量，提高检测速度。优点：端到端，具有一定实时性。缺点：RPN网络训练较为耗时，RPN不善于处理极端尺度及形状的目标检测。</p></li></ul><p>以上为两阶段目标检测算法，检测精度较高，但检测效率低，无法满足实时性要求。因此，单阶段目标检测算法旨在提升检测实时性。</p><ul><li><p>YOLO算法：图像经过一个神经网络即可完成目标位置及其所属类别的检测，YOLOv1检测速度达到45fps，但检测精度不高。</p></li><li><p>YOLOv2在卷积层后加入了批归一化、去掉全连接层进行多尺度训练、加入先验框。使得可检测种类更多、精度更高，时间更快。</p></li><li><p>YOLOv3采用新的网络结构Darknet-53，利用多尺度进行目标检测，使用逻辑回归代替softmax分类器，提高了检测精度，对小目标检测效果更好。</p></li><li><p>YOLOv4加入SPP block改善感受野大小，使用PANet代替FPN进行多通道特征融合，选用CSP-Darknet53作为主干网络，进一步提高检测精度和速度。</p></li></ul><p>以上两篇文章均提到了YOLO，遂根据参考文献找来阅读。</p><h2 id="4-阅读并翻译了《You-Only-Look-Once》REDMON-J，DIVVALA-S，GIRSHICK-R，et-al"><a href="#4-阅读并翻译了《You-Only-Look-Once》REDMON-J，DIVVALA-S，GIRSHICK-R，et-al" class="headerlink" title="4.   阅读并翻译了《You Only Look Once》REDMON J，DIVVALA S，GIRSHICK R，et al."></a>4.   阅读并翻译了《You Only Look Once》REDMON J，DIVVALA S，GIRSHICK R，et al.</h2><p>文章提出了一种新的目标检测方法，将目标检测问题框架化为一个空间分割边界框和相关类概率回归问题。直接从完整图像中用单个神经网络预测边界框和类概率，只需看一次即可预测存在哪些对象以及他们在哪里。YOLO的神经网络有24个卷积层和2个全连接层。Fast-YOLO仅使用9个卷积层，网络的最终输出为7x7x30张量的预测。与基于分类器的方法不同，YOLO直接在对应检测性能的损失函数上训练，并且整个模型联合训练。YOLO有如下优点：（1）速度非常快，Fast-YOLO的帧速率可以超过150fps，具有良好的实时检测性能。（2）YOLO预测时在整幅图像上进行推理，考虑有关类的上下文信息，预测产生的背景错误更少。（3）YOLO可以学习物体的泛化表示，具有高度可通用性，并且可在艺术图像上进行人物检测。缺点：YOLO的检测精度相比其他算法有些逊色，尤其是对成群的小物体的识别效果不好。</p><p>文章中大量出现了Accuracy、Precision、Recall、AP 、mAP、IOU、top-5等计算机视觉评价指标，最初并不理解这些指标的含义，遂查阅资料学习了这些评价指标。</p><ul><li>准确度（Accuracy）：</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggo8gc60k0j30l0040weq.jpg" srcset="/img/loading.gif" alt="img" style="zoom: 33%;" /></p><p>其中TP为真阳性、FN为假阴性、FP为假阳性、TN为真阴性。</p><p>在正负样本分布均匀的情况中，Accuracy能够较好地反映检测指标，但在分布不均匀的样本中Accuracy指标存在缺陷，比如有一个由90张狗的正样本图片和10张非狗的负样本图片组成的数据集，若分类器将所有图片全判断为狗，也能取得90%的Accuracy指标，这显然是不合理的。为此，引出了精确度（Precision）和召回率（Recall）的评价指标</p><ul><li>精确度（Precision）表示所有预测为正的样本中，真实正样本所占的比例。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggo8gcmu3aj30eq03sdfx.jpg" srcset="/img/loading.gif" alt="img" style="zoom:33%;" /></p><ul><li>召回率（Recall）表示所有正样本中，预测为正的样本比例。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggo8gd2c1kj30cc03m3yl.jpg" srcset="/img/loading.gif" alt="img" style="zoom:33%;" /></p><p>用Top-1和Top-5 error rate指标来评价分类性能：</p><ul><li><p>Top-1 error rate：使用预测结果和正确结果进行对比，如果相同则表示预测正确。将输出的C维向量按照降序排列，如果最大值所对应label与ground truth label不符，则该图片属于分类错误，最后用分类错误的样本数量除以样本集的数量得到Top-1 error rate；</p></li><li><p>Top-5 error rate：使用预测结果的Top-5（分类结果标签的前五个）与正确结果进行对比，如果五个之中有一个正确那么就认为分类器预测结果正确。将输出的C维向量按照降序排列，如果前5个对应的label没有包含ground truth label，则该图片属于分类错误，最后用分类错误的样本数量除以样本集的数量得到Top-5 error rate；显然，Top-1比Top-5更严格。</p></li></ul><p>在目标检测中，一般用AP（average precision）和mAP（mean average precision）两个指标评估每个bounding box对应的类别概率，注意AP强调的是每一个类别的检测指标，比如狗的检测AP、汽车的检测AP、人的检测AP等。</p><ul><li><p>AP(average precision)平均精确度。比precision多了一个average，显然计算AP时有一个求平均的过程。precision和recall存在一定关系，当调整算法的阈值提高recall时，precision会降低，反之，precision会提高。通俗的说就是提高recall可以让检测出来的样本更多的预测为正样本（减少FN），但这样会让一些负样本也预测为正样本（FP增高），导致precision降低。对于一个分类器而言，precision和recall往往是此消彼长的，AP这个指标则很好的兼顾了两者，AP即为precision/recall曲线下方围成的曲面面积。</p></li><li><p>Top-N ranked：对排序后的样本统计数据按照前n个样本划分N次rank，分别求出每次rank后的precision和recall，直至n=N，N为预测的总数量。</p></li></ul><p>注意，VOC2010之前AP的计算产生了变化，选取Recall&gt;=0,0.1,0.2,…,1这11个Recall值处的Precision的最大值。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggo8gbr41hj30ej044wel.jpg" srcset="/img/loading.gif" alt="img" style="zoom:33%;" /></p><p>  VOC2010之后对recall的判断值作了修改，选取Recall&gt;=0,0.14,0.29,0.43,0.57,0.71,1这7个位置的Precision最大值。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggo8gb9mj1j30c102uwef.jpg" srcset="/img/loading.gif" alt="img" style="zoom:33%;" /></p><ul><li><p>mAP(mean average precision)。AP指的是一个类别的指标，而一个目标检测任务包含多个类别，如VOC包含20类object，则会计算出20个AP，将其累加取平均便得到mAP。</p></li><li><p>IOU（Intersection Over Union）交并比：预测的bounding box和ground truth bounding box之间的交集和并集的面积比值，当两者完全重合时，IOU为100%，没有任何交集时为0，显然IOU值越大，模型预测的bounding box性能越好。</p></li></ul><h2 id="5-速读了《基于卷积神经网络的目标检测算法综述》"><a href="#5-速读了《基于卷积神经网络的目标检测算法综述》" class="headerlink" title="5.   速读了《基于卷积神经网络的目标检测算法综述》"></a>5.   速读了《基于卷积神经网络的目标检测算法综述》</h2><p>该文章将目标检测方法分为基于人工特征的目标检测算法和基于卷积神经网络的目标检测算法。</p><ul><li><p>基于人工特征的目标检测算法需要人工设计特征描述算子，提取图像特征，然后利用分类器进行图像特征分类。需要对不同类型的目标设计不同形式的对象表征模型，产生大量计算开销。并且特征提取算子大多基于像素级的浅层特征信息，缺乏语义表达能力。</p></li><li><p>针对特征提取与目标检测问题的解决思路，将当前目标检测算法分为多阶段和一阶段两类。多阶段目标检测算法将特征提取与目标分类分开处理。一阶段目标检测算法在一个处理过程中将图像的特征提取与目标检测同时完成。</p></li><li><p>多阶段目标检测算法的核心是利用CNN代替传统的人工提取特征方法。一阶段算法的核心思想是将目标检测问题转换为回归问题求解，直接利用CNN提取图像特征，输出待预测的特征向量，直接得到目标检测结果。</p></li><li>论文介绍了CNN算法的原理及特点：(1)多层卷积与池化。(2)局部感知。(3)权值共享。(4)稀疏性限制。</li></ul><h2 id="6-速读了《深度学习目标检测方法综述》"><a href="#6-速读了《深度学习目标检测方法综述》" class="headerlink" title="6.   速读了《深度学习目标检测方法综述》"></a>6.   速读了《深度学习目标检测方法综述》</h2><ul><li>该文章指出了基于深度学习的目标检测算法的挑战：优化主流目标检测算法的性能、提高小目标物体检测精度、实现多类别物体检测、轻量化检测模型。</li><li>针对以上挑战，文章从双阶段、单阶段目标检测算法的改进与结合的角度分析了改进与优化主流目标检测算法的方法。</li><li>从骨干网络、增加视觉感受野、特征融合、级联卷积神经网络和模型的训练方式的角度分析了提升小目标检测精度的方法。</li><li>从训练方式和网络结构的角度分析了用于多类别物体检测的方法。</li><li>从网络结构的角度分析了用于轻量化检测模型的方法。</li></ul>]]></content>
    
    
    <categories>
      
      <category>阅读笔记</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FPN</tag>
      
      <tag>SSD</tag>
      
      <tag>YOLO</tag>
      
      <tag>Fast-RCNN</tag>
      
      <tag>Faster-RCNN</tag>
      
      <tag>SPPnet</tag>
      
      <tag>FAC</tag>
      
      <tag>RCNN</tag>
      
      <tag>RFCN</tag>
      
      <tag>Mask</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2020小浪底调水调沙</title>
    <link href="/2020/07/10/%E5%B0%8F%E6%B5%AA%E5%BA%95/"/>
    <url>/2020/07/10/%E5%B0%8F%E6%B5%AA%E5%BA%95/</url>
    
    <content type="html"><![CDATA[<p>​      2020年7月10日，阴，受好友热情邀请，前往孟津黄河小浪底水利枢纽观看调水调沙。身为洛阳人，一直都有来小浪底逛逛的想法，今天终于得以实现。抵达时已将近下午四点，天气也不算太热。从坝顶下到坝底的泄洪口就花了二三十分钟。下图红色的部分是石块堆砌的坝体，内部还有混凝土防渗墙。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggod39t5jqj30rs0ktjvh.jpg" srcset="/img/loading.gif" alt="image-20200712174628769"></p><p>小浪底水利枢纽是一座”壤土斜心墙堆石坝“，坝高160M，坝顶高程281M，内有国内最深最厚的混凝土防渗墙。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggod39brg6j30rs0ku0zp.jpg" srcset="/img/loading.gif" alt="image-20200712174644193"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggod3a8xbhj30rs0ijgo3.jpg" srcset="/img/loading.gif" alt="image-20200712174717464"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggod3appswj30rs0kudjr.jpg" srcset="/img/loading.gif" alt="image-20200712174802002"></p><center>壮观的调水调沙口</center><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggohhutj22j31900u04r1.jpg" srcset="/img/loading.gif" alt="image-20200712205648765"></p><center>与好友合影留念</center><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggoca12gdgj30rs0ku7lp.jpg" srcset="/img/loading.gif" alt="image-20200712175619439"></p><center>与好友合影留念</center><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggocc43zb5j30rs0kuk5q.jpg" srcset="/img/loading.gif" alt="image-20200712175819389"></p><center>黄河之水口中来，哈哈</center><p>在游玩即将结束时，征得景区工作人员同意，使用无人机拍摄了壮观的泄洪场面，请您观赏！</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggoce6xj82j30rs0ku1ie.jpg" srcset="/img/loading.gif" alt="image-20200712180019419"></p><center>航拍泄洪口</center><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggocf4n2blj30rs0kue3q.jpg" srcset="/img/loading.gif" alt="image-20200712180113328"></p><center>航拍泄洪口</center><div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=286365451&bvid=BV1Yf4y1R7PS&cid=211635412&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" sandbox="allow-top-navigation allow-same-origin allow-forms allow-scripts"> </iframe></div><p>回到孟津县城，天色已晚，好友推荐了一家肥肠面，光看照片就令人垂涎三尺。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggoco515cbj30rs0ku4ox.jpg" srcset="/img/loading.gif" alt="image-20200712180953139"></p><p>今天虽然没能见到最大的泄洪口开启，但近距离感受了黄河水的汹涌澎湃。从坝顶下台阶也算锻炼了身体，晚上与好友品尝肥肠面，聊聊高中往事，非常开心~</p>]]></content>
    
    
    <categories>
      
      <category>日记</category>
      
      <category>旅行日记</category>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——You Only Look Once:Unified,Real-Time Object Detection</title>
    <link href="/2020/07/02/YOLO/"/>
    <url>/2020/07/02/YOLO/</url>
    
    <content type="html"><![CDATA[<h1 id="You-Only-Look-Once-Unified-Real-Time-Object-Detection"><a href="#You-Only-Look-Once-Unified-Real-Time-Object-Detection" class="headerlink" title="You Only Look Once:Unified,Real-Time Object Detection"></a>You Only Look Once:Unified,Real-Time Object Detection</h1><h2 id="原文"><a href="#原文" class="headerlink" title="原文"></a>原文</h2><div class="row">    <embed src="./1.pdf" width="100%" height="550" type="application/pdf"></div><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​      我们提出了YOLO，一种新的目标检测方法。先前有关对象检测的工作使用分类器执行检测。取而代之的是，我们将目标检测框架化为一个空间分割边界框和相关类概率回归问题。单个神经网络在一次评估中直接从完整图像中预测边界框和类概率。由于整个检测流程是在一个网络中完成的，因此该网络可以端到端的进行性能优化。</p><p>   我们的网络是一个统一的框架，因此其检测速度非常快。我们的YOLO模型每秒可以实时处理45帧图像。较小的网络Fast YOLO，其处理能力达到惊人的155帧/秒，实现了两倍于其他实时检测网络的mAP。与最先进的监测系统相比，YOLO定位误差更大，但预测背景假阳性的可能性较小。（将背景检测为目标的可能性更小）。最终，YOLO可以学习非常通用的目标表示。当从自然图像到艺术品等其他领域进行泛化时，YOLO的性能优于其他方法，包括DPM和RCNN。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>​       人类扫一眼图像就可以立即知道图像中有哪些对象，以及它们在哪和它们的相互关系。人类视觉系统的快速和准确使得我们可以执行复杂的任务，例如很少有意识的思考如何开车。快速，准确的目标检测算法允许计算机无需专用的传感器即可驾驶汽车，使辅助设备能向人类传递实时场景信息，并释放通用的响应式机器人系统的潜力。</p><p>​       当前的目标检测系统重新使用分类器来进行检测。为了检测一个目标，这些系统使用分类器在测试图像的各个位置和比例上对其进行了评估。DPM系统使用了滑窗方法，其中分类器在整幅图像上均匀间隔的位置运行。</p><p>​      最近的R-CNN网络使用区域提取方法，首先在图片上生成潜在边界框，然后在这些框上使用分类器。分类完成以后，使用后期处理来精简边界框，消除重复的检测，以及根据场景中的其他对象对边界框进行评分。这些复杂的过程使得检测速度慢并难以进行优化，因为每个单独的模块都必须分别进行训练。</p><p>​     我们将目标检测重构为单个回归问题，直接从图像像素得到边界框的坐标和类别概率。使用我们的系统，您只需看一次即可预测存在哪些对象以及他们在哪里。</p><p>​      YOLO非常简单，如图1所示，一个简单的卷积网络同时预测多个边界框及其类别概率。YOLO在整幅图像上进行训练并直接优化检测性能，与传统目标检测方法相比，这个统一的模型有更多的优点。</p><p>​      第一，YOLO的速度非常快。由于我们将检测看为一个回归问题，因此我们不需要复杂的过程。我们只需在测试时在新图像上运行神经网络即可预测检测结果。我们的基础网络在没有批处理时可以在Titan X GPU上达到每秒45帧，快速版的网络可以超过150fps。这意味着我们可以实时处理流式视频，延迟时间少于25毫秒。此外，YOLO达到其他实时系统平均平均精度的两倍以上。</p><p>​     第二，YOLO预测时可以在整幅图像上进行推理。与滑窗和基于区域提取的方法不同，YOLO可以在训练和测试时看到整幅图像，因此它隐式的编码有关类的上下文信息及其外观。Fast R-CNN是一种顶部检测方法，它会将图片中的背景误认为是目标，因为它看不到更大的上下文信息，与Fast-R-CNN相比，YOLO产生的背景错误少于一半。</p><p>​      第三，YOLO可以学习物体的泛化表示。当在自然图像上训练，在艺术图像上测试时，YOLO大幅优于DPM和Fast R-CNN等顶级检测方法。由于YOLO具有高度可通用性，因此在应用于新域或不期望的输入时不太可能出错。</p><p>​      YOLO仍在检测精度上落后于其他方法。虽然他可以快速的识别图像中的物体，但它很难精确的定位某些物体，尤其是小物体。我们在实验中进一步研究了这些权衡。</p><p>​     我们所有的训练和测试代码都是开源的，各种预训练模型也可以下载。</p><h2 id="2-统一检测"><a href="#2-统一检测" class="headerlink" title="2. 统一检测"></a>2. 统一检测</h2><p>​        我们将目标检测的单独组件统一到单个神经网络中，我们的网络使用整幅图像的特征来预测每一个边界框。它还可以同时预测所有类别的多有边界框。这意味着我们的网络会从整体上对整个图像和图像中的所有对象进行解释。YOLO设计可实现端到端的培训和实时速度，同时保持较高的平均精度。</p><p>​      我们的系统将输入图像分为SxS的网格，如果目标的中心落入一个网格单元中，那么这个网格单元将负责检测该目标。</p><p>​       每个网格预测B个边界框和每个框的置信度。<strong>置信度反映了网络在多大程度上相信边界框中包含一个物体，以及它认为该框预测的准确性。</strong></p><p>​        最终，我们将置信度定义为Pr(Object) * IOU.如果网格中没有目标存在，那么置信度为0。否则置信度等于预测框和真实值之间的交集（IOU）。</p><p>​        每个边界框包含5个预测值：x,y,w,h,confidence。（x,y)坐标表示相对于grid cell 左上角的边界框中心位置，w,h是被预测的边界框相对于整个图片的宽度和高度。最后，置信度预测表示预测框与真实框之间的IOU。</p><p>​        每个网格单元还预测C个<strong>条件类概率Pr(Class[i] | Object)</strong>，这个概率取决于包含对象的网格单元。我们只预测每个网格单元的一组类概率，而不管方框B的数量。</p><p>​        在测试时我们将每个网格的条件概率和每个边界框的置信度预测相乘，</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggckeyu3h0j30u202g0t3.jpg" srcset="/img/loading.gif" alt="image-20200702092439875" style="zoom:50%;" /></p><p>​     这给出了每个边界框的特定类别的置信度。这些置信度分数编码了该类出现在框中的概率以及预测框拟合目标的程度。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggckey8vh4j30v60toagx.jpg" srcset="/img/loading.gif" alt="image-20200702094849310" style="zoom: 33%;" /></p><p>YOLO在检测PASCAL VOC数据集时，使 <strong>S=7</strong>，B=2. PASCAL VOC数据集有20种标记的类别，因此 <strong>C=20</strong>. 我们最终预测的是一个 <strong>7*7*30</strong> 的张量。（<strong>S * S * (B * 5 + C)</strong>）</p><h3 id="2-1-网络设计"><a href="#2-1-网络设计" class="headerlink" title="2.1 网络设计"></a>2.1 网络设计</h3><p>​    我们将此模型实现为卷积神经网络，并在PASCAL VOC检测数据集上进行评估。网络的初始卷积层从图像中提取特征，而全连接层预测输出概率和坐标。</p><p>​      我们的网络结构受到GoogLeNet图像分类模型的启发。我们的网络有<strong>24个卷积层和2个全连接层</strong>。除了被GoogLeNet使用的初始模块，我们只使用1 <em> 1缩减层，然后使用3 </em> 3卷积层，类似于Lin等人的工作[22]。 完整的网络如图3所示。</p><p>​      我们还训练了一种快速版的YOLO，旨在突破快速物体检测的界限。Fast YOLO使用的神经网络具有较少的卷积层（从9个而不是24个），并且这些层中的过滤器更少。除网络规模外，所有训练和测试参数在YOLO和Fast YOLO之间都是相同的。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggckezs6n2j31us0t2dnj.jpg" srcset="/img/loading.gif" alt="image-20200702100959924"></p><p>我们网络的最终输出是7x7x30张量的预测。</p><h3 id="2-2-训练"><a href="#2-2-训练" class="headerlink" title="2.2 训练"></a>2.2 训练</h3><p>​     我们在1000类的ImageNet竞赛数据集上预先训练我们的卷积层。 对于预训练，我们使用图3中的前20个卷积层，然后是平均池化和全连接层。 我们训练这个网络大约一周，并在ImageNet 2012验证集上实现88％的single crop前5精度，与Caffe’s Model Zoo中的GoogLeNet模型相当。</p><p>​     然后我们转换模型以执行检测。Ren等人表明将卷积和连接层添加到预训练网络可以提高性能。 按照他们的例子，我们添加了四个卷积层和两个全连接层，随机初始化权重。检测通常需要细粒度的视觉信息，因此我们将网络的输入分辨率从224x224提高到448x448。</p><p>​     我们的<strong>最后一层预测了类概率和边界框坐标</strong>。我们将边界框宽度和高度标准化为图像宽度和高度，使它们落在0和1之间。我们将边界框的x和y坐标参数化为特定网格单元位置的偏移量，因此它们也被限制在0和1之间。</p><p>​       我们对<strong>最终层使用线性激活函数</strong>，所有其他层使用以下leaky线性激活：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggckf0qftsj30ks04oaaa.jpg" srcset="/img/loading.gif" alt="image-20200702102658823" style="zoom:33%;" /></p><p>​        我们优化模型输出中的求和平方误差。我们使用求和平方误差，因为它很容易优化，但它并不完全符合我们最大化平均精度的目标。它的定位误差与分类误差相同，这可能并不理想。此外，在每个图像中，许多网格单元不包含任何对象。这会将这些单元格的“置信度”得分推向零，这通常会超过确实包含对象的单元格的梯度。这可能导致模型不稳定，导致训练在早期就出现分歧。为了解决这个问题，对于不包含对象的盒子，我们<strong>增加了边界框坐标预测的损失，并减少了置信度预测的损失。</strong>我们使用两个参数λcoord和λnoobj来完成此操作，设定λcoord= 5和λnoobj= 0.5。</p><p>​       求和误差也同样可以加大大盒子和小盒子中的误差。我们的误差度量应该反映出大箱子中的小偏差比小箱子中的小。为了部分解决这个问题，我们<strong>直接预测边界框宽度和高度的平方根</strong>，而不是宽度和高度。</p><p>​      YOLO预测每个网格单元有多个边界框。在训练时，我们只希望一个边界框预测器对每个对象负责。<strong>我们指定一个预测器只对与真实框具有最大IOU的预测物体负责。</strong>这导致边界框和预测器之间的特殊化。每个预测器都能更好地预测某些大小，宽高比或对象类别，从而提高整体召回率。</p><p>在训练期间，我们优化了以下多部分损失函数：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggckeza6o2j30qg0gkta7.jpg" srcset="/img/loading.gif" alt="image-20200702110322116" style="zoom:50%;" /></p><p>​    1i表示对象在第i个网格中出现，1ij表示第j个边界框预测器在第i个网格中对该预测负责。</p><p>​      请注意，如果对象存在于该网格单元中，则损失函数仅惩罚分类错误。（仅此前面讨论过条件类概率）。如果该预测器对真实框”负责“（即该网格单元中具有任何预测器的最高IOU），它也仅对边界坐标误差进行惩罚。</p><p>​         我们在PASCAL VOC 2007和2012的训练和验证数据集上训练了大约135个epoch的网络。在PASCAL VOC 2012测试时，我们也使用VOC 2007测试数据用作训练。在整个训练过程中，我们使用的<strong>批大小为64，动量为0.9，衰减为0.0005</strong>。</p><p>​        我们的学习率时间表如下：首先，我们将学习率从10−3缓慢提高到10−2。如果我们以较高的学习率开始，我们的模型通常会由于不稳定的梯度而发散。我们继续用10-2的训练率训练75个epoch，然后以10−3训练率训练30个epoch，最后以10−4训练率训练30个epoch。</p><p>​        为避免过度拟合，我们使用了dropout和大量数据扩充。在第一个连接层之后，速率= 0.5的丢失层阻止了层之间的共同适应。对于数据增强，我们引入了高达原始图像大小20％的随机缩放和翻转。我们还在HSV颜色空间中随机调整图像的曝光和饱和度达1.5倍。</p><h3 id="2-3-推论"><a href="#2-3-推论" class="headerlink" title="2.3 推论"></a>2.3 推论</h3><p>​     就像在训练中一样，预测测试图像的检测只需要一次网络评估。在PASCAL VOC上，网络预测每个图像98个边界框和每个框的类概率。YOLO在测试时非常快，因为它只需要一个网络评估，不像基于分类器的方法。</p><p>​     网格设计在边界框预测中强制实施空间多样性。通常很清楚一个对象落入哪个网格单元，并且<strong>网络仅为每个对象预测一个框</strong>。然而，<strong>一些大物体或多个单元边界附近的物体可以被多个单元很好地定位</strong>。 <strong>非最大抑制可用于修复这些多个检测</strong>。虽然对于R-CNN或DPM的性能并不重要，但非最大抑制在mAP中增加2-3％。</p><h3 id="2-4-YOLO的局限性"><a href="#2-4-YOLO的局限性" class="headerlink" title="2.4 YOLO的局限性"></a>2.4 YOLO的局限性</h3><p>​      YOLO在边界框预测上实加了强大的空间约束，因为<strong>每个网格单元只预测两个框，并且只能有一个类。</strong>这个空间约束限制了我们的模型能够预测的附近物体的数量。我们的网络很难检测成群的小物体，比如成群的鸟。</p><p>​        由于我们的模型从数据中学习预测边界框，因此很难在新的或不寻常的宽高比或配置中的对象中进行泛化。我们的模型还使用<strong>相对粗糙的特征来预测边界框</strong>，因为我们的<strong>网络结构具有来自输入图像的多个下采样层</strong>。</p><p>​       最后，当我们训练一个近似于检测性能的损失函数时，我们的损失函数在小边界框中处理误差与大边界框相同。大盒子中的小误差通常是良性的，但小盒子中的小误差对IOU的影响要大得多。我们的主要错误来源是错误的定位 。</p><h2 id="3-与其他检测系统相比"><a href="#3-与其他检测系统相比" class="headerlink" title="3. 与其他检测系统相比"></a>3. 与其他检测系统相比</h2><p>​         目标检测室计算机视觉的一个核心问题。检测管线通常从输入图像中提取一组鲁棒特征开始（Haar [25]，SIFT [23]，HOG [4]，卷积特征[6]）。然后，分类器或定位器在特征空间中识别物体。这些分类器或定位器在整个图像中以滑动窗口方式运行，或者在图像中的某些区域子集上运行。我们将YOLO检测系统与几个顶级检测框架进行了比较，突出了关键的相似性和差异。</p><p>​        <strong>Deformable parts models.</strong>变形零件模型（DPM）使用滑动窗口方法进行物体检测。DPM使用不相交的过程提取静态特征，对区域进行分类，预测高得分区域的边界框等。我们的系统使用单个卷积神经网络替换了所有这些不同的部分。网络同时执行特征提取，边界框预测，非最大抑制和上下文推理。网络在线训练和优化检测任务的特征，而不是静态特征。我们统一的架构实现了比DPM更快，更准确的模型。</p><p>​       <strong>R-CNN：</strong>R-CNN及其变体使用<strong>区域提议</strong>而不是滑动窗口来查找图像中的物体。选择性搜索生成潜在的边界框，卷积网络提取特征，SVM对框进行评分，线性模型调整边界框，非最大抑制消除重复检测。这个复杂过程的每个阶段必须独立精确调整，使得系统非常慢，在测试时每个图像需要超过40秒。</p><p>​        YOLO与R-CNN有一些相似之处。每个网格单元提出潜在的边界框，并使用卷积特征对这些框进行评分。但是，我们的系统对<strong>网格单元提议设置了空间限制</strong>，这有助于<strong>减轻同一对象的多次检测</strong>。我们的系统还提出了更少的边界框，每个图像只有98个，而选择性搜索只有2000个。最后，我们的系统将这些单独的组件组合成一个联合优化的模型。</p><p>​        <strong>其他快速检测器</strong>   Fast and Faster R-CNN专注于通过共享计算并使用神经网络来提议区域而不是选择性搜索来加快R-CNN框架[14] [27]。尽管它们在R-CNN上提供了速度和准确性方面的改进，但两者仍然都缺乏实时性能。</p><p>​        许多研究工作都集中在加速DPM过程 [30] [37] [5]上。它们可以加快HOG计算速度，使用级联并将计算推入GPU。但是，只有30Hz DPM [30]实际上是实时运行的</p><p>​         YOLO并没有尝试优化大型检测过程的各个组件，而是完全淘汰了该过程，并通过设计使其速度很快。像面孔或人这样的单一类别的检测器可以进行高度优化，因为它们只需要处理更少的变化。YOLO是一种通用检测器，可学会同时检测各种物体。</p><p>​       <strong>Deep MultiBox</strong> 与S-CNN不同，Szegedy等人训练卷积神经网络来预测感兴趣区域[8]，而不是使用选择性搜索。MultiBox还可以通过用单个类预测替换置信度预测来执行单个对象检测。但是，Multi-Box无法执行常规的对象检测，并且仍然只是较大检测管道中的一部分，需要进一步的图像块分类。YOLO和MultiBox都使用卷积网络来预测图像中的边界框，但是YOLO是一个完整的检测系统。</p><p>​        <strong>OverFeat.</strong>Sermanet等训练卷积神经网络执行定位并调整该定位器以执行检测。OverFeat有效地执行滑动窗口检测，但它仍然是不相交的系统。Over-Feat针对定位进行优化，而不是对检测性能进行优化。像DPM一样，定位器仅在进行预测时看到本地信息。OverFeat无法推断出全局上下文，因此需要进行大量的后处理才能产生连贯的检测结果。</p><p>​      <strong>MultiGrasp</strong>  我们的工作在设计上与Redmon等[26]在抓取检测方面的工作相似。我们的边界框预测方法基于MultiGrasp系统，可以进行回归分析。但是，抓取检测比对象检测要简单得多。MultiGrasp只需要为包含一个对象的图像预测单个可抓握区域。不必估计物体的大小，位置或边界或预测其类别，只需找到适合抓握的区域即可。<strong>YOLO预测图像中多个类的多个对象的边界框和类概率。</strong></p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><p>​      首先，我们在PASCAL VOC 2007上比较YOLO和其它的实时检测系统。为了理解YOLO和R-CNN变种之间的差异，我们探索了YOLO和R-CNN性能最高的版本之一Fast R-CNN[14]在VOC 2007上错误率。根据不同的误差曲线，我们显示YOLO可以用来重新评估Fast R-CNN检测，并减少背景假阳性带来的错误，从而显著提升性能。我们还展示了在VOC 2012上的结果，并与目前最先进的方法比较了mAP。最后，在两个艺术品数据集上我们显示了YOLO可以比其它检测器更好地泛化到新领域。</p><h3 id="4-1-与其它实时系统的比较"><a href="#4-1-与其它实时系统的比较" class="headerlink" title="4.1 与其它实时系统的比较"></a>4.1 与其它实时系统的比较</h3><p>​     目标检测方面的许多研究工作都集中在快速制定标准检测流程上[5]，[38]，[31]，[14]，[17]，[28]。然而，只有Sadeghi等实际上产生了一个实时运行的检测系统（每秒30帧或更好）[31]。我们将YOLO与DPM的GPU实现进行了比较，其在30Hz或100Hz下运行。虽然其它的努力没有达到实时性的里程碑，我们也比较了它们的相对mAP和速度来检查目标检测系统中精度——性能权衡。</p><p>​     快速YOLO是PASCAL上最快的目标检测方法；据我们所知，它是现有的最快的目标检测器。具有$52.7%$的mAP，实时检测的精度是以前工作的两倍以上。YOLO将mAP推到$63.4%$的同时保持了实时性能。</p><p>​       我们还使用VGG-16训练YOLO。这个模型比YOLO更准确，但也比它慢得多。对于依赖于VGG-16的其它检测系统来说，它是比较有用的，但由于它比实时的YOLO更慢，本文的其它部分将重点放在我们更快的模型上。</p><p>​        最快的DPM可以在不牺牲太多mAP的情况下有效地加速DPM，但仍然会将实时性能降低2倍[38]。与神经网络方法相比，DPM相对低的检测精度也受到限制。</p><p>​         减去R的R-CNN用静态边界框提出取代选择性搜索[20]。虽然速度比R-CNN更快，但仍然不能实时，并且由于没有好的边界框提出，准确性受到了严重影响。</p><p>​       快速R-CNN加快了R-CNN的分类阶段，但是仍然依赖选择性搜索，每张图像需要花费大约2秒来生成边界框提出。因此，它具有很高的mAP，但是0.5的fps仍离实时性很远。</p><p>​        最近更快的R-CNN用神经网络替代了选择性搜索来获得边界框，类似于Szegedy等[8]。在我们的测试中，他们最精确的模型达到了7fps，而较小的，不太精确的模型以18fps运行。VGG-16版本的Faster R-CNN要高出10mAP，但比YOLO慢6倍。Zeiler-Fergus的Faster R-CNN只比YOLO慢了2.5倍，但也不太准确。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggcox6jjhoj30ss0sadl8.jpg" srcset="/img/loading.gif" alt="image-20200702160649591" style="zoom:50%;" /></p><h3 id="4-2-VOC-2007-误差分析"><a href="#4-2-VOC-2007-误差分析" class="headerlink" title="4.2 VOC 2007 误差分析"></a>4.2 VOC 2007 误差分析</h3><p>​       为了进一步检查YOLO和最先进的检测器之间的差异，我们详细分析了VOC 2007的结果。我们将YOLO与Fast R-CNN进行比较，因为Fast R-CNN是PASCAL上性能最高的检测器之一并且它的检测代码是可公开得到的。</p><p>​       我们使用Hoiem等人[19]的方法和工具。对于测试时的每个类别，我们看这个类别的前N个预测。每个预测或者是正确的，或者根据错误类型进行分类：</p><p>Correct：正确的类别且IOU&gt; 0.5。</p><p>Localization：正确的类别，0.1 &lt; IOU &lt; 0.5。</p><p>Similar：类别相似，IOU &gt; 0.1。</p><p>Other：类别错误，IOU &gt; 0.1。</p><p>Background：任何IOU &lt; 0.1的目标。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggcvkg0y4yj30s80li41t.jpg" srcset="/img/loading.gif" alt="image-20200702175153208" style="zoom:50%;" /></p><p>​        图4显示了在所有的20个类别上每种错误类型平均值的分解图。</p><p>​        YOLO努力地正确定位目标。定位错误占YOLO错误的大多数，比其它错误源加起来都多。Fast R-CNN使定位错误少得多，但背景错误更多。它的检测的13.6%是不包含任何目标的误报。Fast R-CNN比YOLO预测背景错误的可能性高出近3倍。</p><h3 id="4-3-结合Fast-R-CNN-和YOLO"><a href="#4-3-结合Fast-R-CNN-和YOLO" class="headerlink" title="4.3 结合Fast R-CNN 和YOLO"></a>4.3 结合Fast R-CNN 和YOLO</h3><p>​       YOLO比Fast R-CNN的背景误检要少得多。通过使用YOLO消除Fast R-CNN的背景检测，我们获得了显著的性能提升。对于R-CNN预测的每个边界框，我们都会检查YOLO是否预测了类似的框。如果是这样，我们根据YOLO预测的概率和两个方框之间的重叠来提升该预测。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggcvkhdu9ij30rq0hw77o.jpg" srcset="/img/loading.gif" alt="image-20200702180926625" style="zoom:50%;" /></p><p><strong>表2 ：VOC 2007模型组合实验。</strong>我们研究了将各种模型与Fast R-CNN的最佳版本相结合的效果。其他版本的Fast R-CNN仅提供很小的好处，而YOLO提供了显着的性能提升。</p><p>​       最好的Fast R-CNN模型在VOC 2007测试集上达到了71.8%的mAP。当与YOLO结合时，其mAP增加了3.2%达到了75.0%。我们也尝试将最好的Fast R-CNN模型与其它几个版本的Fast R-CNN结合起来。这些模型组合产生了0.3到0.6%之间的小幅增加，详见表2。</p><p>​      来自YOLO的提升不仅仅是模型组合的副产品，因为组合不同版本的Fast R-CNN几乎没有什么好处。相反，正是因为YOLO在测试时出现了各种各样的错误，所以在提高Fast R-CNN的性能方面非常有效。</p><p>​     遗憾的是，这个组合并没有从YOLO的速度中受益，因为我们分别运行每个模型，然后结合结果。但是，由于YOLO速度如此之快，与Fast R-CNN相比，不会增加任何显著的计算时间。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggcvkghzvbj31mm0r0tou.jpg" srcset="/img/loading.gif" alt="image-20200702183042884"></p><h3 id="4-4-VOC-2012-的结果"><a href="#4-4-VOC-2012-的结果" class="headerlink" title="4.4 VOC 2012 的结果"></a>4.4 VOC 2012 的结果</h3><p>​     在VOC 2012测试集上，YOLO得分为$57.9%$的mAP。这低于现有的最新技术，接近于使用VGG-16的原始R-CNN，见表3。我们的系统与其最接近的竞争对手相比，在小目标上努力在bottle，sheep和tv/monitor等类别上，YOLO的得分比R-CNN或Feature Edit低8-10%。然而，在<code>cat</code>和<code>train</code>等其它类别上YOLO实现了更高的性能。</p><p>​      我们联合的Fast R-CNN + YOLO模型是性能最高的检测方法之一。Fast R-CNN从与YOLO的组合中获得了2.3%的提高，在公开排行榜上上移了5位。</p><h3 id="4-5-泛化能力：艺术作品中的人物检测"><a href="#4-5-泛化能力：艺术作品中的人物检测" class="headerlink" title="4.5 泛化能力：艺术作品中的人物检测"></a>4.5 泛化能力：艺术作品中的人物检测</h3><p>​     用于目标检测的学术数据集以相同分布获取训练和测试数据。在现实世界的应用中，很难预测所有可能的用例，而且测试数据可能与系统之前看到的不同[3]。我们在Picasso数据集上[12]和People-Art数据集[3]上将YOLO与其它的检测系统进行比较，这两个数据集用于测试艺术品中的人物检测。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggcvkgya9nj312u0hc0wc.jpg" srcset="/img/loading.gif" alt="image-20200702184704139"></p><p>​     图5显示了YOLO与其他检测方法之间的比较性能。作为参考，我们在<code>person</code>上提供VOC 2007的检测AP，其中所有模型仅在VOC 2007数据上训练。Picasso数据集上的模型在VOC 2012上训练，而People-Art数据集上的模型则在VOC 2010上训练。</p><p>​      R-CNN在VOC 2007上有高AP。然而，当应用于艺术品时，R-CNN明显下降。R-CNN使用选择性搜索来调整自然图像的边界框提出。R-CNN中的分类器步骤只能看到小区域，并且需要很好的边界框提出。</p><p>​      DPM在应用于艺术品时保持了其AP。之前的工作认为DPM表现良好，因为它具有目标形状和布局的强大空间模型。虽然DPM不会像R-CNN那样退化，但它开始时的AP较低。</p><p>​      YOLO在VOC 2007上有很好的性能，在应用于艺术品时其AP下降低于其它方法。像DPM一样，YOLO建模目标的大小和形状，以及目标和目标通常出现的位置之间的关系。艺术品和自然图像在像素级别上有很大不同，但是它们在目标的大小和形状方面是相似的，因此YOLO仍然可以预测好的边界框和检测结果。</p><h2 id="5-现实环境下的实时检测"><a href="#5-现实环境下的实时检测" class="headerlink" title="5. 现实环境下的实时检测"></a>5. 现实环境下的实时检测</h2><p>​       YOLO是一种快速，精确的目标检测器，非常适合计算机视觉应用。我们将YOLO连接到网络摄像头，并验证它是否能保持实时性能，包括从摄像头获取图像并显示检测结果的时间。</p><p>​       由此产生的系统是交互式和参与式的。虽然YOLO单独处理图像，但当连接到网络摄像头时，其功能类似于跟踪系统，可在目标移动和外观变化时检测目标。系统演示和源代码可以在我们的项目网站上找到：<a href="https://link.jianshu.com/?t=http%3A%2F%2Fpjreddie.com%2Fyolo%2F" target="_blank" rel="noopener">http://pjreddie.com/yolo/</a>。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggcvljudj6j31dm0q2tiz.jpg" srcset="/img/loading.gif" alt="image-20200702195751922"></p><h2 id="6-结论"><a href="#6-结论" class="headerlink" title="6. 结论"></a>6. 结论</h2><p>​       我们介绍了YOLO，一种统一的目标检测模型。我们的模型构建简单，可以直接在整张图像上进行训练。与基于分类器的方法不同，YOLO直接在对应检测性能的损失函数上训练，并且整个模型联合训练。</p><p>​      Fast YOLO是文献中最快的通用目的的目标检测器，YOLO推动了实时目标检测的最新技术。YOLO还很好地泛化到新领域，使其成为依赖快速，鲁棒的目标检测应用的理想选择。</p>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>文献翻译——深度多任务多标签CNN对人脸属性进行有效分类</title>
    <link href="/2020/06/26/%E6%96%87%E7%8C%AE%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%A4%9A%E6%A0%87%E7%AD%BECNN%E5%AF%B9%E4%BA%BA%E8%84%B8%E5%B1%9E%E6%80%A7%E8%BF%9B%E8%A1%8C%E6%9C%89%E6%95%88%E5%88%86%E7%B1%BB/"/>
    <url>/2020/06/26/%E6%96%87%E7%8C%AE%E7%BF%BB%E8%AF%91%E2%80%94%E2%80%94%E6%B7%B1%E5%BA%A6%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%A4%9A%E6%A0%87%E7%AD%BECNN%E5%AF%B9%E4%BA%BA%E8%84%B8%E5%B1%9E%E6%80%A7%E8%BF%9B%E8%A1%8C%E6%9C%89%E6%95%88%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<p>今天阅读了《Deep Multi-task Multi-label CNN for Effective Facial Attribute Classiﬁcation》做了一些读书笔记。</p><p>原文：</p><div class="row">    <embed src="./1.pdf" width="100%" height="550" type="application/pdf"></div><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>​        最新的FAC方法独立的进行面部检测和属性分类，没有完全利用这两部分之间的依赖性。此外许多方法使用相同的CNN网络结构预测所有的面部属性，忽略了不同面部属性学习的复杂性。DMM-CNN联合优化了两个紧密相关的任务（面部位置检测和属性分类）。为更好的处理不同面部属性的学习复杂性，将面部属性分为两组：客观属性和主观属性。为这两组属性分别设计不同的网络结构提取其特征。提出了一种新颖的动态权重方案来为训练过程中的每一个面部属性自动分配损失权重。此外，提出了一种自适应阈值策略来缓解多标签学习分类不平衡的问题。实验采用CelebA和LFWA数据集进行，表明了所提出的DMM-CNN方法对比现有FAC方法的优越性。</p><h2 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1.介绍"></a>1.介绍</h2><p>​         FAC的任务是给定一张面部图片，预测出多种面部属性，例如性别、吸引力、微笑。尽管FAC的任务仅仅是一个图片等级的分类任务，但它仍然非常重要，主要因为视角、光线造成的面部外观改变。</p><p>最新的FAC方法使用CNN进行面部属性分类，这些方法可被粗略的分类为：</p><p>（1）基于单标签学习的FAC方法。基于单标签学习的FAC方法通常提取面部图片的CNN特征，然后使用支持向量机（SVM）进行面部属性分类。然而这些方法独立的预测每个属性，并且忽略了属性之间的联系。</p><p>（2）基于多标签学习的FAC方法。相反，基于多标签学习的FAC方法可以同时预测多个属性，从相对较低的CNN层中提取分享的特征，从相对高的CNN层中学习详细属性分类。</p><p>​         以上这些方法首先进行面部位置检测，然后进行面部属性预测，也就是将这两个联系紧密的任务分开进行训练。因此，这两个任务之间自身的联系并没有被完全有效的利用。此外，一些基于FAC方法的多标签学习同时使用单独的CNN预测面部属性。这些方法平等的对待不同的属性（对所有属性使用相同的网络框架）。忽略了这些属性不同的学习复杂性。（例如学习预测是否戴眼镜比预测是否是尖鼻子相对更容易）。特别的，一些属性（例如大嘴、鹅蛋脸）非常主观，更难被识别，有时甚至人类也感到困惑。更糟的是，训练集经常遭受一些面部属性标注不平衡的问题。（例如秃头只有很少量的正样本）。重新平衡多标签数据是一个非常重要的任务。</p><p>​         为了缓解以上问题，我们提出了一种新颖的深度多任务多标签CNN方法（DMM-CNN）。联合优化两个紧密相关的任务（面部位置检测和属性分类），以提高基于多任务学习的FAC的性能。为更好的处理不同面部属性的学习复杂性，将面部属性分为两组：客观属性和主观属性。为这两组属性分别设计两个不同的网络结构提取其判别特征。提出了一种新颖的动态权重方案来为训练过程中的每一个面部属性自动分配损失权重。此外，提出了一种自适应阈值策略来缓解多标签学习分类不平衡的问题。</p><p>​         与我们之前提出的MCFA方法相似，DMM-CNN方法也采用多任务学习框架。然而，MCFA和DMM-CNN有一些重大的不同。首先，MCFA聚焦在使用多尺度CNN提取语义属性信息的问题，DMM-CNN针对于克服面部属性的不同学习复杂性问题（通过为主观属性和客观属性设计不同的网络结构，提出一个动态权重体系）。第二，MCFA使用一个固定的判定阈值策略来缓解类不平衡问题。第三，MCFA联合学习人脸检测、面部特征点检测FLD和面部属性分类，而DMM-CNN同时做了FLD和FAC。人脸检测没有包含在DMM-CNN中的原因是使用面部检测的辅助任务仅仅略微提高了FAC的性能，但增加了计算负担。此外，FLD明确扮演了人脸定位的角色。最后，MCFA中的FLD模块仅仅提供了5个可用的面部特征（左右眼、嘴角、鼻尖）。相反，DMM-CNN中的FLD模块输出72个面部特征，可以为FAC提供更多有益的辅助信息。</p><p>本文的主要贡献总结如下：</p><ul><li><p>我们根据学习复杂度将不同的面部特征划分为主观属性和客观属性。其中有两个不同的SPP层被用来提取特征。据我们所知，这篇论文是第一篇考虑不同面部属性的学习复杂度，学习多深度神经网络来提高FAC性能的论文。</p></li><li><p>提出了一种新颖的动态权重体系，利用包含所有验证集的验证损失改变率，来自动的给面部属性分配权重。通过这种方式，训练过程集中于对较难的面部属性分类。</p></li><li><p>我们建立了一个自适应阈值策略来对多标签学习进行精确的面部属性分类。该策略考虑面部属性的不平衡数据分布。因此，一些FAC属性的不平衡分类问题也被从决策水平上有效缓解。</p></li></ul><p>本文结构组织如下：</p><p>第二部分回顾了相关工作，第三部分介绍了所提出方法的详细细节。第四部分在CelebA和LFWA数据集上对所提出方法与其他一些最新的方法的性能进行了评价。第五部分做了总结。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2 相关工作"></a>2 相关工作</h2><p>​        在过去几十年中，FAC取得了巨大的进步。传统的FAC方法依赖于手工标注特征来进行属性分类。随着深度学习的发展，当前最新的FAC方法使用CNN模型来预测属性，并表现出了巨大的性能改善。我们提出的方法将基于CNN的多任务学习，多标签学习和属性分组紧密联系起来。在这一部分我们简要介绍基于CNN的相关工作。</p><h3 id="2-1-多任务学习"><a href="#2-1-多任务学习" class="headerlink" title="2.1 多任务学习"></a>2.1 多任务学习</h3><p>​        多任务学习(MTL)是一个借助相关辅助任务，提高目标任务性能的有效学习范式。MTL已经被证明在多种CV任务中有效。CNN模型可以很自然的应用于MTL，其中所有任务在深层共享和学习共同的特征表示。例如zhang等将FLD与许多相关任务一起进行，例如性别分类和姿态估计。Tan等人通过MTL方式联合学习多种用于行人分析的注意机制（包括解析注意，标记注意和空间注意）。</p><p>​        在多任务深度学习中，为不同的损失函数适当分配权重具有重要意义。Kendall等人提出基于每个任务的同方差不确定性权重损失函数，其中权重是自动从数据中获得的。Chen等人开发了一种梯度归一化(GradNorm)方法，通过动态调整梯度大小来执行多任务深度学习。根据不同任务的训练率分配损失权重。最近，Liu等人开发了一个用于MTL的多任务注意力网络，该网络以端到端的方式自动学习任务分享和任务特定的特征。他们建立了一个新颖的权重体系——动态权值平均（DWA）,基于每个任务的损失率变化来学习权重。</p><h3 id="2-2-多标签学习"><a href="#2-2-多标签学习" class="headerlink" title="2.2 多标签学习"></a>2.2 多标签学习</h3><p>​         一方面，传统的基于CNN的FAC方法主要依赖单标签学习来预测面部属性。例如，Liu等人提出将两个定位网络（LNets）和一个属性网络（ANet）级联，分别对脸部区域定位和提取特征。他们利用从ANet中提取的特征训练40个支持向量机来对40个属性进行分类。基于单标签学习的FAC方法单独考虑每一个属性的分类方法，忽略了属性之间的相关性。此外，这些方法通常是耗时和成本高昂的。</p><p>​        另一方面，基于多标签学习的FAC方法在一个端到端训练的网络中同时预测多种面部属性。由于每张人脸图像都与多个属性标签相关联，因此多标签学习非常适合于FAC。例如，Ehrlich等使用基于限制玻尔兹曼机(RBM)的模型进行属性分类。Rudd等人引入了混合目标优化网络(MOON)来解决多标签不平衡问题。Huang等提出了一种贪婪神经结构搜索方法来自动发现最优的树状网络结构，可以联合预测多个属性。</p><p>​        现有的基于多标签学习的FAC方法对每个属性使用相同的网络架构，通常是在CNN的上层学习面部属性的特征。然而，不同的面部属性具有不同的学习复杂性。因此，开发一种新的CNN模型，考虑属性学习的多样性，而不是在训练阶段将属性平等对待，更有吸引力。</p><h3 id="2-2-属性分类"><a href="#2-2-属性分类" class="headerlink" title="2.2 属性分类"></a>2.2 属性分类</h3><p>​         根据不同的标准，可以将面部属性分为若干组。例如Emily等根据属性位置将面部属性划分为9组，明确地学习人脸图像中相似位置的属性之间的关系。Han等将人脸属性按照数据类型和语义意义分为序数属性和名词性属性、整体属性和局部属性。相应地，定义了四种类型的子网络(具有相同的网络结构)，分别对应于整体标称、整体序数、局部标称和局部序数属性，其中每个子网络使用不同的损耗函数进行FAC。Cao等将人脸属性按照对应位置划分为上、中、下、全图像4个属性组，设计了4个任务专用子网络(对应4个属性组)和一个FAC共享子网络。</p><p>​         在本文中，不同于以上属性分类方法，我们根据不同的学习复杂性将属性分为两组：主观属性和客观属性。对应的，我们设计了两种不同的网络结构，分别能提取出有利于对主观和客观属性进行分类的不同层次的特征。</p><h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3 方法"></a>3 方法</h2><p>​      在这一部分，我们详细介绍了所提出的DMM-CNN方法，该方法利用多任务学习和多标签学习实现有效的FAC。</p><h3 id="3-1-概述"><a href="#3-1-概述" class="headerlink" title="3.1 概述"></a>3.1 概述</h3><p>​         我们所提出方法的概述如图二所示，在本文中为了提取共享特征，我们采用ResNet50并且移去最终的全局池化层。在共享特征的基础上，我们进一步进行多任务多标签学习，提取两个相关任务（FAC和FLD）的任务详细特征。</p><p>​         具体来说，在FAC任务中，为了应对面部属性不同的学习复杂性，我们将面部属性分为两组(客观属性和主观属性)【3.2.1节】，并为这两组设计了两种不同的网络架构。特别是利用两个不同的空间金字塔池(SPP)层对网络中的客观属性和主观属性分别提取不同层次的语义信息【3.2.2节】。对于FLD任务，检测到72个面部特征点。因此，整个网络有三种输出（客观属性预测输出、主观属性预测输出、面部特征点回归）。</p><p>​         在训练阶段【3.3节】整个框架将两个任务的损失合并为最终损失，最终开发出一种新颖的自适应加权方案，将损失权重自动的分配给每个面部属性，从而使训练集中在对更困难面部属性的分类。此外，为了缓解类不平衡的问题，开发了一种自适应阈值策略来准确预测每个属性的标签。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg75fbcytfj31mg0tedp8.jpg" srcset="/img/loading.gif" alt="image-20200627210436393"></p><h3 id="3-2-CNN结构"><a href="#3-2-CNN结构" class="headerlink" title="3.2 CNN结构"></a>3.2 CNN结构</h3><p>​          在下面的小节中，我们分别详细介绍两组面部属性，SPP层和面部标志检测任务。</p><h4 id="3-2-1-客观属性和主观属性"><a href="#3-2-1-客观属性和主观属性" class="headerlink" title="3.2.1 客观属性和主观属性"></a>3.2.1 客观属性和主观属性</h4><p>​        为了有效地利用面部属性的内在联系和异质性，可以将这些属性划分为不同的组。在本文中，我们建议将面部属性分为两类：主观属性（例如“有吸引力”，“大鼻子”）和客观属性（例如“秃头”，“男性”）。有关更多详细信息，请参见图3。 我们的设计基于以下观察：最先进的FAC方法通常在预测主观属性方面的准确性要比客观属性低得多。例如，与“微笑”和“年轻”属性相比，通常更容易对“戴帽子”和“戴眼镜”属性进行分类。这主要是因为主观属性通常以微妙的形式出现，这使得CNN模型更难以学习决策边界。换句话说，客观和主观属性显示出不同的学习复杂性。因此，最好为这两组属性设计不同的网络体系结构。</p><p>​        在我们的实现中，用于学习对象属性的分支包括一个1级SPP层（请参见3.2.2节）和两个完全连接的层，其输出特征分别为1024和22维（客观属性的数量）。学习主观属性的分支包括一个三级SPP层和三个完全连接的层，其输出特征分别为2048、1024和18（主观属性的数量）维。以这种方式，为主观属性设计的网络比为客观属性设计的网络多编码了更高级别的语义信息（这有助于预测主观属性）。</p><h4 id="3-2-2-SPP层"><a href="#3-2-2-SPP层" class="headerlink" title="3.2.2 SPP层"></a>3.2.2 SPP层</h4><p>​        由He等人提出的空间金字塔合并（SPP）层，被引入来解决CNN网络的固定图像大小要求的问题。SPP层池化最后一个卷积层的顶部特征，并且无论输入大小/比例如何，它都可以生成固定长度的输出。SPP聚合了来自网络较深层的信息，从而有效避免了裁剪或扭曲输入图像的约束。在本文中，我们使用1级SPP层提取目标属性的特征，并使用3级SPP层提取主观属性的特征（n级SPP层将特征图划分为n×n个块， 然后在每个块中执行最大池化操作）。1级SPP层和3级SPP层的输出要素图的大小分别为2、048×1和28、672×1。因此，我们可以利用SPP图层将任何大小的面部图像输入到网络。如前所述，高级语义特征用于预测主观属性，而低级外观特征用于对客观属性进行分类。特征的不同级别有利于对两组属性进行分类。</p><h4 id="3-2-3-面部特征点检测"><a href="#3-2-3-面部特征点检测" class="headerlink" title="3.2.3  面部特征点检测"></a>3.2.3  面部特征点检测</h4><p>​         在本文中，通过利用多任务学习共同训练了两个不同但相关的任务（即FLD和FAC）。在此，FAC是目标任务，而FLD是辅助任务。在多任务学习的范式下，利用目标任务和辅助任务之间的内在依赖关系来有效地提高FAC的性能。面部图像的特征点信息有利于提高FAC的准确性。例如，嘴巴周围的特征点可以提供辅助信息，以帮助预测“微笑”属性。</p><p>​         与我们以前的研究仅考虑5个面部标志不同，我们使用dlib库获取更多的面部特征点（总共72个面部标志），这些轮廓点概述了眼睛，眉毛，鼻子，嘴巴和面部边界。注意，不同的面部属性通常与不同的面部标志有关。因此，使用更多的面部标志有利于提高FAC的性能。FLD分支采用由1级SPP层获得的2,048维特征向量作为输入，并由两个完全连接的层组成，其输出特征分别为1,024和144维。</p><h3 id="3-3-训练"><a href="#3-3-训练" class="headerlink" title="3.3 训练"></a>3.3 训练</h3><p>​        如前所述，不同的面部特征具有不同的学习复杂性。为了应对面部属性的各种学习复杂性，除了针对客观和主观属性采用不同的网络体系结构之外，我们还提出了一种新颖的动态加权方案，可以自动将损失权重分配给不同的属性。此外，为了缓解多标签训练类不平衡问题，开发了一种自适应阈值策略来预测每个属性的标签。</p><p>​         在本文中，为了简化不同任务的处理，我们使用均方误差（MSE）损失函数。</p><p>​        1) 面部特征点检测（FLD）：FLD的MSE损失为</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg75e61ffrj30uk04i3yn.jpg" srcset="/img/loading.gif" alt="image-20200627210325932" style="zoom: 33%;" /></p><p>其中N是训练图像的数量。 yˆFLD∈R2T表示从网络获得的面部特征的输出（即坐标矢量）（T是面部界标的数量，在本文中我们使用72个面部界标）。 yF LD∈R2T表示地面真实坐标向量。</p><p>​        2）面部属性分类：FAC的MSE损失为</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg75swh4ixj30u804sq3d.jpg" srcset="/img/loading.gif" alt="image-20200627211739787" style="zoom:33%;" /></p><p>其中ˆyF AC和yF AC（∈{1，−1}）表示预测的输出和 分别对应于第i个训练图像的第j个属性的标签。      </p><p>​        3）共同损失：共同损失包括FLD和FAC的损失，可以写为</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg75zsr149j30s604qmxh.jpg" srcset="/img/loading.gif" alt="image-20200627212417691" style="zoom:33%;" /></p><p>​        其中，J是面部属性的总数。λt= [λ1t，λ2t，···，λJt] T  表示在第t次迭代期间对应于面部属性J</p><p>的权重向量。 β是正则化参数（根据经验将β设置为0.5）。</p><p>​        4）动态加权方案。 在本文中，我们提出了一种动态加权方案来自动为所有面部属性分配权重。 根据验证损失趋势动态分配损失权重。 具体而言，权重定义为</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg765fiyrcj30tu05iaam.jpg" srcset="/img/loading.gif" alt="image-20200627212942632" style="zoom:33%;" /></p><p>​       其中Lj，V AL（t）是验证损失（根据FAC计算到等式 （2）验证集的每个属性）在训练的第t次迭代中。 以这种方式，如果验证损失没有减少，则将与面部属性相对应的权重分配为低值，而如果验证损失显着下降，将为这些权重分配较高的值。</p><p>​        在初始训练过程中，为容易分类的属性分配了较大的权重，以便可以快速减少其相应的MSE损失。随着迭代的进行，难以分类的属性的MSE损失变得相对较大并缓慢下降，而易于分类的属性的MSE损失则变得较小。因此，在训练过程的后期，网络将重点放在对分类困难的属性进行分类的训练上（每个属性的损失由权重与其相应的MSE损失的乘积组成）</p><p>​        注意，加权方案也在[30]和[38]中开发。 但是，所提出的动态加权方案与[30]，[38]中的方案之间存在显着差异。在[30]中，权重是根据训练损失变化率计算的。 在[38]中，根据验证损失和平均验证损失趋势来计算权重。注意，验证损失可能不适用于确定权重。相反，所提出的动态加权方案仅基于验证损失趋势来计算权重。而且，[30]中的权重是根据每个迭代中每个时期的平均训练损失（在训练集中）获得的。与[30]不同，[38]和我们提出的加权方案利用了验证集，这可能有益于提高学习模型的泛化能力（因为在反向传播过程中验证集不直接用于计算梯度）。在[38]中，验证损失是在每次迭代过程中对一小批（仅包含10个验证图像）进行计算的，而在我们的方法中，它是针对每P次迭代的整个验证集计算的。因此，所提出的动态加权方案能使损失更加稳定的减少。</p><p>​        5）自适应阈值策略。 我们根据网络的最终输出预测第j个面部属性lj的标签：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7q6i7cm6j30t40600t6.jpg" srcset="/img/loading.gif" alt="image-20200628090243455" style="zoom:33%;" /></p><p>​        其中τj是阈值参数。 如果预测输出大于阈值τj，则分配正样本标签。</p><p>现有的FAC方法通常将阈值τj设置为0。但是，由于类别不平衡的问题（即，一个类别的样本数量明显大于一个属性的另一类别的样本数量），使用固定阈值不是最佳解决方案，尤其是对于某些高度不平衡的面部属性。在本文中，我们介绍了一种自适应阈值策略，该策略可自适应地更新阈值，如下所示：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7qdvulhij30ug04o74j.jpg" srcset="/img/loading.gif" alt="image-20200628090948892" style="zoom:33%;" /></p><p>​     其中τt∈RJ是第t次迭代的阈值。 V是验证集中的样本数。  NF P∈RJ（NF N∈RJ）表示第t次迭代的验证集中的假阳性（假阴性）。假阳性的值越大（或者假阴性的值越小），阈值应该越大。因此，NFP和NFN之间的差异可以用于调整阈值。我们还在等式中考虑当前迭代。（6），因为随着训练时期的增加（阈值适应更大的值），应该更加重视错误的预测。γ是固定参数（我们在实验中将其设置为0.01）。</p><p>算法1总结了所提出的DMM-CNN方法的训练步骤：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7qw45e4pj30v50u0jxx.jpg" srcset="/img/loading.gif" alt="image-20200628092719860" style="zoom:50%;" /></p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4 实验"></a>4 实验</h2><p>​        在本节中，我们首先介绍两个用于评估的公共FAC数据集。 然后，我们进行消融研究，以讨论所提出的DMM-CNN方法的每个组件的影响。最后，我们将提出的DMM-CNN方法与几种最新的FAC方法进行了比较。</p><h3 id="4-1-数据集和参数设定"><a href="#4-1-数据集和参数设定" class="headerlink" title="4.1  数据集和参数设定"></a>4.1  数据集和参数设定</h3><p>​        CelebA [39]是一个大规模的面部数据集，提供有标记的边界框以及5个特征点和40个面部属性的注释。它包含用于训练的162,770张图像，用于验证的19,867张图像和用于测试的19,962张图像。CelebA中的图像涵盖了较大的姿势变化和背景杂乱。LFWA [40]是另一个具有挑战性的人脸数据集，其中包含13,143张带有73个二值人脸属性注释的图像。我们从LFWA中选择与CelebA相同的40个属性。对于LFWA，我们对在CelebA上训练的模型进行微调，并使用LFWA的原始图像和深层漏斗图像作为训练集，以防止过度拟合。最终，使用了13144张图像进行训练，使用了6571张图像进行LFWA测试。 由于LFWA不提供验证集，因此我们直接更新动态权重，并在训练集上使用自适应阈值策略。</p><p>​         所提出的方法是基于开源深度学习框架pytorch实施的，其中使用一个NVIDIA TITAN X GPU训练了15个时期的模型，块大小为64。基本学习率设置为0.001，当验证损失停止减少时，我们将学习率乘以0.1。模型大小大约为360M。</p><h3 id="4-2-消融研究"><a href="#4-2-消融研究" class="headerlink" title="4.2 消融研究"></a>4.2 消融研究</h3><p>​        在本小节中，我们将进行消融研究，以评估CelebA和LFWA数据集上提出的DMM-CNN不同组件的有效性。</p><p>​        <img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7roia6cyj30ys0ou41e.jpg" srcset="/img/loading.gif" alt="image-20200628095437584" style="zoom: 33%;" /></p><p>​        我们对我们所提出的DMM-CNN方法的一些变体进行了评估。具体来说，Baseline表示我们仅使用ResNet50（具有40个输出单位）来提取特征并对属性进行分类。DMM-FAC表示我们仅执行FAC的单个任务，而不使用FLD的辅助任务。DMM-EQ-FIX表示我们对所有属性使用相等的损失权重（与1.0相同），而不必依赖于建议的动态加权方案，并且使用固定阈值（即0.0）来预测每个属性的标签，而不是使用自适应阈值。DMM-EQ-AT表示我们对所有属性和建议的自适应阈值策略使用相等的损失权重。DMM-DW-FIX表示我们使用动态加权方案和固定阈值。DMM-SPP表示我们使用3级SPP层和三个完全连接的层来预测所有属性（使用与主观属性分支相同的网络体系结构）而不进行属性分组。DMM-CNN is the proposed method. The details of all the competing variants are listed in Table 1.</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7s87pvc9j31dx0u011n.jpg" srcset="/img/loading.gif" alt="image-20200628101333266"></p><p>​        图3显示了通过不同变体获得的性能（即准确率）。我们有以下观察结果：</p><ul><li>与基准相比，其他所有变量均具有更好的性能（尤其是在“拱形的眉毛”，“大嘴唇”和“窄眼”属性上），这表明了使用特定任务特征进行FAC的重要性。</li><li>通过将DMM-FAC与DMM-CNN进行比较，我们可以看到，多任务学习通过利用FAC与FLD之间的内在关系，有利于提高FAC的性能。</li><li>与DMM-EQ-FIX相比，DMM-DW-FIX的平均分类率更高，这表明使用动态加权方案的优越性。</li><li>DMM-EQ-AT获得的平均分类率高于DMM-EQ-FIX获得的平均分类率，这表明使用自适应阈值策略的有效性。</li><li>与基线相比，LFWA上的DMM-DW-FIX和DMM-EQ-AT的改进比CelebA上的改进更为明显。具体而言，DMM- DW-FIX的准确性提高了5.52％（0.91％），而DMM-EQ-AT的LFWA（CelebA）的准确性提高了3.98％（0.95％）。在CelebA上的提升并不明显。在某些论文中也观察到了这种现象[33]，[41]，[42]。这可能是因为训练集的分布和CelebA的测试集存在很大差异，并且在CelebA标签中存在一些噪声，尤其是对于主观属性[43]，这导致CelebA的测试集难以显着改进。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7teiog8bj314m0istbe.jpg" srcset="/img/loading.gif" alt="image-20200628105413581" style="zoom:33%;" /></p><ul><li><p>与DMM-SPP相比，DMM-CNN具有更好的准确性（即CelebA和LFWA分别提高了0.30％和1.81％）。因此，考虑到面部属性的不同学习复杂性，设计不同的网络体系结构将有助于提高FAC的性能。</p></li><li><p>在所有变体中，DMM-CNN达到了最佳准确性，这可以归因于利用面部属性的不同学习复杂性的多任务学习和多标签学习框架。</p><p>​    损失加权方案在FAC的性能中起着至关重要的作用。我们比较了不同加权方案的性能。 具体来说，我们评估以下四个代表性的加权方案：1）统一加权（UW）方案，其中对应于不同属性的所有权重均设置为1.0；2）中提出的动态加权平均（DWA）方案，其中训练集中的损失变化率用于自动学习权重；3）文献[38]中提出的自适应加权（AW）方案，其中使用块中的验证损失和平均验证损失趋势来获得权重；4）提出的动态加权方案，该方案利用了整个验证集中验证损失的变化率。表2给出了CelebA和LFWA数据集上不同加权方案的实验结果。我们可以看到，与其他加权方案相比，采用提出的动态加权方案的方法可获得最佳性能，这可以验证所提出方案的有效性。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7u01a8ydj30y70u0tfi.jpg" srcset="/img/loading.gif" alt="image-20200628111453942" style="zoom: 33%;" /></p><p>在图5中，我们进一步可视化了训练阶段验证集上的平均验证损失和两个代表性属性损失（即，客观属性“ MouthOpen”和主观属性“ Young”）的变化。这里，分别采用了提出的动态加权方案和固定加权方案（即，将每个属性的权重设置为1.0）。我们可以看到，基于动态加权方案的平均验证损失比基于固定加权方案的平均验证损失下降得更快。客观属性（即“张开嘴”）的训练收敛速度快于主观属性（即“年轻”）的收敛速度。在初始训练阶段，“ MouthOpen”属性的损失在大约15,000次迭代后迅速下降并收敛。相反，“ Young”属性的损失在大约30,000次迭代后缓慢下降并收敛。随着训练的进行，网络将重点放在对那些困难的主观属性进行分类上。通常，使用动态加权方案的损失通常比使用固定加权方案的损失下降得更多和更快。这表明在优化具有不同学习复杂性的多标签学习任务时，动态权重至关重要。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7u9kl3gxj30ya0ssaf3.jpg" srcset="/img/loading.gif" alt="image-20200628112403503" style="zoom:33%;" /></p><p>我们分别在图6和图7的训练阶段中可视化动态权重和自适应阈值的变化。首先，在图6中，给出了在训练阶段对应于两个代表性面部属性（即，“张开嘴”和“年轻”）的两个动态权重的曲线。我们可以观察到对应于两个属性的动态权重的变化是不稳定的。这主要是因为所提出的加权方案根据属性损失变化的速率动态地为每个属性分配了权重（参见等式（4））。换句话说，当属性的损失显着下降时，将为该属性分配较大的权重（因为该属性的学习过程不会收敛）。因此，动态权重反映了不同属性的学习率，这些学习率可能会发生很大变化。但是，注意到这两个属性的损失不断减少并稳定收敛（请参见图5）。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7unwfzizj30yb0u0dks.jpg" srcset="/img/loading.gif" alt="image-20200628113750271" style="zoom:33%;" /></p></li></ul><p>其次，在图7中，给出了在训练阶段对应于十个随机选择的面部属性的自适应阈值的曲线。我们可以观察到阈值的变化是稳定的。 这主要是由于使用了假正样本和假负样本之间的不同来调整阈值。 随着迭代的进行，差异变得更加稳定。</p><h3 id="4-3-与最新FAC方法的对比"><a href="#4-3-与最新FAC方法的对比" class="headerlink" title="4.3 与最新FAC方法的对比"></a>4.3 与最新FAC方法的对比</h3><p>在本小节中，我们将提出的DMM-CNN方法与几种最新的FAC方法的性能进行比较，包括（1）PANDA [11]，它使用部分模型来提取特征和SVM作为分类器；（2）LNets + ANet [31]，它级联两个定位网络和一个属性网络，并对每个属性使用一个SVM分类器；（3）MOON [19]，一种新颖的混合目标优化网络，解决了多标签不平衡问题。（4）NSA（采用中位数规则）[14]，它使用分步方法进行FAC；（5）MCNN-AUX [20]，根据属性位置将40个属性分为九组；（6）MCFA [18]，我们先前的工作，利用了FAC和辅助任务（面部检测和FLD）之间的固有依赖性。请注意，由于MOON没有报告LFWA的结果，因此没有在LFWA数据集上给出通过MOON获得的准确性。（7）GNAS [33]，提出了一种有效的贪婪神经体系结构搜索方法，可以自动学习多属性深度网络体系结构。（8）AW-CNN [36]，它开发了一种新颖的自适应加权多任务深度卷积神经网络来预测人的属性。 （9）PS-MCNN-LC [35]，它通过利用身份信息和属性关系引入了部分共享的多任务网络。</p><p> 表3显示DMM-CNN优于这些竞争方法，在CelebA（LFWA）上的平均准确度达到91.70％（86.56％）。与使用按属性SVM分类器的PANDA和LNets + ANet相比，DMM-CNN通过利用多标签学习获得了卓越的性能。我们的DMM-CNN也比MCNN-AUX，NSA和MOON具有更好的性能。值得指出的是，我们的方法仅利用了两组属性（即客观属性和主观属性），而MCNN-AUX使用了9组属性。即使使用较少的属性组，DMM-CNN仍可以实现比MCNN-AUX更高的精度。 DMM- CNN在很大程度上优于MCFA，这证明了使用更多面部标志信息和属性分组机制的有效性。</p><p>​        所提出的DMM-CNN方法与LFWA上的MCNN-AUX具有相似的精度。 DMM-CNN在所有40个属性中的20个属性上实现了最高的准确性，与竞争方法相比，主观属性（例如“尖鼻子”，“微笑”和“眉毛”）的性能得到了显着改善。就CelebA和LFWA数据集的平均识别率而言，提出的DMM-CNN方法比GNAS具有更好的性能。这可以归因于所提出的多任务多标签学习框架的有效性，在该框架中，分别设计了两种不同的网络体系结构以提取用于对客观和主观属性进行分类的特征。与手动设计网络体系结构的DMM-CNN不同，GNAS会自动发现树状的深度神经网络体系结构以进行多属性学习。因此，GNAS的培训过程比较耗时。 与AW-CNN相比，提出的DMM-CNN方法获得了相似的精度。与通过使用多任务学习框架（识别一个属性被视为一个任务）来预测多人属性的AW-CNN不同，该方法联合学习了两个紧密相关的任务（即FLD和FAC） 。注意，在CelebA和LFWA数据集上，提出的DMM-CNN方法都比PS-MCNN-LC实现了更差的性能。PS-MCNN-LC设计了一个共享网络（SNet），以学习不同属性组的共享特征，同时针对从低层到高层的每组属性采用任务专用网络（TSNet）。但是，PS-MCNN-LC利用了局部约束丢失（LCLoss），后者要求将人脸身份作为附加属性。此外，还需要谨慎选择SNet和TSNets中的通道数，以确保最终性能。总体而言，所有竞争方法之间的性能比较表明了该方法的有效性。<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1gg7xkt5jh2j30x50u0qlp.jpg" srcset="/img/loading.gif" alt="image-20200628131839750"></p><h2 id="5-总结"><a href="#5-总结" class="headerlink" title="5 总结"></a>5 总结</h2><p> 在本文中，我们提出了一种新颖的用于FAC的深度多任务多标签CNN方法（DMM-CNN）。 DMM-CNN通过共同执行FAC和FLD任务有效地提高了FAC的性能。基于主观和客观属性的划分，采用了不同的网络架构和新颖的动态加权方案来处理面部属性的各种学习复杂性。对于多标签学习，开发了自适应阈值策略来减轻分类不平衡的问题。在公开的CelebA和LFWA数据集上进行的实验表明，与几种最新的FAC方法相比，DMM-CNN具有更高的性能。</p><h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>本文工作得到了国家重点研发计划（2017YFB1302400），国家自然科学基金（61571379，U1605252、61872307）以及中国福建省自然科学基金（2017J01127和2018J01576）的支持。</p>]]></content>
    
    
    <categories>
      
      <category>文献翻译</category>
      
    </categories>
    
    
    <tags>
      
      <tag>FAC</tag>
      
      <tag>CNN</tag>
      
      <tag>DMM-CNN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>学习规划(6.25)</title>
    <link href="/2020/06/25/%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%92-6-25/"/>
    <url>/2020/06/25/%E5%AD%A6%E4%B9%A0%E8%A7%84%E5%88%92-6-25/</url>
    
    <content type="html"><![CDATA[<h1 id="近期要看的论文"><a href="#近期要看的论文" class="headerlink" title="近期要看的论文"></a>近期要看的论文</h1><p>焦点：目标检测，目标跟踪，目标分割。</p><p>看近三年关于目标检测、目标分割的综述论文，根据目录、参考文献扩展看。</p><p>这两周要看6篇论文（精读/速读）。</p><h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1><p>学习YOLOv3算法，并做一些实践。</p><p>未来阅读计划：学习R-CNN系列、SPP-Net、Selective Search、FPN(特征金字塔网络)</p>]]></content>
    
    
    <categories>
      
      <category>学习规划</category>
      
    </categories>
    
    
    <tags>
      
      <tag>加油</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>猴头菇&amp;豆豆</title>
    <link href="/2020/06/21/hello-world/"/>
    <url>/2020/06/21/hello-world/</url>
    
    <content type="html"><![CDATA[<p>欢迎来到猴头菇的小屋，这是我搭建的第一个博客~</p><p>相遇即是缘分~</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>二七广场航拍+延时摄影</title>
    <link href="/2019/12/07/%E4%BA%8C%E4%B8%83%E5%B9%BF%E5%9C%BA/"/>
    <url>/2019/12/07/%E4%BA%8C%E4%B8%83%E5%B9%BF%E5%9C%BA/</url>
    
    <content type="html"><![CDATA[<div style="position: relative; width: 100%; height: 0 ; padding-bottom: 75%;"><iframe src="//player.bilibili.com/player.html?aid=78399905&bvid=BV1iJ411e7rz&cid=134147965&page=1&high_quality=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true" style="position: absolute; width: 100%; height: 100%; left: 0; top: 0;" sandbox="allow-top-navigation allow-same-origin allow-forms allow-scripts"> </iframe></div>]]></content>
    
    
    <categories>
      
      <category>视频作品</category>
      
    </categories>
    
    
    <tags>
      
      <tag>video</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
